{"pages":[{"url":"http://carsonfarmer.com/about/","text":"My research tends to fall under the general banner of computational GIScience and encompasses work on networks, transportation, big data, snow/water processes, and geospatial algorithms. Beginning Fall 2015, I will be Assistant Professor in the Geography Department of the University of Colorado at Boulder where I will focus on spatial-temporal analysis and data-intensive modeling of dynamic geographical processes. I am a strong advocate for open source software and open data, and I enjoy cycling, snowboarding, and collecting robots. If you'd like to get in touch, please contact me . $(document).ready(function(){ $(\".contact\").hover( function() {$(this).attr(\"src\",\"../images/carson_circle_300_contact.png\");}, function() {$(this).attr(\"src\",\"../images/carson_circle_300.png\"); }); });","tags":"pages","title":"About"},{"url":"http://carsonfarmer.com/curriculum-vitae/","text":"Academic Positions 2015 — present Assistant Professor Department of Geography University of Colorado at Boulder 2014 — 2015 Assistant Professor Earth and Environmental Sciences, The Graduate Center City University of New York 2013 — 2015 Assistant Professor of GIScience Department of Geography, Hunter College City University of New York 2013 — 2015 Associate Director Center for Advanced Research of Spatial Information ( CARSI ) City University of New York Education 2008 — 2011 PhD in Geocomputation National University of Ireland, Maynooth, Ireland Thesis: Commuting flows & local labour markets: Spatial interaction modelling of travel-to-work Advisor: Prof. A. Stewart Fotheringham 2006 — 2008 MSc in Geography University of Victoria, Victoria, B.C., Canada Thesis: Spatial time-series analysis of satellite derived snow water equivalence Advisor: Dr. Trisalyn A. Nelson 2001 — 2006 BSc, Honours (with Distinction), in Geography University of Victoria, Victoria, B.C., Canada Thesis: Mapping the effects of the modifiable areal unit problem on local measures of spatial autocorrelation Advisor: Dr. Ian J. O'Connell Awards & Scholarships 2014 PSC CUNY Research Award, Hunter College, CUNY 2012 — 2014 Research Fellowship, Centre for GeoInformatics, University of St Andrews 2008 — 2012 Doctoral Fellowship, Social Science & Humanities Research Council of Canada 2011 Postgraduate Travel Fund/Collins Kitchin Fund, NUI Maynooth 2008 — 2011 PhD Fellowship, Irish Social Sciences Platform 2007 Derrick Sewell Graduate Scholarship, University of Victoria 2007 Physical Geography Student Paper Presentation Award, CAG 2006 Derrick Sewell Graduate Scholarship, University of Victoria 2006 Student Paper Presentation Award, Canadian Cartographic Association 2006 Ross Fund Undergraduate Scholarship, University of Victoria Publications & Presentations Refereed Publications Stojanovic T. and Farmer C. J. Q., (2013) The development of world oceans & coasts and concepts of sustainability . Marine Policy. 42: 157-165. Farmer, C. J. Q. and Pozdnoukhov, A. (2012) Building streaming GIScience from context, theory, and intelligence , Proceedings of the Workshop on GIScience in the Big Data Age . Columbus, Ohio. Andrew, M. E., T. A. Nelson, M. A. Wulder, G. W. Hobart, N. C. Coops, and C. J. Q. Farmer (2012) Ecosystem classifications based on summer and winter conditions . Environmental Monitoring and Assessment . July 2012: 1-23. Comber, A. J., Brunsdon, C. F., and Farmer, C. J. Q. (2012) Community detection in spatial networks: Inferring land use from a planar graph of land cover objects . International Journal of Applied Earth Observation and Geoinformation , 18: 274—-282. Farmer, C. J. Q. and Fotheringham, A. S. (2011) Network-based functional regions . Environment and Planning A . 43 (11): 2723-2741. Kaiser, C., Walsh, F., Farmer, C., and Pozdnoukhov, A. (2010) User-Centric Time-Distance Representation of Road Networks . In S. Fabrikant, T. Reichenbacher, M. van Kreveld, and C. Schlieder (Eds.), Geographic Information Science, Lecture Notes in Computer Science (Vol. 6292), Proceedings of at the 6th International Conference, GIScience 2010 , Zurich, Switzerland: Springer Berlin / Heidelberg, pp. 85-99. Farmer, C. J. Q., Nelson, T. A., Wulder, M. A., and Derksen, C. (2010) Identification of snow cover regimes through spatial and temporal clustering of satellite microwave brightness temperatures . Remote Sensing of Environment . 114 (1): 199-210. Farmer, C. J. Q., Nelson, T. A., Wulder, M. A., and Derksen, C. (2009) Spatial-temporal patterns of snow cover in western Canada . The Canadian Geographer , 53 (4): 473-487. Robertson, C., Farmer, C. J. Q., Nelson, T. A., Mackenzie, I. K., Wulder, M. A., and White, J. A. (2008) Determination of the compositional change (1999-2006) in the pine forests of British Columbia due to mountain pine beetle infestation. Environmental Monitoring and Assessment , DOI : 10.1007/s10661-008-0607-9. Robertson, C. and Farmer, C. J. (2008) Developing an open-source framework for surveillance and analysis of emerging zoonotic diseases. Proceedings of the Fifth National Symposium on Geo-Informatics of the Geo-Informatics Society of Sri Lanka . p. 123-134, July 25, 2008, Colombo, Sri Lanka. Non-Refereed Publications Farmer, C. J. Q. (2011) Commuting flows & local labour markets: Spatial interaction modelling of travel-to-work. PhD Thesis, National University of Ireland, Maynooth, Co. Kildare, Ireland , 251p. Farmer, C. J. Q. (2008) Spatial Time-series analysis of satellite derived snow water equivalence. Masters Thesis, University of Victoria, Victoria, British Columbia, Canada , 104p. Farmer, C. J. Q. (2006) Mapping the effects of the modifiable areal unit problem on local measures of spatial autocorrelation. Honors Thesis, University of Victoria, Victoria, British Columbia, Canada , 85p. Presentations Farmer, C. J. Q. (2014) Geospatial data in Python: Database, desktop, and the web. Invited Keynote, Interdisciplinary Workshop on Geospatial Computing, Kitchener, Ontario, Canada. [ Video ] Farmer, C. J. Q. (2014) Streaming GIScience: Data as flows, flows as data . Invited Keynote, GEOCROWD - Creating a Geospatial Knowledge World, Summer School II , July 14th-17th, St Andrews, Scotland, UK . Farmer, C. J. Q. (2014) Light-weight real-time event detection with Python . Scientific Computing with Python (SciPy 2014), July 6th-12th, Austin, Texas, USA . [ Video ] Farmer, C. J. Q. (2014) Geospatial data in Python: Database, desktop, and the web . Scientific Computing with Python (SciPy 2014), July 6th-12th, Austin, Texas, USA . [ Part 1 , Part 2 , Part 3 ] Farmer, C. J. Q. and Marcotullio, P. (2014) Agent-based models for spatially-explicit population projections. AAG Annual General Meeting. April 8th-12th, Tampa, Florida, USA . Farmer, C. J. Q. (2013) Cities as agents of growth: Agent models for population projections. Meeting on Global Spatial Population Projections: What Can Be Done Now? September 20th, 2013, New York, NY . Farmer, C. J. Q. (2012) Building streaming GIScience from context, theory, and intelligence. Workshop on GIScience in the Big Data Age, GIScience 2012, Columbus, Ohio. 7th International Conference, GIScience 2012. September 19th-21st, Columbus, Ohio, USA . Farmer, C. J. Q. (2012) The Centre for Geoinformatics at St Andrews: Multi-disciplinary opportunities for collaboration. Interaction and Visualisation Technologies in the Library . July 6th 2012, St Andrews, Scotland. Farmer, C. J. Q. (2012) Exploring spatio-temporal dynamics of networks . EPSRC workshop and launch, Featured speaker. June 27th 2012, St Andrews, Scotland. Farmer, C. J. Q. (2012) Spatial interaction modelling of commuting flows within local labour markets. Centre for GeoInformatics Seminar Series, Guest speaker. June 21st 2012, St Andrews, Scotland. Farmer, C. J. Q. (2012) Spatial interaction modelling of commuting flows within local labour markets. CAG Annual General Meeting. May 22nd-June 2nd 2012, Waterloo, Ontario, Canada. Farmer, C. J. Q (2012) Choice-set generation for spatial interaction models. AAG Annual General Meeting. February 22nd-28th, New York, N.Y., USA . Farmer, C. J. Q. and Fotheringham, A. S. (2011) Communities in commuting networks . Joint ICA / ISPRS Workshop, Our Complex World: Representation, Analysis and Modeling, Agust 10-12th, Simon Fraser University, Burnaby, B.C., Canada. Farmer, C. J. Q. (2011) Why geographers need to take another look at spatial behaviour. AAG Annual General Meeting. April 12th-16th, Seattle, W.A., USA . Kaiser, C., Walsh, F., Farmer, C. J. Q., and Pozdnoukhov, A. (2010) User-Centric Time-Distance Representation of Road Networks . 6th International Conference, GIScience 2010. September 14th-18th, University of Zurich, Zurich, Switzerland. Farmer, C. J. Q. (2009) Data driven functional regions . Geocomputation 2009. November 30th-December 2nd, University of New South Wales, Sydney, Australia. Farmer, C. J. Q., Walsh, F. and Pozdnoukhov, A. (2009) Dynamic ISOMAP embedding for real-time road network visualization. 16th European Colloquium on Quantitative and Theoretical Geography . September 4th-8th, Maynooth, Co. Kildare, Ireland. Farmer, C. J. Q., (2009) Why pay to do your own research? An introduction to open source geospatial software. Invited speaker, NCG lecture series . National Centre for Geocomputation, National University of Ireland Maynooth, Ireland. Farmer, C. J. Q., Nelson, T., Wulder, M. and Derksen, C. (2008) Identification of snow cover regimes through hierarchical clustering of remotely sensed time series'. Invited speaker, CHICAS lecture series . Department of Medicine, Lancaster University, UK . Farmer, C. J. Q., Nelson, T., Derksen, C. and Wulder, M. (2008) Determination of Canadian snow regimes using spatial-temporal analysis methods. AAG Annual General Meeting . April 15-19, Boston, M.A., US . Farmer, C. J. Q., Nelson, T., Derksen, C. and Wulder, M. (2008) Determination of Canadian snow regimes using passive microwave radiometry. Spatial Knowledge and Information . February 14-17, Fernie, B.C., Canada. Farmer, C. J. Q., Robertson, C., Nelson, T.A., Mackenzie, I.K., Wulder, M.A., and White, J.A. (2007) Integrating disparate mountain pine beetle datasets for detailed large-area mapping. University of Victoria, Geography Student Conference, Bridging the Gap 2007 , November 28, Victoria, B.C. Farmer, C. J. Q., and Nelson, T., Derksen, C. and Wulder, M. (2007) Linking the Spatial-Temporal Interactions of Spatial Association in SWE to Land-cover Characteristics. Canadian Association of Geographers Annual General Meeting . May 29-June 2, Saskatoon, S.K. (Winner of the CAG paper presentation award in Physical Geography) Farmer, C. J. Q., and Nelson, T., Derksen, C. and Wulder, M. (2007) The Impact of Temporal Resolution on Snow Water Equivalence Monitoring. Western Canadian Association of Geographers . March 8-10, Abbotsford, B.C. Farmer, C. J. Q., O'Connell, I., and Nelson, T. (2006) Mapping the effects of the modifiable areal unit problem on local measures of spatial autocorrelation. GeoTec Event 2006 . June 18-21, Ottawa, O.N. (Winner of the CCA best student paper presentation award.) Farmer, C. J. Q., O'Connell, I., and Nelson, T. (2006) Mapping the effects of the modifiable areal unit problem on local measures of spatial autocorrelation. Western Canadian Association of Geographers . March 10-11, Kamloops, B.C. Teaching Courses Introduction to GIScience, Advanced GIS , Spatial Data Analysis, Transportation Geography, Quantitative Methods Student Supervision Taylor Oshan* MS Thesis Advisor (Hunter College, CUNY ) Kristina Schmidt MS Thesis Advisor (Hunter College, CUNY ) Adele Balders MS Thesis Advisor (Hunter College, CUNY ) Joan Lee MS Thesis Advisor (Hunter College, CUNY ) Kristin Graves Thesis Co-Advisor (Hunter College, CUNY ) Silvia Lorenzo Thesis Co-Advisor (Hunter College, CUNY ) * Completed Academic Service July 2014 Program Committee Member (2014 International SciPy Conference) 2013 - present Committee Member (Public Health Certificate, Hunter College) 2013 - present Editorial Board Member (Spatial Demography) 2012 - present Reviewer (Journal of Spatial Science) 2012 - present Reviewer (Computers, Environment and Urban Systems) 2012 - present Reviewer (International Journal of Geographical Information Science) June 2013 Co-Organizer and Co-Chair/Presenter (GIScience Study Group Special Sessions, CAG AGM ) 2011 - present Member (Regional Studies Association) 2009 — present Reviewer (Transactions in GIS ) 2008 — present Reviewer (Remote Sensing of Environment) 2008 — 2011 Member (National University of Ireland, Maynooth Graduate Feedback Council) 2007 — present Member (American Association of Geographers) 2007 — present Member (Canadian Association of Geographers) 2009 Reviewer ( FOSS4G 2009 Conference Scientific Committee) 2006 — 2008 Organizer (Spatial Analysis Salon, Department of Geography, University of Victoria) 2008 Session Chair (American Association of Geographers AGM , Boston, USA ) 2007 Organizer (Geography Graduate Student Conference, University of Victoria) Impact & Outreach Visualization work appears in article on Wired.co.uk Visualization featured in University of St Andrews Press Release Profile in National University of Ireland, Maynooth 09/10 Presidents Report Analysis and visualization featured in news (10-12 August 2012): The Guardian data-blog The Scotsman news website The Telegraph (full article and interview) University of St Andrews Press Release ITV new website","tags":"pages","title":"Curriculum Vitae"},{"url":"http://carsonfarmer.com/research/","text":"Background Beginning Fall 2015, I will be Assistant Professor in the Geography Department of the University of Colorado at Boulder where I will focus on spatial-temporal analysis and data-intensive modeling of dynamic geographical processes. Prior to joining the faculty at Colorado I was Associate Director of the Center for Advanced Research of Spatial Information ( CARSI ) and Assistant Professor of GIScience at Hunter College of the City University of New York ( CUNY ). In 2012, I held a postdoctoral position at the University of St Andrews in Scotland, and in 2011 I completed a PhD funded by the Irish Social Sciences Platform ( ISSP ) and the Social Sciences and Humanities Research Council ( SSHRC ) of Canada at the National Center for Geocomputation ( NCG ) at the University of Maynooth (formerly National University of Ireland Maynooth) in Ireland. Both my undergraduate and Masters were completed in the Geography Department at the University of Victoria, in B.C. Canada (my hometown!). Current & Future Research My current and future research interests are ultimately concerned with modelling and understanding spatial processes. My work tends to fall under the general banner of ‘computational GIScience' and encompasses work on networks, transportation, big-data, snow/water processes, and geospatial algorithms. I am as much interested in theories of interactions of human and environmental processes as I am in the methods and models designed to characterize them. As part of my long-term research goals, I continue to incorporate theories of spatial interaction, time-geography, and GIScience into my work on complex systems and dynamics. My primary interest is in flow and movement data associated with underlying networks within human and natural environments. Spatial models are my primary means of quantitatively characterizing urban processes, and are used to explore data at a range of spatial and temporal resolutions. I also have a keen interest in developing new approaches to solving geographical problems by fostering expanded use of ideas and methods from outside geography and GIScience (such as data-streams and computer graphics). Additionally, I aim to promote the use of spatial analysis methods in data-intensive research to help explore the complex hierarchies and interactions within, across, and between human and natural systems. PhD Research Commuting flows & local labour markets: Spatial interaction modeling of travel-to-work One of the most promising approaches to mitigating land-use and transportation problems is continued research on urban commuting. Commuting is essential to many individuals, allowing them to participate in the labour market and earn a living to meet their essential needs. As such, a better understanding of the determinants of commuting will ultimately lead to a better understanding of the complexities of employment, housing, and the many spatial processes underlying commuting. However, in order to understand the commuting process, it is important to examine the milieu within which commuting takes place: the local labour market ( LLM ). In my PhD research, the interplay between commuting and LLMs is explored through the use of regionalization techniques and spatial interaction models. I try to show that LLM characteristics play a significant role in intra-regional commuting patterns and that a failure to account for LLM conditions may seriously hinder the applicability of models of commuting. Specifically, I suggest that there are many different LLMs across Ireland and that these LLMs characterize the commuting patterns of population sub-groups. By incorporating these LLMs into models of commuting, I show that in addition to distance and working population size, the spatial structure of origins and destinations and a number of non-spatial attributes such as unemployment, housing density, and education, all significantly affect commuting flows. Furthermore, the distance decay component of these models can be shown to capture a combination of geographical distance and regional differentiation due to LLM boundaries, leading to ‘functional' distance decay. This concept of functional distance decay is a key finding of my research, and, indicates that in addition to the configuration of origins and destinations, distance decay is also dependent on the spatial structure of LLMs, or more generally, the totality of surrounding conditions within which spatial interaction takes place. This research is funded in part by the ISSP , StratAG, and SSHRC , and is being completed under the supervision of Prof. Stewart Fotheringham. MSc Research Spatial time-series analysis of satellite derived snow water equivalence My MSc research focused primarily on examining the spatial-temporal characteristics of snow water equivalence ( SWE ) in Canada, and comparing these characteristics to known or hypothesized climate and ecological processes. The spatial and temporal distribution of terrestrial snow cover has implications for many ecological processes, such as local snowmelt release, local and global atmospheric circulation, as well as climate, and hydrological cycles. The sensitivity of terrestrial snow cover and SWE to atmospheric conditions and overlying air temperatures also makes snow cover a useful indicator of climate change. Thus, examining the spatial distribution of terrestrial snow cover and SWE over time aides in understanding current and future trends in changing climate conditions. Our research links the spatial-temporal interactions of SWE to underlying land-cover characteristics, as well as generates relevant temporal characteristics from SWE time-series' in order to explain the dominant SWE regimes across Canada. In addition to its broad implications for climate change research in general, this research also has implications for use in a national biodiversity monitory system for Canada, driven by remote sensing of multiple indicators of biodiversity. These indicators include productivity, disturbance, topography, and land-cover, of which terrestrial snow cover and SWE play an integral part.","tags":"pages","title":"Research"},{"url":"http://carsonfarmer.com/2015/09/teen-science-cafe-boulder-colorado/","text":"I am very excited to announce that I will be giving a talk at the Boulder Teen Science Cafe next Tuesday September 29th from 5:30-7pm at the CU Museum of Natural History ‘s BioLounge . My talk is entitled \"Leave No (Digital) Trace: The Geography of Social Media\", and here is the ‘snippet' from the website announcing my talk : Our first Teen Café of the 2015-16 academic year features self-described \"Geo Nerd\" Dr. Carson Farmer, assistant professor in the CU Geography Department. When the average person thinks about geography, they might imagine a weathered, old map or a globe. When Dr. Farmer thinks about geography, he sees the streams of data about our locations, movements, and actions that we leave in a digital trail behind us every time we post to Facebook, send a text message, or upload a photo to Instagram. It turns out that social media is a geographer's goldmine. Come learn about how social media can be used for good and evil, and about the fascinating ways in which scientists take advantage of the \"big data\" our many electronic devices now generate to study everything from the carbon footprint of your morning commute to snow and water processes around the globe. I should probably mention that the Teen Cafe is for teenagers . To quote the organizers: In order to maintain an atmosphere where teens can be themselves and feel comfortable engaging with our speaker and their peers, we encourage parents to drop teens off when possible and to leave younger siblings at home. In any case, it should be a lot of fun, and it sounds like an awesome program for teens in and around the Boulder and Denver area… go science!","tags":"News","title":"Boulder Teen Science Café: Leave No (Digital) Trace"},{"url":"http://carsonfarmer.com/2015/01/citizen-based-enviro-monitoring-nyc/","text":"Last fall, Carsten Kessler and I gave a online talk about our EnviroCar project . The talk was entitled \"EnviroCar - A GIS Framework for Citizen-based Environmental Monitoring in NYC \" and was hosted by the New York State GIS Association . It was the first talk of the Transportation Professional Affiliation Group, which was quite an honor for Carsten and I. Check out the NYSGIS YouTube channel for more awesome talks, and check out the following embedded video to see the EnviroCar recording:","tags":"Announcement","title":"Citizen-based Environmental Monitoring in NYC"},{"url":"http://carsonfarmer.com/2014/10/workshop-on-geospatial-computing/","text":"Hey all, I'll be running a half-day workshop on Geospatial Computing with Python at the upcoming Interdisciplinary Workshop on Geospatial Computing ( IWGC -2014) on November 20-21, 2014 at TheMuseum, Kitchener, Ontario . Its going to be a really great event, with lots of experts in the field of geospatial computing. It is particularly relevant to students who have an interest in geospatial computing. Organizers: Colin Robertson (Laurier), Graham Taylor (Guelph), Rob Feick (Waterloo) We invite participants to an Interdisciplinary Workshop on Geospatial Computing. The workshop is aimed broadly at bringing together disparate communities working with geospatial tools and data. We invite participants from academia (faculty, postdocs, grad students), industry (geospatial developers, data scientists, GIS analysts), and government to connect and share knowledge. Day 1 of the workshop will include invited talks from researchers from a variety of disciplines including engineering, statistics, ecology, geography, and information science. The day will conclude with a panel discussion and networking event. Day 2 will include hands-on interactive sessions so participants are encouraged to bring laptops and follow along. The day will conclude with a poster session and industry networking event which will include product demos and research posters. Our list of speakers includes: Krista Amolins (Esri Canada) Jennifer Baltzer (Laurier) Andrew Davidson (Agriculture and Agri-Food Canada) Carson Farmer ( CUNY ) Bahram Gharabaghi (Guelph) Steve Grise (vertex3) Dan Gillis (Guelph) Nicole Rabe ( OMAFRA ) Tarmo Remmel (York) Martin Sykora (Loughborough) Matthew Tenney (McGill) Participation: Open to all . Participants should register for the specific day or days they are interested in attending. We also encourage participants (especially grad students) to submit a poster abstract and present a poster during the session on the afternoon of Nov 21. Poster abstracts can be submitted via the registration page or can be emailed directly The deadline for poster abstract submissions is November 8th . Cost: Totally Free! Space is limited for this event. We hope to see you in November!","tags":"Announcement","title":"Interdisciplinary Workshop on Geospatial Computing"},{"url":"http://carsonfarmer.com/2014/10/what-to-do-with-all-this-data/","text":"This is an interesting and thought-provoking TED Talk by Susan Etlinger about how important it is to encourage ‘critical thinking' when it comes to Big Data. Here, the idea is not let the ‘data speak for themselves' but rather tell an engaging and intelligent story ‘with data'. Basically, we need to move beyond counting things in order to really understand anything. I agree with pretty much everything Susan says, and her use of a very personal example really deepens the impact of her talk. I really liked her focus on the need for ‘deepening our critical thinking skills', and I think this is something we need to ensure we are passing on to our students as well. Putting less focus on what the data say, and more focus on what we can learn with the data is something we should be teaching more at the undergraduate, graduate, and even high-school levels. I think in general, the statistics literature and curriculum are better at this perspective than their machine learning counterparts (although statistics curruculum [my own included] aren't always at the cutting-edge of modern teaching methods either). But this need not be the case, and I hope as we continue to generate, collect, store, and analyze more and more data, we keep in mind the need for context .","tags":"Big Data","title":"What Do We Do With All This Big Data?"},{"url":"http://carsonfarmer.com/2014/10/history-of-social-media/","text":"I've been spending a fair bit of time looking at material on social media data and social media usage, and I came across this video is from a CNBC piece called \"11 Predictions on the future of social media\" by Mary Catherine Wellons : It is very interesting to see how short the history of this relatively pervasive set of technologies is!","tags":"Social Media","title":"History of Social Media in 90 Seconds"},{"url":"http://carsonfarmer.com/2014/09/geos-seminar-graduate-center/","text":"The Geography, Earth Science and Oceanography Seminars ( GEOS ) is hosting myself and Prof Sean Ahearn this Thursday to give a talk on \"Computational GIScience: Current Research at the CARSI Lab\". The talk will feature a bunch of the fun and exciting stuff we're doing at CARSI , including our EnviroCar project and some of my recent work with spatial data flows. The talks will start around 5:30 PM on Thursday October 2nd 2014 in the Science Center, Room 4102 at the CUNY Graduate Center (365 5th Avenue, NYC ). I'm told that light snacks and refreshments will be served, so if anything, it's a chance to get some free grub! Here's a link to the original flyer . If you can't make it but want to hear about what CARSI is up to these days, drop me a line .","tags":"Announcement","title":"GEOS seminar series at the Graduate Center"},{"url":"http://carsonfarmer.com/2014/09/what-if-google-was-a-guy/","text":"This is absolutely hilarious! Everything you ask Google sounds a lot less intelligent when you actually ask Google. Don't forget to checkout Parts 2 and 3!","tags":"Funny","title":"What if Google was a Guy?"},{"url":"http://carsonfarmer.com/2014/09/become-citizen-scientist/","text":"Do you have an Android phone? Do you use a car on a regular basis? Become a Citizen Scientist! enviroCar is an open platform to collect and analyze moving vehicle data. It accesses the car's sensors with an Android smartphone and a Bluetooth OBD - II adapter that we will provide for free. The enviroCar app provides you with information about your car and your driving characteristics. By uploading the data to the enviroCar server you contribute to a growing collection of anonymized open data, helping scientists and traffic experts understand our city. We are looking for volunteers to join the envircoCar community in NYC and help us learn more about traffic flows and emissions. Sound exciting? Contact myself or Carsten Kessler to get your free OBD - II adapter and become a citizen scientist! Learn more about enviroCar at http://envirocar.org . View the original PDF poster .","tags":"Announcement","title":"Become a Citizen Scientist with EnviroCar"},{"url":"http://carsonfarmer.com/2014/05/guest_speaker_may_2014/","text":"Please join us Wednesday, May 21st from 4:00-6:00 pm in the Hunter College Geography Conference Room ( Hunter North 1004 ) for a talk by guest speaker Stephen Flood, who will be talking about Communicating across disciplines in the climate change sphere . If you can't make it in person, there will also be a live feed available here (the feed will also be recorded, so you can view it at a later date as well). Topic : This seminar will leverage the work carried out in the Climate Changes Impacts and Implications for New Zealand ( CCII ) project to demonstrate some of the key issues and approaches associated with communicating across a diverse range of disciplines and stakeholders. The broad aim of the presentation is to provide the epistemic community within the climate change sphere with grounded examples of effective communication approaches while also pointing out potential pitfalls and how best to avoid them. Bio : Dr. Stephen Flood is a Postdoctoral Research Fellow at the Climate Change Research Institute ( CCRI ) in Victoria University of Wellington, New Zealand. His research is focused on climate change adaptation, communication and decision-making associated with the interdisciplinary Climate Changes Implications and Impacts ( CCII ) project. Dr. Flood is also a lead consultant with SmartEarth consulting, which works with Governments and civil society in least developed countries (LDCs) to secure the necessary funding for capacity building and institutional development in the sphere of climate change mitigation and adaptation. Stephen is also a former colleague of mine from the National University of Ireland Maynooth , where he worked at the Irish Climate Analysis and Research Units ( ICARUS ) . For any questions, contact me via email or @carsonfarmer . Hope to see you there!","tags":"Speaker","title":"Stephen Flood Guest Speaker"},{"url":"http://carsonfarmer.com/2014/05/google-summer-of-code-2014/","text":"The decisions for Google Summer of Code 2014 have been made, and I'll be mentoring Rahul Raja from India. He will be working on the enviroCar app UX design. Right next door, Carsten Keßler (who has already posted an annoucement ) will be mentoring Tao Lin from China, who will be working on the Linked Data service provided by enviroCar. We are both looking forward to working with the students and with the folks at 52°North, who are also mentoring 3 more students in other projects : Sensor Data Access for Rasdaman, Simona Badoiu (Romania) Using the ILWIS framework for geo-data capture with a mobile application, Bouke Pieter Ottow (Netherlands) Proposal for Access Control User Interface for SOS Servers, Dushyant Sabharwal (India) enviroCar App UX Design, Rahul Raja (India) The Improvements of enviroCar Linked Data Service, Tao Lin (China)","tags":"Announcement","title":"Google Summer of Code 2014"},{"url":"http://carsonfarmer.com/2014/05/playing-with-citibike-trip-histories/","text":"I recently attended the April 2014 #BetaNYC (#BikeNYC) @CitiBikeNYC Hacknight, and after seeing several interesting presentations on what people are doing, want to be doing, and are thinking of doing with the recently released Citi Bike Trip Histories , I was inspired. A few of us got together to ‘quickly' and ‘easily' hack together a Sankey diagram of the bike trip flows… Turns out this wasn't nearly as quick and easy as we though: By the end of the evening, we were still struggling with getting D3js ‘s Sankey plugin to play nicely with our data (which I was manipulating in Python using Pandas ). I ended up playing around with the data later, and opted to visualize the flows between NYC neighborhoods using a simpler chord diagram . A chord diagram arranges the nodes (neighborhoods) radially, drawing thick curves between nodes. In my version, the thickness of links between neighborhoods encodes the relative frequency of rides between two neighborhoods: thicker links represent more frequent rides. Only flows that represent more than 1000 trips are represented to avoid too many small flows. Links are directed, and are colored by the more frequent origin (i.e., colored according to where most of the trips originate from). Whereas thecColors themselves are pretty much arbitrary. The visualization is here , and you can move the slider around to change which year/month is shown. Play around by sliding around and comparing flows over different time periods. Also watch for chord ‘flipping', where the dominant flow direction changes from month to month. This is particularly common in the smaller flows, where there isn't a strong dominant direction. The whole thing was built using D3js and based very heavily on this , this , and this . As I mentioned, the initial visualization was started at the April 2014 #BetaNYC Hacknight, and the version linked here is what I ended up with. Checkout the linked visualization for details on the data sources and the actual code/data used to produce it.","tags":"Visualization","title":"Playing with CitiBike Trip Histories"},{"url":"http://carsonfarmer.com/2014/04/foss_master_class_2014/","text":"FREE AND OPEN SOURCE GIS – August 4th to 8th, 2014 The Department of Geography at Hunter College of the City University of New York and Hunter Continuing Education are offering a five day professional course in Free and Open Source GIS from August 4th to 8th, 2014 . This five day course will span the entire range of GIS data capture, management, analysis, and visualization of geographic information using Free and Open Source Software ( FOSS ). These different elements of the GIS workflow will be discussed over the first four days and will then be applied in a final project completed on Friday. The course will combine lectures with hands-on sessions where participants will work with different free and open source GIS packages. Since we expect participants from many different organizations in the tri-state area, this training course also presents an excellent networking opportunity. The course is designed for experienced GIS users who want to broaden their skill set with expertise in the ever-growing world of free and open source GIS . Participants are expected to have a technical background and an interest in developing comprehensive workflows using multiple software components. While we do not require any programming experience, we will be working on the command line and developing some small scripts. Participants should be eager to master these valuable skills. This course is offered at Hunter College, CUNY , in the heart of the upper eastside of Manhattan very convenient to public transportation. Click here For course description, tuition, instructor bios and contacts, or call the Hunter Continuing Education office at 212-650-3850.","tags":"Announcement","title":"Free and Open Source Masterclass, Aug 2014"},{"url":"http://carsonfarmer.com/2014/04/guest_speaker_april_2014/","text":"Please join us Wednesday, April 30th from 3:30-5:00 pm in the Hunter College Geography Conference Room ( Hunter North 1004 ) for a talk by guest speakers JD Godchaux and Lela Prashad, and Robert Buchanan, who will be talking about Free & Open Source Software for Geospatial Applications . If you can't make it in person, there will also be a live feed available here (the feed will also be recorded, so you can view it at a later date as well). Announcement : We are excited to announce that the department will be hosting the second Free and Open Source for Geospatial ( FOSS4G ) presentation/discussion this Wednesday, from 3:30-5 pm in the Geography Department conference room ( HN 1004). The discussion will be on use of FOSS4G and web mapping technologies for community-based projects. Our theme is water quality and safety in the NYC region, and our guest speakers will be discussing two FOSS4G projects: JD Godchaux and Lela Prashad from Nijel.org will discuss the NY State Sewage Project which monitors sewage overflows in public waterways throughout the state. Robert Buchanan from the New School will discuss a separate project on water quality mapping in the New York metropolitan region using volunteer data collection efforts and Google maps. Both tools have been employed by the NY Surfriders Foundation chapter as ways to monitor overall water quality and respond to deficiencies in government efforts to ensure clean and safe water for humans and other species to enjoy. We hope you join us if you have time, it should be a great discussion on the ways you can employ easy to use FOSS4G tools to support community service and non-profit causes of all kinds. Checkout the flyer here ! For any questions, contact Gene via email . Hope to see you there!","tags":"Speaker","title":"Two Guest Speakers"},{"url":"http://carsonfarmer.com/2014/03/irish_famine_cartograms/","text":"In celebration of St Patrick's day last week, I decided to dig up an old dataset from when I was living/working in Ireland on historical Irish populations by county, and have a play with D3js and cartograms. Click here to view it ‘live'. If you've read any of my previous posts , you'll know that I like cartograms as a useful and fun way to visualize data. The Great Famine was an important and significant event in Irish (and global) history, and cartograms provide a fun and informative way to explore the resultant population change in Ireland from around the Famine era (and beyond). The cartogram ‘time-series' provides a simple visualization of population change in Ireland after the Famine era. It uses continuous area cartograms and population estimates from 1841 to 2001 to demonstrate change. The cool thing about this visualization is how dramatically it emphasizes population loss from 1841 to 1851 (and beyond), and how, even in modern Ireland, many counties remain well below their pre-famine population levels. As a whole, the population of Ireland remains less than 70% of its pre-famine levels! I've added a few nice interactive features to the map, including a popover feature that gives you additional information on mouse over. This includes county name, total population for the give year, and a nice little sparkline showing population change over time (with the current year highlighted for reference). This gives you a quick feel for the population change over time, and was pretty easy to do using D3js and Twitter Bootstrap. To produce the visualization, I leaned heavily on D3js , colorbrewer , Twitter's Bootstrap , jQuery , and some helpful examples from here , here , and here (among others). The code is based on the d3-cartogram example by Shawn Allen at Stamen .","tags":"Visualization","title":"After the Irish Famine: Population Change in Cartograms"},{"url":"http://carsonfarmer.com/2014/03/python_resources_qgis/","text":"There's a discussion thread on the QGIS LinkedIn Group page about Python tutorials and resources. There were a few good suggestions, so I thought I'd share these with others. It starts with a very common question from a GIS (or any software that supports scripting) user: I'm a real ‘end-user' of qgis and I want to improve my skills a little… I've found many python tutorials online but I don't know which are any good. Can anyone point me to some good resources? The responses were useful, but not exhaustive: The PyQGIS Programmer's Guide: http://locatepress.com/ppg PyQGIS developer cookbook: http://www.qgis.org/en/docs/pyqgis_developer_cookbook/ Geoprocessing with Python using FOSS GIS : http://www.gis.usu.edu/~chrisg/python/2009/ 10 Resources to Learn Python Programming Language: http://codecondo.com/10-ways-to-learn-python/ Do you have another suggestion? Please sound off in the comments below! [ UPDATE ]: I've added a link to resources for learning Python in general, very useful and comprehensive list. Check it out !","tags":"Python","title":"Python Resources for QGIS Users"},{"url":"http://carsonfarmer.com/2014/03/scipy_2014_submission_extended/","text":"Due to popular demand, the deadline for submitting talks, tutorials and posters has been extended to April 1, 2014 - no ‘foolin!'. We encourage submissions related to general scientific computing with Python, one of the two special themes for this year, or the domain-specific mini-symposia held during the conference. Take a look at a few talks from previous years, our guidelines for this year , and we look forward to reviewing submissions! Submit your abstracts today !","tags":"Update","title":"Submission deadline extended!"},{"url":"http://carsonfarmer.com/2014/03/esri_and_open_source/","text":"Here's a blog post from ESRI about ESRI 's transition to open source, open development, and social coding. * It features GitHub pretty prominently, which continues to be an awesome resource for collaborative work — and not just for code. My colleagues and I have started using it for planning meetings and workshops, developing research papers, maintaining websites (this site is hosted on GitHub), and yes, even open source software projects. ESRI obviously also thinks GitHub is a useful resource, and their keynote for the ESRI DevSummit is GitHub CEO and Co-Founder Chris Wanstrath ! Here's a particularly nice snippet from the post: The value is clear: through active and public collaboration we can effectively deliver a platform that empowers anyone with the freedom and ownership to customize solutions to their own domain. Our community spans the entire domain of government, science, education, commercial, and industrial practice. As a company it would be impossible for us to effectively serve every users need. Instead, open-source enables the community to scale itself. * Thanks to Carsten Keßler for the link","tags":"News","title":"ESRI and Open Source"},{"url":"http://carsonfarmer.com/2014/03/scipy_2014_mini_symposium/","text":"I have recently been asked to help out with the Geospatial Data in Science track for SciPy 2014 in Austin, Texas this coming July. The conference is being held at the AT &T Executive Education and Conference Center at the University of Texas campus in Austin, Texas from July 6th to 12th 2014 . It promises to be an awesome gathering of scientific Python users, developers, and organizations. You can checkout the conference announcement on the SciPy 2014 website , where you can register to submit a proposal and/or abstract, and generally find out all about the SciPy community and conference. I'm really excited about the conference, especially because the two main Specialized Tracks (which run parallel to the main conference) for the conference this year are Scientific Computing in Education and Geospatial Data in Science ; both topics near and dear to my heart! In particular, the geospatial track \"will focus on libraries, tools and techniques for processing Geospatial data of all types and for all purposes — from low-volume to high-volume, local and global\". Cool stuff! In addition to these two specialized tracks, we have Domain-specific Mini-symposia which you might be interested in: Introduced in 2012, mini-symposia are held to discuss scientific computing applied to a specific scientific domain/industry during a half afternoon after the general conference. Their goal is to promote industry specific libraries and tools, and gather people with similar interests for discussions. Mini-symposia on the following topics will take place this year: Astronomy and astrophysics Bioinformatics Geophysics Vision, Visualization, and Imaging Computational Social Science and Digital Humanities Engineering If you decide to submit an abstract, be sure to select Geospatial Data in Science as your \"Topic Track\". If you have any questions, feel free to get in touch . And don't forget, submission deadline is March 14th, 2014 ! Important Dates March 14th : Presentation abstracts, poster, tutorial submission deadline. Application for sponsorship deadline. April 17th: Speakers selected April 22nd: Sponsorship acceptance deadline May 1st: Speaker schedule announced May 6th: Early-bird registration ends (or after 150 registrants) July 6-12th: 2 days of tutorials, 3 days of conference, 2 days of sprints Hope to see you in Austin!","tags":"Announcement","title":"SciPy 2014 Geospatial Data in Science"},{"url":"http://carsonfarmer.com/2014/03/carsi_is_hiring_feb_2014/","text":"We have two openings for undergraduate research assistants here at the Center for Advanced Research of Spatial Information . You can find the original announcement on Dr. Carsten Keßler ‘s website. In short, we are looking for A student who can help us move our website over to WordPress ( detailed description ); and A student who can support us with Android app development in the enviroCar project ( detailed description ). If either of these topics sounds interesting to you, please get in touch with myself or Dr. Carsten Keßler .","tags":"Announcement","title":"CARSI is looking for research assistants"},{"url":"http://carsonfarmer.com/2014/03/first_gtech_experiement/","text":"In case you missed Eric Brelsford's talk last Wednesday on Free & Open Source Software for Geospatial Applications , I've embedded the recording below for your viewing pleasure (Eric's slides are also available ). This is the first in a series of talks organized within the department of Geography at Hunter College , CUNY around GIS and Technology (we're calling them GTECH Experiments). Each talk is organized by a student (thanks go to Mara Gittleman this time), and features a member of the wider geo-technology community. Check out the video below: Stay tuned for news and events around GTECH Experiments in the future!","tags":"Speaker","title":"First GTECH Experiment Recording"},{"url":"http://carsonfarmer.com/2014/03/nyc_geocoder/","text":"Recently, on the betaNYC Meetup email list, John Krauss and Tom Swanson both posted Python code for accessing the NYC Geoclient REST API , which is an awesome resource developed by the NYC Department of Information Technology and Telecommunications GIS /Mapping unit. The Geoclient API is a RESTful web service interface to the NYC Department of City Planning's Geosupport system developed by the Department of Information Technology and Telecommunications GIS /Mapping unit. Geosupport is a mainframe-based geocoding system used by NYC government. Geosupport provides coordinate and geographic attributes for supported input locations (address, intersection, blockface). Geoclient exposes the most widely used Geosupport functions and provides them in a more intuitive and modern manner. This is what John has to say about his code : I've been messing around with NYC 's geoclient API . It's quite powerful! I wrapped the REST calls in a Python module, which is accessible for all on PyPI. You can check it out here: https://github.com/talos/nyc-geoclient And the documentation here: http://nyc-geoclient.readthedocs.org/en/latest/index.html On a side-note, according to Geoclient, almost 20% of the intersections in the city's own collision statistics releases are ambiguous or invalid. And this is what Tom has to say about his : My code is nowhere near as clean as John's but if might be of interest that I ran ~3millions records through the NYC GeoClient in December. Overall, the services worked great and was able to make ~1,500 calls per min. I was geocoding the parking ticket data on nyc open data. https://github.com/tswanson/NYCParkingGeocode In addition to the above two Python implementations, Edgar Gonzalez also recently released a ruby gem for the NYC GeoClient API : Github: http://github.com/edgar/NYCGeoClient Rubygems: http://rubygems.org/gems/nyc_geo_client Note that you need to register an app with DoITT (and sign it up for the Geoclient API ) then wait a few days before being able to use the API . So get registered ASAP !","tags":"Python","title":"NYC Geoclient REST API from Python"},{"url":"http://carsonfarmer.com/2014/02/guest_speaker_feb_2014/","text":"Please join us Wednesday, February 26th from 3:00 to 5:00 pm in the Hunter College Geography Conference Room ( Hunter North 1004 ) for a talk by guest speaker Eric Brelsford, who will be talking about Free & Open Source Software for Geospatial Applications . UPDATE : If you can't make it in person, check out a live feed of the talk here . Topic : Eric will give us an overview of the Free & Open Source Software for Geospatial Applications ( FOSS4G ) terrain, followed by a few examples of workflow. What tools are out there for making useful and interesting online maps? What is \"open source\" software and how is it different from other software? Where does it fit in with the history of web mapping, who are the people (and what is the technology) on the cutting edge? Eric will then discuss which programs talk to each other, what types of things you can do with each platform. He'll talk about (and hopefully have time to walk us through) CartoDB (and cartocss), geojson, TileMill, and more, depending on what we have time to cover. If there's interest, he can go over using javascript libraries like leaflet to customize these online maps. Bio : Eric teaches web mapping (mostly open source) at The New School and co-runs 596 Acres . He's never taken a GIS class or used ArcGIS, but has produced maps here and here and here (among other places). He's also active in the Open Street Map community and just started a thing called Maptime (an idea that originated on the west coast, \"Our mission is to create a safe space where interdisciplinary communities can learn, socialize, and code maps with each other (and for each other)\"), where he led its first meeting. He's also on the internet @ebrelsford .","tags":"Announcement","title":"Eric Brelsford Guest Speaker"},{"url":"http://carsonfarmer.com/2014/02/call_for_abstracts_ugec_2014/","text":"The Urbanization and Global Environmental Change Project is pleased to announce that we will begin accepting abstracts for our 2nd International Conference on February 7th. We invite abstract submissions for oral and poster presentations. We particularly encourage contributions that exhibit an innovative set of conceptual and methodological approaches. Abstracts should be focused on synthesizing UGEC research, lessons learned, key ways forward, and should fall under one of the four conference themes: Urbanization Patterns and Processes Urban Responses to Climate Change: Adaptation, Mitigation and Transformation Global Environmental Change, Urban Health and Well-Being Equity and Environmental Justice in Urban Areas Click here for more details on the themes . Click here to read the concept note . Abstract Submission Guidelines : Abstract submissions should be written in English, not exceeding 300 words. The abstracts will be chosen based on clarity, appropriateness of the topic, methodology, originality and contribution to the overall synthesis-focus of our conference. Please indicate the title of the session(s) under which you are submitting your abstract. If no preference, leave blank. A list of sessions will be made available before February 7th. Deadline for submission is march 30th, 2014!","tags":"Announcement","title":"Call for Abstracts: 2nd International Conference UREC 2014"},{"url":"http://carsonfarmer.com/2014/02/ugec_conf_2014/","text":"The Urbanization and Global Environmental Change ( UGEC ) 2nd International Conference on \"Urban Transitions and Transformations: Science, Synthesis and Policy\" is scheduled to take place in Taipei, Taiwan from November 6th-8th, 2014. I am organizing a special session entitled \"Forecasting Urbanization: Population and Land Dimensions\" which promises to be very exciting. This is the ‘final wrap-up' for UGEC , so the main purpose of the conference is tp provide a synthesis of UGEC research and practice. Sessions are supposed to be reflective (i.e., not solely presentations on ‘new research'), so the talks should have a ‘lessons learned' focus. Hopefully there will be time in our session to discuss key points, research gaps, and ways forward for forcasting urban population growth. I'm going to start soliciting papers for this session pretty soon, but in the mean time, if you are interseted in presenting, please get in touch ! Abstracts can be submitted here , and you can register here (click on the Registration tab). When submitting an abstract, if you want to be included in my session, please indicate so when submitting online. It's probably a good idea to register early for the conference, plus, the Early Bird rates will only be available through June 15th, 2014. If you have any questions about the conference, you can email UGEC . P.S. If you aren't yet convinced that you should attend the 2014 UGEC Synthesis Conference, maybe Dr. Keren Seto can help convince you:","tags":"Announcement","title":"Urbanization and Global Environmental Change Conference 2014"},{"url":"http://carsonfarmer.com/2014/01/one_year_in_nyc/","text":"One year ago today, January 3rd 2014, my wife and I officially moved to New York City. It has flown by extremely fast for both of us, but we've managed to enjoy the city and all of the benefits that come with it. Some highlights (in no particular order) include: Starting my own research agenda Skating at Rockefeller Center Seeing Chicago on Broadway Wondering through Manhattan at Christmas time Road-trip to Boston Living in North America again (Europe was wonderful, but its nice to be closer to family) Trip up to Montreal Seeing The Nance on Broadway Staff passes to Macy's Thanksgiving Day Parade Meeting new friends and colleagues Seeing the Rockettes at Radio City Music Hall Teaching full courses for the first time Getting my first keynote speaker invite Wedding 2.0 Getting two Thanksgivings NYC parades (Pride, St Patrick's, Thanksgiving) Movie date-nights in Bryant Park Amazing skyline views from our rooftop Entertaining friends and family Ordering pretty much anything I could possibly want on-line Taking the Subway every day (Sometimes that sucks…) Getting a new dog Having my own office :-) Celebrating New Year's Eve only blocks from Times Square Walking the High Line Seeing the New York Philharmonic in Central Park (With fireworks!) Having access to so many world class restaurants all the time Street vendors (Street meat) Turning 30 Extreme temperatures And many more things, too numerous to list! Here's to the next year, may it be even more fun and exciting than the last.","tags":"Announcement","title":"One year in New York City"},{"url":"http://carsonfarmer.com/2013/12/bookmarklet_for_off_campus_library_access/","text":"I have been doing a fair bit of research off-campus lately, and as usual, have been having trouble accessing research materials (mainly academic publications) from home. Fortunately , Hunter College provides off-campus access to all electronic resources available to Hunter students, faculty and staff via their Library proxy server. Unfortunately , it turns out to be a huge pain to use anything other than the library search facilities (like Google Scholar ) through the proxy server*. In fact, when working off-campus, you actually have to preface each URL address to licensed resources with http://proxy.wexler.hunter.cuny.edu/login?url= in order to be able to access it. Not very handy… Bookmarklets to the rescue! This problem is actually something that bookmarklets are perfect for. A bookmarklet is (usually) just a small piece of JavaScript that resides in your browser and provides additional functionality to a web page. With that in mind, I decided to create a simple bookmarklet to automatically reload a given page with the above prefix prepended to the URL ; giving me access to the material via the library proxy server, while still being able to use whatever search tools I want. In this case, all the bookmarklet contains is the following JavaScript code: javascript : location . href = \"http://proxy.wexler.hunter.cuny.edu/login?url=\" + location . href So that the whole link is simply: <a href= \"javascript: location.href='http://proxy.wexler.hunter.cuny.edu/login?url='+location.href\" > Library Proxy </a> ‘ Installing' a bookmarklet is as simple as dragging it onto your browser's bookmarks toolbar (I think on some versions of Internet Explorer, you might have to right-click and select \"Add to Favorites…\"). If you drag this Library Proxy link onto your bookmarks bar, you'll have a handy little tool to automatically access the current page via the Hunter College library proxy (note that you'll need Hunter College credentials for this to work), instantly increasing your productivity by 12.45%… or so. * I might just be missing something, in which case, hopefully someone will correct me.","tags":"Helpful Tip","title":"A quick bookmarklet for off-campus library access"},{"url":"http://carsonfarmer.com/2013/12/cgi-lecturer-positions-2014/","text":"Researchers in the Centre for GeoInformatics ( CGI ) in the School of Geography and Geosciences at the University of St Andrews have been selected for a prestigious award under the Q-Step Quantitative Methods Programme funded by a combination of the Nuffield Foundation and the ESRC . This programme will employ two new lecturers to add substantial new courses to the school of Geography and Geosciences' existing undergraduate curriculum and help deliver a new MSc in GeoInformatics. They are looking for candidates with research interests in each of: Remote sensing Spatio-temporal analysis, specialization spatial statistics For more information on these two positions please click here . Should you have any informal queries about the posts, the university, or life in St Andrews, contact the CGI folks here .","tags":"Announcement","title":"Lectureships (x2) in GeoInformatics at the University of St Andrews"},{"url":"http://carsonfarmer.com/2013/11/statistical-modeling-python-variation/","text":"The 3rd in a series of tutorials on using Python for introductory statistical analysis, this tutorial covers methods for describing data via simple statistical calculations and statistical graphics. As always, the notebook for this tutorial is available here . In the 1880s, Sir Francis Galton, one of the pioneers of statistics, collected data on the heights of approximately 900 adult children and their parents in London. Galton was interested in studying the relationship between a full-grown child's height and his or her mother's and father's height. In order to do so, Galton collected height measurements from about 200 families in the city of London. As a setting to illustrate computer techniques for describing variability, take the data that Galton collected on the heights of adult children and their parents. The file \"galton.csv\" stores these data in a modern, case/variable format. [`galton.csv`][link] In [1]: import numpy as np import pandas as pd gal = pd . read_csv ( \"http://www.mosaic-web.org/go/datasets/galton.csv\" ) Simple Statistical Calculations Simple numerical descriptions are easy to compute. Here are the mean, median, standard deviation and variance of the children's heights (in inches). In [2]: gal . height . mean () Out[2]: 66.760690423162515 In [3]: gal . height . median () Out[3]: 66.5 In [4]: gal . height . std () Out[4]: 3.5829184699744614 In [5]: gal . height . var () Out[5]: 12.837304762484134 Notice that the variance function ( var() ) returns the square of the standard deviation ( std() ). Having both is merely a convenience. A percentile tells where a given value falls in a distribution. For example, a height of 63 inches is on the short side in Galton's data: In [6]: import scipy.stats as st # import some useful stats function from the scipy.stats library st . percentileofscore ( gal . height . values , 63 , kind = \"weak\" ) Out[6]: 19.153674832962139 Note that in the above case, `kind=\"weak\"` corresponds to the definition of a ‘cumulative distribution function'. A `percentileofscore()` of 80% means that 80% of values are less than or equal to the provided score. This usage is consistent with how the ‘mosaic' R package calculates percentiles, using the `pdata` function. Only about 19% of the cases have a height less than or equal to 63 inches. The percentileofscore() function from the scipy.stats package takes an array (values in a column) as a first argument and finds where the ‘score' (second argument) falls in the distribution of values in the array. We use the values attribute of the height column from the gal data frame to get the values in the column as an array . A quantile refers to the same sort of calculation, but inverted. Instead of giving a value in the same units as the distribution, you give a percentage: a number between 0 and 100. The scoreatpercentile() function then calculates the value whose percentile would be that value: In [7]: st . scoreatpercentile ( gal . height . values , per = 20 ) Out[7]: 63.5 Note that numpy has a simpler version of this function, but it uses a different naming convention which readers of ‘Statistical Modeling: A Fresh Approach' and these tutorials might find confusing. In this case, the function is called `percentile`, and returns the ‘score' or value at a given percentile. For example, `np.percentile(gal.height, 20)` returns `63.5` as in the above example. You can also use the following functionality from ‘pandas' which is consistent with how the ‘mosaic' R package calculates quantiles, using the qdata function, and is a bit simpler. Note that instead of a percentage, a probability is given (a number between 0 and 1), but the output is the same. In [8]: gal . height . quantile ( 0.2 ) Out[8]: 63.5 Building on this, the 25th and 75th percentiles - in other words, the 50 percent coverage interval, can be computed as: In [9]: gal . height . quantile ( . 25 ), gal . height . quantile ( . 75 ) Out[9]: (64.0, 69.700000000000003) The 50 percent coverage interval can also be computed as a single command using ‘list comprehension' - a ‘Pythonic' way to clearly and concisely construct lists. In [10]: [ gal . height . quantile ( q ) for q in [ . 25 , . 75 ]] Out[10]: [64.0, 69.700000000000003] [List comprehensions][lists] can be used to construct lists in a very natural, easy way, like a mathematician is used to do. The following are common ways to describe lists (or sets, or tuples, or vectors) in mathematics: $S = \\{ x&#94;{2} : \\{x \\in 0 \\dots 9\\} \\}$ $V = (1, 2, 4, 8, \\dots, 2&#94;{12})$ $M = \\{x$ $|$ $x$ $\\in S$ and $x$ even$\\}$ You probably know things like the above from previous math courses. In Python, you can write these expression almost exactly like a mathematician would do, without having to remember any special cryptic syntax. This is how you do the above in Python: `S = [x**2 for x in range(10)]` `V = [2**i for i in range(13)]` `M = [x for x in S if x % 2 == 0]` We can use the same techniques to compute the 2.5th and 97.5th percentiles - in other words, the 95 percent coverage interval: In [11]: [ gal . height . quantile ( q ) for q in [ . 025 , . 975 ]] Out[11]: [60.0, 73.0] Another simple way to compute different coverage intervals is to provide a percentile_width argument to the describe() function: In [12]: gal . height . describe ( percentile_width = 50 )[[ 4 , 6 ]] # Subset to return only the coverage interval values Out[12]: 25% 64.0 75% 69.7 Name: height, dtype: float64 The interquartile range is the width of the 50 percent coverage interval: the diﬀerence between the 75th and 25th percentiles: In [13]: np . diff ([ gal . height . quantile ( q ) for q in [ . 25 , . 75 ]]) Out[13]: array([ 5.7]) Simple Statistical Graphics There are several basic types of statistical graphics to display the distribution of a variable: histograms, density plots, and boxplots. These work in a manner that's similar to mean() , quantile() and so on in terms of syntax, but there are a few important additional items to consider. Firstly, most plotting functions we use will come from the ‘matplotlib' Python library, which integrates nicely with numpy and other Scientific Python libraries. There are several different ways that we can interact with ‘matplotlib', inlcuding via the pyplot interface (which is simply a submodule of ‘matplotlib' and is useful for ‘scripting') and the pylab interface, which is useful for interactive plotting. If you prefer to use the `pyplot` interface, then remember to import it in the usual way at the beginning of your Python session: `import matplotlib.pyplot` If you want interactive plotting (and why wouldn't you?!), start ‘IPython' with the --pylab flag. With this flag enabled, you don't have to import matplotlib , as it will be done for you automatically): ipython --pylab Or, for inline figures in an IPython qtconsole or notebook, use: ipython notebook --pylab inline </span> Histograms and Distributions Constructing a histogram involves dividing the range of a variable up into bins and counting how many cases fall into each bin. This is done in an almost entirely automatic way using the hist() function from a column in a dataframe: In [14]: h = gal . height . hist () You can also create a histogram using ‘pylab' commands directly: In [15]: h = hist ( gal . height . values ) When constructing a histogram, Python makes an automatic but sensible choice of the number of bins. If you like, you can control this yourself. For instance: In [16]: h = gal . height . hist ( bins = 25 ) The horizontal axis of the histogram is always in the units of the variable. For the histograms above, the horizontal axis is in \"inches\" because that is the unit of the height variable in the galton dataset. The vertical axis is conventionally drawn in one of two ways, controlled by an optional argument named normed : Absolute Frequency or Counts - A simple count of the number of cases that falls into each bin. This is the default, as in: gal.height.hist() Normalized - The vertical axis area of the bar gives the relative proportion of cases that fall into the bin. In other words, the areas can be interpreted as probabilities and the area under the entire histogram is equal to 1. Set the normed argument to True , as in: gal.height.hist(normed=True) You can also produce a histogram of relative frequencies, where the vertical axis is scaled so that the height of the bar give the proportion of cases that fall into the bin: gal.height.hist(weights=np.zeros_like(gal.height) + 100. / len(gal.height)) Other useful optional ‘pylab' commands set the labels for the axes and the graph as a whole and color the bars. For example, In [17]: xlabel ( \"Height (inches)\" ) ylabel ( \"Density\" ) title ( \"Distribution of Heights\" ) grid () h = gal . height . hist ( normed = True , color = \"grey\" ) show () The above plot requires multiple lines of commands to produce. Python evaluates each line on its own, updating the plot as each command is issued. Once the histogram is created, we can ‘show' it with show() . Notice also the use of quotation marks to delimit the labels and names like \"grey\". Density Plots A density plot avoids the need to create bins and plots out the distribution as a continuous curve. Making a density plot generally involves two operations. First, a density function performs the basic density computation, which is then displayed using the plot() function. Pandas provides a shorthand for these two operations to produce a simple ‘density plot' (where kind=\"kde\" stands for \"kind of plot equals ‘kernel density estimator'\"): In [18]: d = gal . height . plot ( kind = \"kde\" ) Box-and-Whisker Plots Box-and-whisker plots are made with the boxplot() command: In [19]: b = gal . boxplot ( column = \"height\" ) The median is represented by the red line in the middle. Outliers, if any, are marked by x ‘ s outside the whiskers. Note that instead of using gal.height , the boxplot() function operatres on the data frame, and takes the column name via the column argument. This is because the real power of the box-and-whisker plot is in comparing distributions. This will be raised again more systematically in later tutorials, but just to illustrate, here is how to compare the heights of males and females: In [20]: b = gal . boxplot ( column = \"height\" , by = \"sex\" ) Displays of Categorical Variables For categorical variables, it makes no sense to compute descriptive statistics such as the mean, standard deviation, or variance. Instead, look at the number of cases at each level of the variable. In [21]: gal . sex . value_counts () Out[21]: M 465 F 433 dtype: int64 Proportions can be found in a similar way (use float() to return a ‘float' rather than an ‘integer'): In [22]: gal . sex . value_counts () / float ( gal . sex . count ()) Out[22]: M 0.517817 F 0.482183 dtype: float64 Reference As with all ‘Statistical Modeling: A Fresh Approach for Python' tutorials, this tutorial is based directly on material from ‘Statistical Modeling: A Fresh Approach (2nd Edition)' by Daniel Kaplan . This tutorial is based on Chapter 3: Describing Variation. I have made an effort to keep the text and explanations consistent between the original (R-based) version and the Python tutorials, in order to keep things comparable. With that in mind, any errors, omissions, and/or differences between the two versions are mine, and any questions, comments, and/or concerns should be directed to me .","tags":"Statistical Modeling for Python","title":"Describing Variation"},{"url":"http://carsonfarmer.com/2013/11/spar-msc-positions-2014/","text":"Two MSc positions are available at the University of Victoria in the Department of Geography ‘s Spatial Pattern Analysis and Research ( SPAR ) Lab . Students will be involved in the development of the thesis topic with potential research areas including web mapping (e.g., bike accidents), mining and mapping social media (people's hunting activities), spatial environmental modelling, and spatial ecological research. Preference will be given to students with experience in GIS , spatial analysis, spatial statistics, programming, and/or statistics. Funding includes a graduate student stipend, as well as, support through teaching assistantships, research assistantships, and internal fellowships. Students can anticipate funding of about $15K per year. Start date is September 2014 . If interested, please contact Dr. Trisalyn Nelson by January 15th, 2014 .","tags":"Announcement","title":"MSc Positions at University of Victoria"},{"url":"http://carsonfarmer.com/2013/11/statistical-modeling-python-data/","text":"The second in a series of tutorials on using Python for introductory statistical analysis, this tutorial covers data , including cases, variables, samples, and a whole lot more. As always, the iPython Notebook associated with this tutorial is available here on github . Data used in statistical modeling are usually organized into tables, often created using spreadsheet software. Most people presume that the same software used to create a table of data should be used to display and analyze it. This is part of the reason for the popularity of spreadsheet programs such as ‘Excel' and ‘Google Spreadsheets'. For serious statistical work, it's helpful to take another approach that strictly separates the processes of data collection and of data analysis: use one program to create data ﬁles and another program to analyze the data stored in those ﬁles. By doing this, one guarantees that the original data are not modiﬁed accidentally in the process of analyzing them. This also makes it possible to perform many diﬀerent analyses of the data; modelers often create and compare many diﬀerent models of the same data. Reading Tabular Data into Python Data is central to statistics, and the tabular arrangement of data is very common. Accordingly, Python provides a large number of ways to read in tabular data. These vary depending on how the data are stored, where they are located, etc. To help keep things as simple as possible, the ‘pandas' Python library iprovides an operator, read_csv() that allows you to access data ﬁles in tabular format on your computer as well as data stored in repositories such as the one associated with the ‘Statistical Modeling: A Fresh Approach' book, or one that a course instructor might set up for his or her students. The ‘pandas' library is available here , and you can follow these installation instructions to get it working on your computer (installation via pip is the easiest method). Once you have ‘pandas' installed, you need to import pandas in order to to use read_csv() , as well as a variety of other ‘pandas' operators that you will encounter later (it is also usually a good idea to import numpy as np at the same time that we import pandas as pd ). An alternative to writing `pds.xxx` when calling each ‘pandas' operator is to import all available operators from ‘pandas' at once: `from pandas import *`. This makes things a bit easier in terms of typing, but can sometimes lead to confusion when operators from different libraries have the same name. In [1]: import pandas as pd import numpy as np You need do this only once in each session of Python, and on systems such as IPython, the library will sometimes be reloaded automatically (if you get an error message, it's likely that the ‘pandas' library has not been installed on your system. Follow the installation instructions provided at the link above.) Reading in a data table that's been connected with read_csv() is simply a matter of knowing the name (and location) of the data set. For instance, one data table used in examples in the ‘Statistical Modeling: A Fresh Approach' book is \"swim100m.csv\" . To read in this data table and create an object in Python that contains the data, use a command like this: [`swim100m.csv`][link] In [2]: swim = pd . read_csv ( \"http://www.mosaic-web.org/go/datasets/swim100m.csv\" ) The csv part of the name in \"swim100m.csv\" indicates that the ﬁle has been stored in a particular data format, comma-separated values that is handled by spreadsheet software as well as many other kinds of software. The part of this command that requires creativity is choosing a name for the Python object that will hold the data. In the above command it is called swim , but you might prefer another name (e.g., s or sdata or even ralph ). Of course, it's sensible to choose names that are short, easy to type and remember, and remind you what the contents of the object are about. To help you identify data tables that can be accessed through read_csv() , examples from these tutorials will be marked with a flag containing the name of the data file. The files themselves are mostly available automatically through the web site for the ‘Statistical Modeling: A Fresh Approach' book. Data Frames The type of Python object created by read_csv() is called a data frame and is essentially a tabular layout. To illustrate, here are the ﬁrst several cases of the swim data frame created by the previous use of read_csv() : In [3]: swim . head () Out[3]: year time sex 0 1905 65.8 M 1 1908 65.6 M 2 1910 62.8 M 3 1912 61.6 M 4 1918 61.4 M Note that the head() function, one of several functions built-into ‘pandas' data frames, is a function of the Python object (data frame) itself; not from the main ‘pandas' library. Data frames, like tabular data generally, involve variables and cases. In ‘pandas' data frames, each of the variables is given a name. You can refer to the variable by name in a couple of diﬀerent ways. To see the variable names in a data frame, something you might want to do to remind yourself of how names a spelled and capitalized, use the columns attribute of the data frame object: In [4]: swim . columns Out[4]: Index([u&apos;year&apos;, u&apos;time&apos;, u&apos;sex&apos;], dtype=object) Note that we have not used brackets () in the above command. This is because columns is not a function; it is an attribute of the data frame. Attributes add ‘extra' information (or metadata) to objects in the form of additional Python objects. In this case, the attributes describe the names (and data types) of the columns. Another way to get quick information about the variables in a data frame is with describe() : In [5]: swim . describe () Out[5]: year time count 62.000000 62.000000 mean 1952.145161 59.924194 std 29.472881 9.916588 min 1905.000000 47.840000 25% 1924.500000 53.642500 50% 1956.500000 56.880000 75% 1975.750000 65.200000 max 2004.000000 95.000000 This provides a numerical summary of each of the variables contained in the data frame. To keep things simple, the output from describe() is itself a data frame. There are lots of different functions and attributes available for data frames (and any other Python objects). For instance, to see how many cases and variables there are in a data frame, you can use the shape attribute: In [6]: swim . shape Out[6]: (62, 3) Variables in Data Frames Perhaps the most common operation on a data frame is to refer to the values in a single variable. The two ways you will most commonly use involve referring to a variable by string-quoted name ( swim[\"year\"] ) and as an attribute of a data frame without quotes ( swim.year ). Each column or variable in a ‘pandas' data frame is called a ‘series', and each series can contain one of many different data types. For more information on series', data frames, and other objects in ‘pandas', [have a look here][intro]. Most of the statistical modeling functions you will encounter in these tutorials are designed to work with data frames and allow you to refer directly to variables within a data frame. For instance: In [7]: swim . year . mean () Out[7]: 1952.1451612903227 In [8]: swim [ \"year\" ] . min () Out[8]: 1905 It is also possible to combine ‘numpy' operators with ‘pandas' variables: In [9]: np . min ( swim [ \"year\" ]) Out[9]: 1905 In [10]: np . min ( swim . year ) Out[10]: 1905 The swim portion of the above commands tells Python which data frame we want to operate on. Leaving oﬀ that argument leads to an error: In [11]: year . min () --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-11-2ef03df1cde8> in <module> () ----> 1 year . min ( ) NameError : name &apos;year&apos; is not defined Of course, you know that the variable year is deﬁned within the data frame swim , but you have to tell Python which data frame you want to operate on explicitly, otherwise it doesn't know where to find the variable(s). Think of this notation as referring to the variable by both its family name (the data frame's name, \"swim\" ) and its given name ( \"year\" ), something like einstein.albert . The advantage of referring to variables by name becomes evident when you construct statements that involve more than one variable within a data frame. For instance, here's a calculation of the mean year, separately for (grouping by) the different sexes: In [12]: swim . groupby ( 'sex' )[ 'year' ] . mean () Out[12]: sex F 1950.677419 M 1953.612903 Name: year, dtype: float64 You will see much more of the groupby function, starting in Tutorial 4 (Group-wise Models). It's the ‘pandas' way of grouping or aggregating data frames. In subsequent chapters, we will build on this notion to develop more complex ways of \"grouping\" and \"modeling\" variables \"by\" other variables. Both the mean() and min() functions have been arranged by the ‘pandas' library to look in the data frame when interpreting variables, but not all Python functions are designed this way. For instance: In [13]: swim . year . sqrt () --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-13-e6382fdf6716> in <module> () ----> 1 swim . year . sqrt ( ) AttributeError : &apos;Series&apos; object has no attribute &apos;sqrt&apos; When you encounter a function that isn't supported by data frames, you can use ‘numpy' functions and the special apply function built-into data frames (note that the func argument is optional): In [14]: swim . year . apply ( func = np . sqrt ) . head () # There are 62 cases in total Out[14]: 0 43.646306 1 43.680659 2 43.703547 3 43.726422 4 43.794977 Name: year, dtype: float64 Alternatively, since columns are basically just arrays, we can use built-in numpy functions directly on the columns: In [15]: np . sqrt ( swim . year ) . head () # Again, there are 62 cases in total Out[15]: 0 43.646306 1 43.680659 2 43.703547 3 43.726422 4 43.794977 Name: year, dtype: float64 Adding a New Variable Sometimes you will compute a new quantity from the existing variables and want to treat this as a new variable. Adding a new variable to a data frame can be done similarly to accessing a variable. For instance, here is how to create a new variable in swim that holds the time converted from seconds to units of minutes: In [16]: swim [ 'minutes' ] = swim . time / 60. # or swim['time']/60. By default, columns get inserted at the end. The insert function is available to insert at a particular location in the columns. In [17]: swim . insert ( 1 , 'mins' , swim . time / 60. ) You could also, if you want, redeﬁne an existing variable, for instance: In [18]: swim [ 'time' ] = swim . time / 60. As always, we can take a quick look at the results of our operations by using the head() fuction of our data frame: In [19]: swim . head () Out[19]: year mins time sex minutes 0 1905 1.096667 1.096667 M 1.096667 1 1908 1.093333 1.093333 M 1.093333 2 1910 1.046667 1.046667 M 1.046667 3 1912 1.026667 1.026667 M 1.026667 4 1918 1.023333 1.023333 M 1.023333 Such assignment operations do not change the original file from which the data were read, only the data frame in the current session of Python. This is an advantage, since it means that your data in the data file stay in their original state and therefore won't be corrupted by operations made during analysis. Sampling from a Sample Frame Much of statistical analysis is concerned with the consequences of drawing a sample from the population. Ideally, you will have a sampling frame that lists every member of the population from which the sample is to be drawn. With this in hand, you could treat the individual cases in the sampling frame as if they were cards in a deck of hands. To pick your random sample, shuffle the deck and deal out the desired number of cards. When doing real work in the ﬁeld, you would use the randomly dealt cards to locate the real-world cases they correspond to. Sometimes in these tutorials, however, in order to let you explore the consequences of sampling, you will select a sample from an existing data set. For example, the \"kidsfeet.csv\" data set has n=39 cases. [`kidsfeet.csv`][link] In [20]: kids = pd . read_csv ( \"http://www.mosaic-web.org/go/datasets/kidsfeet.csv\" ) kids . shape Out[20]: (39, 8) There are a number of procedures to draw a random sample of 5 cases from this data frame. The preferred option however, is to randomly select a subset of case ids (in this case 5) using np.random.choice , and return a subsetted data frame using the ix[] operator. The `ix[]` property is a bit tricky to figure out at first. For more information, see [the official docs][selecting]. In [21]: rows = np . random . choice ( kids . index , 5 , replace = False ) kids . ix [ rows ] Out[21]: name birthmonth birthyear length width sex biggerfoot domhand 23 Erica 9 88 24.5 9.0 G L R 16 Caroline 12 87 24.0 8.7 G R L 4 Lang 2 88 25.1 8.9 B L R 32 Leigh 3 88 24.5 8.6 G L R 7 Caitlin 6 88 23.0 8.8 G L R To make things a bit more concise, you can `import np.random.choice as choice`, which will allow you to simply use `choice()` without including the library *and* module when typing. This can also be done in a single line: In [22]: kids . ix [ np . random . choice ( kids . index , 5 , replace = False )] Out[22]: name birthmonth birthyear length width sex biggerfoot domhand 19 Heather 3 88 25.5 9.5 G R R 4 Lang 2 88 25.1 8.9 B L R 3 Josh 1 88 25.2 9.8 B L R 31 Caitlin 7 88 22.5 8.6 G R R 7 Caitlin 6 88 23.0 8.8 G L R The results returned by the above methods will never contain the same case more than once (because we told the function not to sample with replacement), just as if you were dealing cards from a shuffled deck. In contrast, ‘re-sampling with replacement' replaces each case after it is dealt so that it can appear more than once in the result. You wouldn't want to do this to select from a sampling frame, but it turns out that there are valuable statistical uses for this sort of sampling with replacement . You'll make use of re-sampling in Tutorial 5 (Conﬁdence Intervals). In [23]: np . random . seed ( 1237 ) # Set seed so results are reproducible kids . ix [ np . random . choice ( kids . index , 5 , replace = True )] Out[23]: name birthmonth birthyear length width sex biggerfoot domhand 11 Ray 3 88 24.8 8.9 B L R 25 Glen 7 88 27.1 9.4 B L R 36 Teshanna 3 88 26.0 9.0 G L R 7 Caitlin 6 88 23.0 8.8 G L R 25 Glen 7 88 27.1 9.4 B L R Notice that ‘Glen' was sampled twice. Reference As with all ‘Statistical Modeling: A Fresh Approach for Python' tutorials, this tutorial is based directly on material from ‘Statistical Modeling: A Fresh Approach (2nd Edition)' by Daniel Kaplan . This tutorial is based on Chapter 2: Data: Cases, Variables, Samples. I have made an effort to keep the text and explanations consistent between the original (R-based) version and the Python tutorials, in order to keep things comparable. With that in mind, any errors, omissions, and/or differences between the two versions are mine, and any questions, comments, and/or concerns should be directed to me .","tags":"Statistical Modeling for Python","title":"Data: Cases, Variables, Samples"},{"url":"http://carsonfarmer.com/2013/11/statistical-modeling-python-introduction/","text":"Welcome to the first in a series of tutorials on using Python for introductory statistical analysis. As I put more of these tutorials online, you should be able to access them easily by clicking or searching for the relevant category: \"Statistical Modeling for Python\". This series of tutorials is based on the ‘Computational Technique' sections of each chapter from ‘Statistical Modeling: A Fresh Approach (2nd Edition)' . The goal of this series of tutorials is to show how all of the R analysis and commands used in the book can be done just as easily using the Python programming language. This has the dual goal of introducing ‘Scientific Python' to students learning statistics, as well as showcasing the recent advances in statistical computing that have been introduced to Python in recent years. Each tutorial in the series will cover the Computational Technique section of a different chapter from the book, starting with Section 1.4.3 from the introduction (which technically isn't a Computational Technique section, but is a useful introduction none-the-less), which is available online here . Note that many of these tutorials will require you to have read the corresponding chapter(s) from the book in order to be useful. All the tutorials assume that Python is installed and running on your computer. To use the notebooks associated with these tutorials , you'll also need to have IPython (with notebook) installed. There are plenty of resources online with information on IPython , Notebooks , and installation instructions ) for both. You'll also need to have the Scientific Python libraries installed for additional statistical functionality (we'll also introduce some other statistical libraries in later tutorials), and matplotlib for plotting and visualization. The IPython Command Console Once you have IPython installed, you are ready to perform all sorts of statistical and other mathematical and scientific operations. However, to use the powerful range of tools, functions, commands, and statistical methods available in Python, you first need to learn a little bit about the syntax and meaning of Python commands. Once you have learned this, operations become simple to perform. Before staring this tutorial, read section 1.4 from ‘Statistical Modeling: A Fresh Approach', which outlines some general concepts surrounding computational statistics (in the context of R). In particular, it provides some explanation of a ‘language-based approach' to statistical computing - which is an important concept throughout the book and this series of tutorials. Invoking an Operation People often think of computers as doing things : sending email, playing music, storing files. Your job in using a computer is to tell the computer what to do. There are many different words used to refer to the \"what\": a procedure, a task, a function, a routine, and so on. Like in the book, I'll use the word computation . Admittedly, this is a bit circular, but it is easy to remember: computers perform computations. Complex computations are built up from simpler computations. This may seem obvious, but it is a powerful idea. An algorithm is just a description of a computation in terms of other computations that you already know how to perform. To help distinguish between the computation as a whole and the simpler parts, it is helpful to introduce a new word: an operator performs a computation. It's helpful to think of the computation carried out by an operator as involving four parts: The name of the operator The input arguments The output value Side effects A typical operation takes one or more input arguments and uses the information in these to produce an output value . Along the way, the computer might take some action: display a graph, store a file, make a sound, etc. These actions are called side effects . Because R is a programing language designed specifically for statistical analysis, many of the ‘base' commands (such as sqrt() ) are available ‘out-of-the-box'. However, since Python is a more general-purpose programming language, we usually need to import statistical commands (think of this as adding words to a language) before we can use them. For Scientific Python, the most important library that we need is numpy (Numerical Python), which can be loaded like this: In [1]: import numpy as np It is best to ensure you are using the latest version of `numpy`, which is available [from here][numpy]. To tell the computer to perform a computation - call this invoking an operation or giving a command - you need to provide the name and the input arguments in a specific format. The computer then returns the output value. For example, the command np.sqrt(25) invokes the square root operator (named sqrt from the numpy library) on the argument 25 . The output from the computation will, of course, we 5 . The syntax of invoking an operation consists of the operator's name, followed by round parentheses. The input arguments go inside the parentheses. The software program that you use to invoke operators is called an interpreter (the interpreter is the program you are running when you start Python). You enter your commands as a ‘dialog' between you and the interpreter (just like when converting between any two languages!). Commands can be entered as part of a script (a text file with a list of commands to perform) or directly at a ‘command prompt': In [2]: np . sqrt ( 25 ) Out[2]: 5.0 In the above situation, the ‘prompt' is In [ ]: , and the ‘command' is np.sqrt(25) . When you press ‘Enter', the interpreter reads your command and performs the computation. For commands such as the one above, the interpreter will print the output value from the computation: In [3]: np . sqrt ( 25 ) Out[3]: 5.0 In the above example, the ‘output marker' is Out[ ]: , and the output value is 5.0 . The dialog continues as the interpreter prints another prompt and waits for your further command. Often, operations involve more than one argument. The various arguments are separated by commas. For example, here is an operation named arange from the numpy library that produces a range of numbers (increasing values between 3 and 10): In [4]: np . arange ( 3 , 10 ) Out[4]: array([3, 4, 5, 6, 7, 8, 9]) The first argument tells where to start the range and the second tells where to end it. The order of the arguments is important. For instance, here is the range produced when 10 is the first argument, 3 is the second, and the third is -1 (decreasing values between 10 and 3): In [5]: np . arange ( 10 , 3 , - 1 ) Out[5]: array([10, 9, 8, 7, 6, 5, 4]) For some operators, particularly those that have many input arguments, some of the arguments can be referred to by name rather than position. This is particularly useful when the named argument has a sensible default value. For example, the arange operator from the numpy library can be instructed what type of output values to produce (integers, floats, etc). This is accomplished using an argument named dtype : In [6]: np . arange ( 10 , 3 , - 1 , dtype = 'float' ) Out[6]: array([ 10., 9., 8., 7., 6., 5., 4.]) Note that all the values in the range now have decimal places. Depending on the circumstances, all four parts of an operation need not be present. For example, the ctime operation from the time library returns the current time and date; no input arguments are required: In [7]: import time time . ctime () Out[7]: &apos;Mon Sep 23 15:58:25 2013&apos; In the above example, we first imported the time library, which provides a series of commands that help us work with dates and times. Next, even though there are no arguments, the parentheses are still used when calling the ctime command. Think of the pair of parentheses as meaning, ‘ do this ‘. Naming and Storing Values Often the value returned by an operation will be used later on. Values can be stored for later use with the assignment operator . This has a different syntax that reminds the user that a value is being stored. Here's an example of a simple assignment: In [8]: x = 16 The command has stored the value 16 under the name x . The syntax is always the same: an equal sign (=) with a name on the left side and a value on the right. Such stored values are called objects . Making an assignment to an object defines the object. Once an object has been defined, it can be referred to and used in later computations. Notice that an assignment operation does not return a value or display a value. Its sole purpose is to have the side effects of defining the object and thereby storing a value under the object's name. To refer to the value stored in the object, just use the object's name itself. For instance: In [9]: x Out[9]: 16 Doing a computation on the value store in an object is much the same (and provides and extremely rich syntax for performing complex calculations): In [10]: np . sqrt ( x ) Out[10]: 4.0 You can create as many objects as you like and give them names that remind you of their purpose. Some examples: wilma , ages , temp , dog_houses , foo3 . There are some general rules for object names: Use only letters and numbers and ‘underscores' (_) Do NOT use spaces anywhere in the name (Python won't let you) A number cannot be the first character in the name Capital letters are treated as distinct from lower-case letters (i.e., Python is case-sensitive ) the objects named wilma , Wilma , and WILMA are all different If possible, use an ‘underscore' between words (i.e., my_object ) For the sake of readability, keep object names short. But if you really must have an object named something like ages_of_children_from_the _clinical_trial , feel free (it's just more typing for you later!). Objects can store all sorts of things, for example a range of numbers: In [11]: x = np . arange ( 1 , 7 ) When you assign a new value to an existing object, as just done to x above, the former values of that object is erased from the computer memory. The former value of x was 16, but after the new assignment above, it is: In [12]: x Out[12]: array([1, 2, 3, 4, 5, 6]) The value of an object is changed only via the assignment operator. Using an object in a computation does not change the value. For example, suppose you invoke the square-root operator on x : In [13]: np . sqrt ( x ) Out[13]: array([ 1. , 1.41421356, 1.73205081, 2. , 2.23606798, 2.44948974]) The square roots have been returned as a value, but this doesn't change the value of x : In [14]: x Out[14]: array([1, 2, 3, 4, 5, 6]) An assignment command like x=np.sqrt(x) can be confusing to people who are used to algebraic notation. In algebra, the equal sign describes a relationship between the left and right sides. So, $x = \\sqrt{x}$ tells us about how the quantity $x$ and the quantity $\\sqrt{x}$ are related. Students are usually trained to ‘solve' such a relationship, going through a series of algebraic steps to find values for $x$ that are consistent with the mathematical statement (for $x = \\sqrt{x}$, the solutions are $x = 0$ and $x = 1$). In contrast, the assignment command x = np.sqrt(x) is a way of replacing the previous values stored in x with new values that are the square-root of the old ones. If you want to change the value of x , you need to use the assignment operator: In [15]: x = np . sqrt ( x ) Connecting Computations The brilliant thing about organizing operators in terms of unput arguments and output values is that the output of one operator can be used as an input to another. This lets complicated computations be built out of simpler ones. For example, suppose you have a list of 10000 voters in a precinct and you want to select a random sample of 20 of them for a survey. The np.arange operator can be used to generate a set of 10000 choices. The np.random.choice operator can then be used to select a subset of these values at random. One way to connect the computations is by using objects to store the intermediate outputs: In [16]: choices = np . arange ( 1 , 10000 ) np . random . choice ( choices , 20 , replace = False ) # sample _without_ replacement Out[16]: array([7863, 8378, 9128, 3340, 5674, 9055, 6374, 8668, 3768, 6798, 8066, 6443, 5154, 5991, 1535, 3580, 8516, 4872, 8618, 7240]) You can also pass the output of an operator directly as an argument to another operator. Here's another way to accomplish exactly the same thing as the above (note that the values will differ because we are performing a random sample): In [17]: np . random . choice ( np . arange ( 1 , 10000 ), 20 , replace = False ) Out[17]: array([5732, 6833, 7705, 4459, 3131, 3515, 4177, 6312, 2820, 2705, 4580, 9125, 7395, 1927, 728, 4725, 1854, 6147, 4421, 2756]) Numbers and Arithmetic The Python language has a concise notation for arithmetic that looks very much like the traditional one: In [18]: 7. + 2. Out[18]: 9.0 In [19]: 3. * 4. Out[19]: 12.0 In [20]: 5. / 2. Out[20]: 2.5 In [21]: 3. - 8. Out[21]: -5.0 In [22]: - 3. Out[22]: -3.0 In [23]: 5. ** 2. # same as 5&#94;2 (or 5 to the power of 2) Out[23]: 25.0 Arithmetic operators, like any other operators, can be connected to form more complicated computations. For instance: In [24]: 8. + 4. / 2. Out[24]: 10.0 The a human reader, the command 8+4/2 might seem ambiguous. Is it intended to be (8+4)/2 or 8+(4/2) ? The computer uses unambiguous rules to interpret the expression, but it's a good idea for you to use parentheses so that you can make sure that what you intend is what the computer carries out: In [25]: ( 8. + 4. ) / 2. Out[25]: 6.0 Traditional mathematical notations uses superscripts and radicals to indicate exponentials and roots, e.g. $3&#94;2$ or $\\sqrt{3}$ or $\\sqrt[3]{8}$. This special typography doesn't work well with an ordinary keyboard, so Python and most other computer languages uses a different notation: In [26]: 3. ** 2. Out[26]: 9.0 In [27]: np . sqrt ( 3. ) # or 3.**0.5 Out[27]: 1.7320508075688772 In [28]: 8. ** ( 1. / 3. ) Out[28]: 2.0 There is a large set of mathematical functions: exponentials, logs, trigonometric and inverse trigonometric functions, etc. Some examples: Traditional Python $e&#94;2$ np.exp(2) $\\log_{e}(100)$ np.log(100) $\\log_{10}(100)$ np.log10(100) $\\log_{2}(100)$ np.log2(100) $\\cos(\\frac{\\pi}{2})$ np.cos(np.pi/2) $\\sin(\\frac{\\pi}{2})$ np.sin(np.pi/2) $\\tan(\\frac{\\pi}{2})$ np.tan(np.pi/2) $\\cos&#94;{-1}(-1)$ np.acos(-1) Numbers can be written in scientific notation . For example, the ‘universal gravitational constant' that describes the gravitational attraction between masses is $6.67428 \\times 10&#94;{11}$ (with units meters-cubed per kilogram per second squared). In the computer notation, this would be written as 6.67428e-11 . The Avogadro constant, which gives the number of atoms in a mole, is $6.02214179 \\times 10&#94;{23}$ per mole, or 6.02214179e+23 . The computer language does not directly support the recording of units. This is unfortunate, since in the real world numbers often have units and the units matter. For example, in 1999 the Mars Climate Orbiter crashed into Mars because the design engineers specified the engine's thrust in units of pounds, while the guidance engineers thought the units were newtons. There are *some* Python packages for handling units, including [pint], [quantities], [units], [sympy.physics.units], [etc]. Computer arithmetic is accurate and reliable, but it often involves very slight rounding of numbers. Ordinarily, this is not noticeable. However, it can become apparent in some calculations that produce results that are (near) zero. For example, mathematically, $sin(\\pi) = 0$, however, the computer does not duplicate the mathematical relationship exactly: In [29]: np . sin ( np . pi ) Out[29]: 1.2246467991473532e-16 Whether a number like this is properly interpreted as ‘close to zero' depends on the context and, for quantities that have units, on the units themselves. For instance, the unit ‘parsec' is used in astronomy in reporting distances between stars. The closest start to the Sun is Proxima, at a distance of 1.3 parsecs. A distance of $1.22 \\times 10&#94;{-16}$ parsecs is tiny in astronomy but translates to about 2.5 meters - not so small on the human scale. In statistics, many calculations relate to probabilities which are always in the range 0 to 1. On this scale, 1.22e-16 is very close to zero. There are several ‘special' numbers in the Python world; two of which are inf , which stands for $\\infty$ (infinity), and nan , which stands for ‘not a number' (nan results when a numerical operation isn't define), for instance: Mathematically oriented readers will wonder why Python should have any trouble with a computation like $\\sqrt{-9}$; the result is the imaginary number $3\\jmath$ (imaginary numbers may be represented by a $\\jmath$ or a $\\imath$, depending on the field). Python works with complex numbers, but you have to explicitly tell the system that this is what you want to do. To calculate $\\sqrt{-9}$ for example, simply use np.sqrt(-9+0j) . In [30]: np . float64 ( 1. ) / 0. -c:1: RuntimeWarning: divide by zero encountered in double_scalars Out[30]: inf In [31]: np . float64 ( 0. ) / 0. -c:1: RuntimeWarning: invalid value encountered in double_scalars Out[31]: nan Types of Objects Most of the examples used so far have dealt with numbers. But computers work with other kinds of information as well: text, photographs, sounds, sets of data, and so on. The word type is used to refer to the kind of information. Modern computer languages support a great variety of types. It's important to know about the types of data because operators expect their input arguments to be of specific types. When you use the wrong type of input, the computer might not be able to process your command. In Python, data frames are not ‘built in' as part of the basic language, but the excellent [‘pandas'][pandas] library provides data frames and a whole slew of other functionality to researchers doing data analysis with Python. We will be learning more about ‘pandas' in future tutorials. For the purposes of starting computational statistics with Python, it's important to distinguish among three basic types: numeric positive and negative numbers, decimal and fractional numbers ( floats ), and whole numbers ( integers ) - numbers of the sort encountered so far data frames collections of data (more or less) in the form of a spreadsheet table - the ‘Computational Technique' section from chapter 2 will introduce data frames and the operators and libraries for working with data frames strings textual data - you indicate string data to the computer by enclosing the text in quotation marks (e.g., name = \"python\" ) A Note on Strings There is something a bit subtle going on in the previous command involving the string \"python\" , so look at it carefully. The purpose of the command is to create a new object, called name , which stores a little bit of text data. Notice that the name of the object is not put in quotes, but the text characters are. Whenever you refer to an object name, make sure that you don't use quotes. For example, in the following, we are first assigning the string \"python\" to the name object, and then returning (and printing automatically) the name object. In [32]: name = \"python\" name Out[32]: &apos;python&apos; If you make a command with the object name in quotes, it won't be treated as referring to an object. Instead, it will merely mean the text itself: In [33]: \"name\" Out[33]: &apos;name&apos; Similarly, if you omit the quotation marks from around the text, the computer will treat it as if it were an object name and will look for the object of that name. For instance, the following command directs the computer to look up the value contained in an object named python and insert that value into the object name : In [34]: name = python --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-34-43a69bc65ba8> in <module> () ----> 1 name = python NameError : name &apos;python&apos; is not defined As it happens, there was no object named python because it had not been defined by any previous assignment command. So, the computer generated an error. For the most part, you will not need to use vary many operators on text data when doing statistical analyses; you just need to remember to include text, such as file names, in quotation marks, \"like this\" . Reference As with all ‘Statistical Modeling: A Fresh Approach for Python' tutorials, this tutorial is based directly on material from ‘Statistical Modeling: A Fresh Approach (2nd Edition)' by Daniel Kaplan . This tutorial is based on Chapter 1: Introduction. Another useful source of information for statistics in Python is this introduction to statistics web-book by Thomas Haslwanter which is licensed under a Creative Commons Attribution-NonCommercial 3.0 Unported License. I have made an effort to keep the text and explanations consistent between the original (R-based) version and the Python tutorials, in order to keep things comparable. With that in mind, any errors, omissions, and/or differences between the two versions are mine, and any questions, comments, and/or concerns should be directed to me .","tags":"Statistical Modeling for Python","title":"A Fresh Approach using Python: Introduction"},{"url":"http://carsonfarmer.com/2013/10/ftools-is-no-more/","text":"I recently decided to drop ftools.ca , since I hadn't updated it in a very long time, and it was really just costing me money to keep a ‘dead' website up and running. Additionally, with the new QGIS plugin infrastructure , hosting my own plugins (the website's primary purpose) was no longer needed. The site has served me well for many years, and really helped get fTools (the plugin) into the QGIS core codebase. The website has served its purpose, and now that I have very little involvement with fTools and the QGIS Processing Toolbox that is poised to replace it, I'm moving on: ftools.ca is dead, long live ftools.ca ! However, now that QGIS 2.0 has rolled out, it seems that at least one part of ftools.ca is missed: my old cartogram plugin. If I have some spare time, I'll try to update the plugin to the latest and greatest QGIS 2.0 standards and upload it to the new QGIS plugins system. In the mean time, for those out there who would like to use it right away, you can get the original code from here or grab it from github . In fact, if someone is able and willing, they can grab the code from github, update it for QGIS 2.0, and submit a pull request which I will (more than likely) happily accept.","tags":"Announcement","title":"ftools is dead… long live ftools!"},{"url":"http://carsonfarmer.com/2013/10/guest-speaker-nathan-storey/","text":"Nathan Storey will be speaking to my spatial data analysis class later today, October 10th, 2013, and you are invited to attend! Nathan is a former Hunter Urban Affairs student and current Open Data Guru working with Ontodia doing GIS /open data projects for NYC . Check out PediaCities , their platform to curate, organize, and link data about cities. This will be a great opportunity to see GIS data applied to real-world problem sets here in NYC . The discussion will go approx 1 hour, beginning at 5:35 in the large lab of Hunter College, CUNY , North Building, Room 1090B. Due to the location, no refreshments will be served, but its a good opportunity to feed your brain :) Here is what Nathan (the speaker) has to say about this talk: I will be speaking to Carson Farmer's spatial data analysis class next week about open data in NYC , data discovery, and PediaCities , a data encyclopedia project I've been working on for the past 6 months.","tags":"Speaker","title":"Nathan Storey Guest Lecture"},{"url":"http://carsonfarmer.com/2013/10/reimagine-new-york-streets/","text":"Here's a TED talk from Janette Sadik-Khan, the New York City Transportation Commissioner, on how they've transformed New York City streets over the past several years. I love the Citi Bike program (especially the data), which she helped introduce, so it is interesting to hear her talk about how and why the program was started. There is also an interesting article on Sadik-Khan on the TED blog which provides some additional insight into her and the various programs she's helped develop.","tags":"Transportation","title":"Re-imagining New York Streets"},{"url":"http://carsonfarmer.com/2013/10/public-transportation-time-warp/","text":"I recently came across two extremely cool videos while preparing lectures for my transportation geography course. It is pretty cool to see the development of the regions around the transportation network while the network itself remains pretty much unchanged. Worth a quick watch! The first video depicts the London to Brighton Train Journey for three time periods. In 1953, the BBC made a point-of-view film from a London to Brighton train, 30 years later (1983) they did the same trip again, and again 30 years after that (2013). The second video is from the Vancouver SkyTrain, with older footage (1985) from BCRTC /Translink, and newer footage (2013) by Celgen Studios. You can get the original Translink footage from here .","tags":"Visualization","title":"Public transportation time warp"},{"url":"http://carsonfarmer.com/2013/09/maps-art-other-experiments/","text":"With the recent (and long anticipated) release of Quantum GIS 2.0 , there has been a lot of ‘buzz' in the open source geospatial community about all the cool new features that QGIS now boasts, and how far it has come in such a short time. I was recently inspired by such a post by Anita Graser (aka Underdark) (who is a wonderfully talented cartographer/designer) on data driven labeling in QGIS , so I thought I'd throw something together on a gray Friday afternoon to test it out. I also wanted an excuse to play around with Reveal.js slides in IPython notebook , so I produced the following slide show using the images from QGIS and some IPython magic:","tags":"Visualization","title":"Maps as Art and Other Experiments"},{"url":"http://carsonfarmer.com/2013/09/researcher-position-geoInformatics/","text":"This is just a quick note about a great opportunity for early career researchers interested in the field of geoinformatics . The Centre for GeoInformatics at the University of St Andrews in Scotland has two new early career researcher positions available to start right away. These are really great opportunities for someone in the first 4-years (full-time equivalent) of their research careers who has not yet have been awarded a doctoral degree. This is also especially good for foreign students, as the Marie Curie regulations require that candidates must not have resided or carried out his/her main activity in the UK for more than 12 months in the past 3 years. For more information, check out these links on the Centre for GeoInformatics website .","tags":"Announcement","title":"Early Stage Researcher Position in GeoInformatics"},{"url":"http://carsonfarmer.com/2013/07/essential-python-geo-libraries/","text":"Just so I don't forget, here is a list of really awesome Python libraries that I'm using these days to do lots of fun things with spatial data [ UPDATE : I've added a few more]: pandas - For data handling and munging shapely - For geometry handling cartopy - For plotting spatial data rtree - For efficiently querying spatial data nodebox-opengl - For playing around with animations statsmodels - For models and stats in Python (otherwise I'd use R) numpy - For pretty much anything that involves arrays geopy - For geolocating and things like that ipython - For a wondering interactive environment in which to play freetype-py - For converting font glyphs to polygons (odd I know…) ogr/gdal - For reading, writing, and transforming geospatial data formats pyqgis - For anything and everything GIS fiona - For making it easy to read/write geospatial data formats matplotlib - For all my plotting needs networkx - For working with networks (duh!) pelican - For blogging about all this stuff… pysal - For all your spatial econometrics needs (and more) descartes - For plotting geometries in matplotlib Based on Twitter and some of the comments below, I should also add: geographiclib - For solving geodesic problems pyshp - For reading and writing shapefiles (in pure Python) pyproj - For conversions between projections Any others I've missed?","tags":"Helpful Tip","title":"Essential Python Geospatial Libraries"},{"url":"http://carsonfarmer.com/2013/06/nyc-panel-climate-change/","text":"This was recently announced on the Hunter Geography Department website: CISC Produced Map Spearhead of Mayor Bloomberg's SIRR Announcement The New York City Panel on Climate Change ( NPCC2 ) Climate Risk Information 2013 Report was released on June 11, 2013 in conjunction with the release of the NYC Special Initiative for Rebuilding and Resiliency's ( SIRR ) report entitled \"A Stronger, More Resilient New York.\" The reports were released by NYC Mayor Michael Bloomberg during a press conference on Tuesday June 11 at the Brooklyn Navy Yard. Work by the CUNY Institute for Sustainable Cities ( CISC ) is heavily featured in the reports. Hunter College Geography Professor William Solecki and director of the CUNY Institute for Sustainable Cities, is co-chair of the NPCC2 and Lesley Patrick , CISC program manager is the CISC lead of the NPCC2 Technical Team.","tags":"Announcement","title":"New York City Panel on Climate Change"},{"url":"http://carsonfarmer.com/2013/06/new-journal-spatial-demography/","text":"I have recently joined the editorial team at Spatial Demography — a new journal outlet for demographers and others who use spatial data, methods, and theory. A bit more about the journal : Spatial Demography [ ISSN : 2164-7070 (online)] focuses on the spatial analysis of demographic processes. This cross-disciplinary work involves modern demographic data visualization, enhanced geo-referenced data availability, and spatial statistics, facilitated through full color graphics, motion video tools, and a quick time-to-publication. The journal publishes research articles, essays, research reports, data sources, computing software, teaching notes, and book reviews on a wide range of topics of interest to the social demographer. Spatial Demography is more than just another new online journal - the editors strongly promote the use of the forums as a place to engage with others on general research topics, as well as discuss specific articles from the journal and Online First area. One of the really nice thing about the forums is that they have a familiar ‘blog-feel', where users are encourage to post their thoughts about the work being done by others as well as their own work. Another thing I really like about Spatial Demography is that the Associate Editors embrace technology and sharing of ideas in a very open way. For example, Corey Sparks does a cool column/forum for the journal on ‘Software and Code' : The purpose of this forum is to highlight the tools of the trade, our methodological toolbox, if you will. With so many scientists in so many disciplines contributing to the area known as \"Spatial Demography\", we all have our old stand by routines, our tricks and our tips for new researchers. The ‘Software and Code' forum will routinely feature ‘how-to guides' for various spatial analysis techniques using primarily open source computing applications, with lots of annotated code to help others learn everything from the basics to the more advanced spatial statistics methods. A big thanks to Frank Howell and Jeremy Porter (Editors-in-Chief) for inviting me on-board. I'm excited to see where this journal goes, and I hope you'll check it out as well.","tags":"Announcement","title":"New Journal: Spatial Demography"},{"url":"http://carsonfarmer.com/2013/05/guest-speaker-june-2013/","text":"Dr. Tim Stojanovic will be coming to the Department of Geography at Hunter College , CUNY next week to give a talk entitled: \"Analysing Change in the Human Development of the World's Oceans\". The talk will be held on June 5th, 2013 at 3:00 p.m. in the Hunter North building, room 1004. The talk is hosted by the Department of Geography, and the poster is available here . Abstract The World's oceans are a frontline for sustainability, given the expansion of human activity in coasts and seas, changes brought about by human forcing factors, and changes in the global ocean-atmosphere system. Making an assessment of the extent and significance of change is a challenge for science, given the relative paucity of data and the multiple variables involved. This paper reports on a study to empirically tests notions of ‘industrialisation' and ‘colonisation' in the oceans for the first time. The approach draws on a key global spatial dataset developed by Halpern et al (2008). The methods include the combined use of Raster and R to overcome methodological challenges in analysing large spatial datasets which map the footprint of human activity in the world's oceans. The findings show that human activity in the oceans has increased by multiple factors in the most recent long term wave of economic development and show distinct spatial patterns of development. Further refinement of this assessment is required and is likely to be driven by a recently established UN Global Assessment of the State of the Marine Environment. Speaker bio Dr. Tim Stojanovic is a Lecturer (i.e. Associate Professor) in Geography and Sustainable Development at the University of St Andrews, UK . He has a PhD and BSc in Marine Geography from Cardiff University, UK . Dr Stojanovic's research interests relate to the sustainability of oceans and coasts, including work with interdisciplinary research teams in issues such as climate adaptation and ecosystem services, and social science research on effective governance. If you would like to learn more about the talk, the venue, or Dr Stojanovic's work, please contact me for further details.","tags":"Guest speaker","title":"Tim Stojanovic Guest Lecture"},{"url":"http://carsonfarmer.com/2013/05/making-the-switch-to-pelican/","text":"Welcome to the new and improved carsonfarmer.com ! If you are reading this, then you are enjoying my new, responsive static website/blog. The new site is powered by Pelican — a static website generator written in Python — and is hosted on GitHub using GitHub Pages . Most of the content on the site is written in Markdown, which makes it really easy to add headings, anchors, and all sorts of goodies to simplify writing blog posts and web-pages. The move from WordPress to Pelican was relatively painless, though there were some issues with comments and converting (some) existing posts to Markdown. I also took the opportunity to update the site, change the page structure a bit and try out a few things like adding icons ( FontAwesome ), using Twitter Bootstrap for some of the UI , and some other tweaks. To get me through the process, I took advantage of several blogs and sites dedicated to documenting the switch to Pelican: Pelican documentation (which is great) Creating A Pelican-Powered Site on GitHub Pages Pelican Guide - Moving From WordPress and Initial Setup Yes, this blog is now powered by Pelican Once I get things working, I'll also start to think about some of the points here , to make things even more responsive and readable. One of the things that I did have trouble with was getting my RSS feeds set up like it was in my WordPress site: /?feed=rss2 . For now, I'm just rerouting things to /feeds/all.rss.xml , but search engines won't recognize this, and I'm sure there is a better solution out there… any thoughts? I am still missing some things that WordPress did quite nicely, including comments (I'm now relying on Disqus for comments), site search (I've started using Tapir for this, and have implemented a cool search tool that I may turn into a Pelican plug-in if I find some time), and the plethora of plug-ins and themes available for WordPress sites. Having said that, it is relatively easy to create new themes, and adding social networking components like a Twitter feed using standard html is pretty simple.","tags":"Announcement","title":"Making the switch to Pelican"},{"url":"http://carsonfarmer.com/2013/03/marine-policy-paper/","text":"An article I worked on with Dr Tim Stojanovic , \"The development of world oceans & coasts and concepts of sustainability\", has recently been published online , with the journal Marine Policy . The article can be cited as: Stojanovic T. & Farmer C. J. Q., (2013) The development of world oceans & coasts and concepts of sustainability. Marine Policy 42 157-165 I am particularly excited about this article because it marks a major shift from any previous work that I've been involved with, and is aimed directly at the world of policy - something I've been thinking about for a while now. Additionally, the data that we used for this paper provides enumerable avenues for further research and questions, so hopefully this paper is the first in a series of collaborative works with Dr. Stojanovic… stay tuned for more to come! If you would like a copy of the paper but do not have access to the article online, please contact me and I will forward you a PDF version. The DOI for the paper is: http://dx.doi.org/10.1016/j.marpol.2013.02.005 .","tags":"Publication","title":"Paper published in Marine Policy"},{"url":"http://carsonfarmer.com/2013/03/geocomp-special-session/","text":"Special session titled \"Geocomputational landscapes and spaces\" at the International Conference on Applied Mathematics, Modeling and Computational Science ( AMMCS -2013), to be held at Wilfrid Laurier University , August 26-30, 2013. The deadline for abstracts is April 15, 2013 . Description of the session: This session focuses on new applications and theoretical developments in geocomputation. Increasingly, spatial analysis and modelling requires advanced computational techniques ( MCMC , INLA , GAs etc.) to address problems related to land and resource management. This session is devoted to current developments and advances in geocomputation for understanding landscape patterns, processes, and building spatial decision-support systems. For example, talks will explore computational solutions to problems related to multi-objective optimization of spatial configuration of land-uses, new computational approaches to spatial model development and assessment and map comparison analysis. We encourage submissions of additional presentations related to geocomputation including spatial modelling, genetic algorithm development, spatial databases, spatial representation, visualization, and spatial analysis of ‘Big Data'. Abstract submission is here , and be sure to note the special session . Please share with colleagues and feel free to contact the organizers of this session Dr Colin Robertson and Dr Steve Roberts .","tags":"Announcement","title":"Geocomputational landscapes and spaces"},{"url":"http://carsonfarmer.com/2013/02/humans-as-systems/","text":"From xkcd comes another brilliant insight into the human condition: A human is a system for converting dust billions of years ago into dust billions of years from now via a roundabout process which involves checking email a lot. And since it is Valentines day…","tags":"Funny","title":"Humans as systems"},{"url":"http://carsonfarmer.com/2013/02/will-it-python/","text":"Over the past few weeks, I've been following a really great blog by Carl Vogel . This blog has an excellent (growing) collection of Python examples based on porting code and examples from R to Python. In general, it is useful for those \"interested in the Python data analysis toolkit and its viability as an alternative to R\". Carl draws on examples from Machine Learning for Hackers by Drew Conway and John Miles White , as well as Gelman and Hill's Data Analysis Using Regression and Multilevel/Hierarchical Models . From the blog: The objective [of this blog] isn't to just make a key that translates functions and methods in R into Python equivalents. Instead, the goal is to reproduce the results and insights of the analysis in idiomatic Python […] Sometimes there will be a direct translation from a line of R to a line of Python; other times Python will suggest an altogether different approach to the problem. To make things even more useful for us, Carl has made the code, examples, and IPython notebooks available in his Github repo , which makes it really easy to work through the examples. He's also open to requests/suggestions, so if you have a good R resource you'd like to see ported to Python, maybe give him a shout? As an extra bonus, I suggest you check out Blendtec's hilarious \"Will it blend?\" ad campaign, from which Carl derived his \"Will in Python\" name and logo… Happy coding!","tags":"Python","title":"Will it Python?"},{"url":"http://carsonfarmer.com/2013/01/science-canadian-style/","text":"Recently, a colleague of mine from Wilfred Laurier University has started a website with some colleagues called RinkWatch ( http://rinkwatch.org/ ). The website is designed to track climate-change by keeping tabs on local community ice rink(s). This innovative use of Citizen Science is getting quite a bit of buzz up in Canada, but the idea has implications for any country with a culture of outdoor skating rinks. It is also a great way to get the public involved in the climate debate, and science in general! RinkWatch received a mention in the Canadian Association of Geographers' Mailing list, which I'm posting here to help spread the good word. You can also check out the full story and video here . The site, created by a group of geographers at Wilfred Laurier University, invites people to register online and record the state of their homemade ice surfaces. Researchers will use the data - when the flooding is done, how many weeks the ice is useable - to track the progression of climate change. According to Robert McLeman, one of the project's creators, it's also a chance to educate Canadians about the real-world implications of climate change, an issue that can seem abstract when discussed on TV or in the classroom. \"People appreciate the scale of the problem, but don't understand personally how it fits in with their life,\" McLeman, an associate professor of geography and environmental science, told CBC Hamilton. \"We thought this sort of provides an opportunity to connect people to environmental research literally through their own backyards.\" He said the website, which has already registered 375 rinks across Canada and the northern U.S., has spawned some unintended, but very welcome uses. Participants, he said, are sharing ice-making tips with each other, creating a richer, more interactive online community than he'd anticipated. \"People who are coming to it really just are passionate.\"","tags":"Announcement","title":"Science, Canadian Style!"},{"url":"http://carsonfarmer.com/2012/11/postdocs-lectureships-st-andrews/","text":"The Centre for GeoInformatics ( CGI ) at the University of St Andrews in Scotland has the following open positions: Lectureship in Geoinformatics , permanent position, application DL 21 Dec 2012 Postdoctoral researcher in Geoinformatics , preferably Visual Analytics/Visualization (three year fixed post, application DL 7 Dec 2012) Postdoctoral researcher in Geoinformatics (two year fixed post, application DL 21 Dec 2012) In addition, there are three lectureships open at the Department of Geography & Sustainable Development in which CGI is located: Two lectureships in Environmental Geography Lectureship in Human Geography I've just recently moved on from the CGI and St Andrews and I can tell you that it is a great place to work! The people are fun to work with, there are plenty of opportunities for collaborations and cross-disciplinary research, there are funding opportunities for geoinformatics research in the UK and EU right now, and you simply can't beat the setting - St Andrews is a beautiful coastal town on the East Coast of Scotland. If you are interested in any of the above positions, don't hesitate to contact the CGI . Additionally, the CGI would greatly appreciate if you could please forward this message to any potential candidates. You can find more information about the above positions, plus other exciting opportunities at CGI .","tags":"Announcement","title":"Postdocs and Lectureships at St Andrews"},{"url":"http://carsonfarmer.com/2012/10/phd-position-in-geoinformatics-and-housing-research/","text":"The CGI is looking for a talented PhD student to fill an interdiciplinary PhD Studentship co-funded by the Economic and Social Research Council and HOME Housing Association. Suitable candidates should apply by 23 November 2012 for a start date of 1 January 2013 . Have a look at the CGI vacancies webpage for more details. This is a great opportunity for students interested in using and developing state-of-the-art methods in geoinformatics and applying them to fundamental questions in the social housing sector.","tags":"Announcement","title":"PhD position in GeoInformatics and Housing research"},{"url":"http://carsonfarmer.com/2012/10/carson-moves-to-the-big-apple/","text":"I am very excited to announce that I will soon be moving to New York to take up a faculty position in the Department of Geography at Hunter College of the City University of New York . The plans have been in motion for quite a while, and now that my wife and I have started to pack up our stuff, I thought I'd post something here to make it ‘official'. I start full time in NY at the end of January, at which point I will be diving head first into teaching and research. I am really looking forward to engaging with the active NY open source community, teaching a whole slew of new courses, interacting more directly with students, and working with my new colleagues on all sorts of cool research projects. With that in mind, I will also be sad to leave St Andrews . The past year working in the School of Geography and Geosciences has been a great experience; I've learned a lot about the inner workings of a research centre by helping to set up the Centre for GeoInformatics and I've made a lot of great personal and professional connections. Combined, the past four years in Ireland and the UK have been very exciting, and I have had excellent opportunities to make friends, build collaborative networks, hone my research skills, carve out my own research niche, and develop a longer-term research program that should keep me busy for years to come! I'll try to continue posting the occasional blog post over the next few months, and hopefully compile a decent record of my progression from postdoc to professor (lecturer to my EU friends) along the way. In the mean time, feel free to get in touch with me if you are ever in the NY area!","tags":"Announcement","title":"Carson moves to the Big Apple"},{"url":"http://carsonfarmer.com/2012/10/manager-and-rpy2-installation-problems/","text":"Unfortunately, I haven't had much time recently to update or work on manageR , but I'm hoping that will change in the next few months… Having said that, there are quite a few people out there that have been having trouble installing manageR (and the required rpy2 ) on their system to get things working at all! I have had some individuals provide possible fixes and suggestions on how to get things working properly on various platforms, and I'm going to use this post to amalgamate them, and hopefully create a one stop post for all your rpy2 and manageR needs. I'm also hoping that people will post potential fixes in the comments to help others with more specific problems? For now, I have the following potential fix for OSX (Lion 10.7.2 at least): Update R to latest version ( binary from r-project.org ); Reinstall QGIS ( Kyngchaos installer ); Reinstall GDAL_complete ( Kyngchaos again ); Reinstall rpy2 (latter via pip ) Reboot… Apparently the (potential) problem is actually related to previous R builds, where symlinks were referring to the wrong location. This potential fix is courtesy of this Stackexchange thread Ok, so does anyone else have some suggestions to get things working on Windows? Perhaps someone out there has a Windows build they'd like to share? As far as I know (i.e., it works for me), things are working fine on Linux, but if someone else has a different experience, please share in the comments.","tags":"Helpful Tip","title":"manageR and rpy2 installation problems"},{"url":"http://carsonfarmer.com/2012/08/cartogram-updates/","text":"It seems my Olympic medals cartogram is getting a bit more attention ( Guardian data blog , and Telegraph data and graphics blog ), so I've updated a few things and wanted to highlight/explain them a bit here. Firstly, you can now explore the medal data together with population and GDP as well as without any warping to get a feel for how much things change. Secondly, in order to be able to display the map in a way that is familiar to most people (i.e., landscape style), I had to take a few liberties in terms of representation. For instance, the ‘projection' used is not area preserving (in fact, it is just the geographical coordinates), so some countries appear larger (smaller) than they should, even with the warping. We all make compromises though right? Third, because I wasn't happy with adding artificial medal counts to make my algorithm happy, I decided to create a more ‘realistic' graphic, so this time around, countries with zero medals have zero area (i.e. are removed). Related to this, because Grenada only has about 110,821 people, its per capita medal ranking is off the charts (note that I'm not using the the http://www.medalspercapita.com/ figures for the graphic)! This, coupled with its tiny size, means that my cartogram algorithm emphasises Grenada at the expense of all the other countries, making the graphic pretty boring unless you live in Grenada. As a result, I've excluded all countries with less than two medals in total (I know, I'm sorry, but it had to be done). Having said that, if you click to view the ‘normal' map, then all countries are added back, and you get medal counts for all of them.","tags":"Visualization","title":"cartogram updates"},{"url":"http://carsonfarmer.com/2012/08/cross-browser-iframe-scaling/","text":"This is just a quick post to document an annoyance (and solution) that I've recently discovered when trying to scale a webpage embedded in another page using an iframe . When trying to come up with a nice way to embed this page inside this page , I found that webkit based browsers were not behaving as they should. After a lot of fiddling about, I discovered that the following css seems to fix the issues: #wrap { width : 630px ; height : 300px ; padding : 0 ; overflow : hidden ; } #frame { - ms - zoom : 0 . 5 ; - ms - transform - origin : 0 0 ; - moz - transform : scale ( 0 . 5 ); - moz - transform - origin : 0px 50px ; - o - transform : scale ( 0 . 5 ); - o - transform - origin : 0px 50px ; - webkit - transform : scale ( 0 . 5 ); - webkit - transform - origin : 0 0 ; } #frame { width : 1230px ; height : 530px ; overflow : hidden ; } Note that if instead of -ms-zoom you use zoom , webkit browsers seem to ‘double scale' everything, which turned out to be the root of my problem. With the above tweaks, everything works fine (for now) using the following HTML : <div id= \"wrap\" > <iframe id= \"frame\" src= \"http://www.website.com/\" ></iframe> </div> Hopefully this post will save someone (or me in the future) some frustration and time. The above fix was cobbled together based on suggestions from here (see answers from Kip , lxs , and r3cgm ). Carson","tags":"Helpful Tip","title":"Cross-browser iframe scaling"},{"url":"http://carsonfarmer.com/2012/08/olympic-cartogram/","text":"The London 2012 Summer Olympics have generated quite a bit of buzz in terms of Visualizations and interesting data analysis. In fact, news sites here in the UK are doing all sorts of cools things with Olympic data, and The Guardian has an entire series devoted to Olympic data. A colleague of mine also pointed out a cool graphic on The Telegraph website, which is essentially a live cartogram of Olympic medal counts. The cartogram is basically a spatial bubble plot, with the size of the bubbles representing the number of medals obtained by each country. The location of each bubbled is based on the corresponding country's approximate geographic location. The graphic is pretty effective, and it certainly tells a clear story. I'm a big fan of these types of abstract representations of space , so I thought The Telegraph's graphic was pretty fun. Having said that, I'm always a sucker for a more ‘traditional' rubber-sheet cartogram, which is generally less abstract than a bubble plot, but can sometimes lead to dramatic results . Since I felt like the only person on the internet without their own Olympics Visualization, I decided to throw together a cartogram to visualize Olympic medal achievements. Drawing inspiration from The Telegraph graphic, I created a rubber-sheet cartogram based on an iterative warping method . The ‘live' version of the cartogram is available here (or by clicking on the image below). [ UPDATE ] If you'd like to include the map on a web page, you can now do that by including this in your HTML source: <iframe src= \"http://www.carsonfarmer.com/examples/olympic_countries/map.html\" width= 1230 height= 545\\ ></iframe> The cartgram is interactive, and was created using Python ( pandas , ujson , and shapely ) and D3.js . If I get a chance, I'll try to post the code at some stage. In the mean time, here is a bit more info about the graphic: The size of each country is based on the total number of medals that they have achieved, weighted by the type of medal (gold, silver, bronze). For example, a country with one gold medal should be approximately the same size as a country with three bronze medals. Because the algorithm attempts to maintain the topological relationships between countries, this relationship might not be perfect, but the general trend is clear: the US and China and cleaning up! [ UPDATE ] I've also added relative per capita medal counts and counts by GDP , which shrinks China down significantly… now who's winning ‘big'?! I don't have full access to the server that this website is running on, so I couldn't get things set up with regular updates, however, the entire process is now automated, so I'll try to have the code run at fairly regular intervals so that the results are relatively up-to-date I now have the maps updating every hour. In any case, have a play around and let me know what you think, and if you know of any other cool Olympics graphics or cool applications of cartograms, please let me know in the comments!","tags":"Visualization","title":"Olympic cartogram"},{"url":"http://carsonfarmer.com/2012/07/paper-published-in-environmental-monitoring-and-assessment/","text":"An article I worked on with Margaret E. Andrew, Trisalyn A. Nelson, Michael A. Wulder, George W. Hobart, and Nicholas C. Coops, \"Ecosystem classifications based on summer and winter conditions\", has recently been published on-line , with Environmental Monitoring and Assessment . For now, the article can be cited as: Andrew, M. E., T. A. Nelson, M. A. Wulder, G. W. Hobart, N. C. Coops, and C. J. Q. Farmer (in press). Ecosystem classifications based on summer and winter conditions. Environmental Monitoring and Assessment. doi:10.1007/s10661-012-2773-z If you would like a copy, but do not have access to the article, please email me and I can forward you a PDF version.","tags":"Publication","title":"Paper published in Environmental Monitoring and Assessment"},{"url":"http://carsonfarmer.com/2012/05/Visualization-featured-in-wired-magazine/","text":"A few weeks back the CGI and I were approached by Miguel Nacenta from SACHI about putting together an infographic for an article on FatFonts (also see my previous article ) to be featured on the Wired ( UK ) magazine website. Since Wired is one of my favourite magazines, I jumped at the chance! Using data that Dr Timothy Stojanovic and I are working with as part of Tim's work linking off-shore cumulative human impacts to on-shore terrestrial urbanization, I put together several infographics depicting human impacts on the oceans surrounding Edinburgh and London. One of these graphics was selected for the article, which is now available online . So go check out the article, and click on \"View Gallery\" to check out my infographic, along with several cool images from Miguel Nacenta , Uta Hinrichs , and Sheelagh Carpendale ! The infographic in the article doesn't have a title or legend per se, so I'm going to create an amended version soon with an explanation of what the values represent, and a bit of context to the data and what we are hoping to show with it. So stayed tuned for updates!","tags":"Announcement","title":"Visualization featured on wired.co.uk"},{"url":"http://carsonfarmer.com/2012/05/presentation-at-cag-2012-in-waterloo/","text":"I am giving a talk and chairing a session at the Canadian Association of Geographers ( CAG ) Annual Meeting this week in Waterloo, Ontario, Canada. My session, entitled \"Spatial Modelling\", is scheduled for Thursday at 08:30, and is supported by the CAG GIS Study Group. If you are in the neighbourhood, check out the session, and stick around for my talk: \"Spatial interaction modelling of commuting flows within local labour markets\", which is also available online (best viewed at 90% on full screen). Hope I see you there!","tags":"Announcement","title":"Presentation at CAG 2012 in Waterloo"},{"url":"http://carsonfarmer.com/2012/05/visualising-data-with-fatfonts/","text":"I recently posted an article on the CGI blog about some Visualizations that I produced with researchers from [St Andrews' Computer Human Interaction Research Group ( SACHI )][] using FatFonts , a tpographic Visualization technique developed by SACHI co-founder Miguel Nacenta and colleagues ( Uta Hinrichs , and Sheelagh Carpendale ). The initial Visualizations are now on-line, and feature flow matrices for English internal migration and commuting between Irish local labour markets . We have also produced several inforgraphics based on global oceans data that Dr Timothy Stojanovic and I are working with as part of Tim's work linking off-shore cumulative human impacts to on-shore terrestrial urbanization (more on these graphics soon). I'm still experimenting with FatFonts at this stage, but so far, I'm quite pleased with the results, and find that they offer a nice way to add beauty to my potentially boring data! Carson","tags":"Visualization","title":"Visualising data with FatFonts"},{"url":"http://carsonfarmer.com/2012/05/introducing-the-centre-for-geoinformatics/","text":"The new Centre for GeoInformatics ( CGI ) within the School of Geography and Geosciences and the Department of Geography and Sustainable Development is now an official university Research Centre at the University of St Andrews in Scotland! A few months ago, I started my new job as Research Fellow with CGI , where I am continuing to work with Prof. A. Stewart Fotheringham , who is the director of the new centre, along with several new and continuing PhD students and faculty members . Now that we have the centre officially up and running, we are quickly getting ready for our official launch next month, as well as the launch of our website (which is still ‘in development'). If you are interested in any aspect of GeoInformatics, GIScience, Geocomputation, or spatial analysis, I strongly recommend you check out our website to get a feel for what we are working on. We will also be featuring a research blog , where we will post code, ideas, thoughts, and results from our various research endeavours. Check it out, and let us know what you think!","tags":"Announcement","title":"Introducing the Centre for GeoInformatics"},{"url":"http://carsonfarmer.com/2012/04/paper-in-int-journal-applied-earth-observation-geoinformation/","text":"An article I worked on with Lex Comber and Chris Brunsdon , \"Community detection in spatial networks: Inferring land use from a planar graph of land cover objects\", has recently been published on-line , with the International Journal of Applied Earth Observation and Geoinformation . The article can be cited as: Comber, A. J., Brunsdon, C. F., and Farmer, C. J. Q. (2012) Community detection in spatial networks: Inferring land use from a planar graph of land cover objects. International Journal of Applied Earth Observation and Geoinformation , 18: 274–282 If you would like a copy, but do not have access to the article, please email me and I can forward you a PDF version.","tags":"Publication","title":"Paper published in International Journal of Applied Earth Observation and Geoinformation"},{"url":"http://carsonfarmer.com/2012/04/research-dissemination-and-interactive-visuals/","text":"One of my goals for this year is to spend more time and effort developing effective Visualizations for my various research projects, in an effort to make my research more accessible to others. This is one thing that I think many academics are particularly bad at: letting others know what they are up to, and why it might be something worth looking at. In order to avoid this pitfall, I plan to focus on producing interactive, web-based visuals suitable for a more general audience in addition to more traditional forms of research dissemination such as journals and conference papers. It is my hope that by doing this, I will be making my research more readily available to those who might actually be able to use it, and maybe even create some compelling Visualizations in the process. While I'm not quite ready to start creating full-blown interactive websites yet, I thought it might be a good idea to start with something small to get the ball rolling; so I put together an upgraded version of my previous map of visitors to www.carsonfarmer.com. I used the excellent D3 JavaScript library to re-create the visitor map, this time providing some basic interaction with the data. Most of the functionality is based on examples from the D3 website, and at this point, the map is really more of a learning tool than anything. As in my previous static map, the IP addresses were geocoded using the Data Science Toolkit API via the RDSTK R package , and all data processing and manipulation was done using R. Additionally, the colour scheme functionality comes from slide 25 of this presentation , and uses the sequential colour palettes from colorbrewer.org . There are still a few kinks to work out (like how to get the onmouseout event to work properly in Internet Explorer), and tonnes of additional features and functions could be added, so comments and suggestions are welcome. Having said that, I think the new version looks quite nice, and will likely form the basis for more complex visuals as I become more familiar with Javascipt and various other web-development tools.","tags":"Visualization","title":"Research dissemination and interactive visuals"},{"url":"http://carsonfarmer.com/2012/03/because-its-fun-to-map-stuff/","text":"Its been quite a while since my last post, and its Friday and I was feeling creative, so I decided to map something! I've been looking for an excuse to produce a nice graphic like the one Anita Graser created to represent Vienna's green-spaces. She used Quantum GIS to produce a hexagonal grid for representing the density of Viennese trees instead of the standard heat map or kernel density map, and the results are quite nice! I'm a huge fan of QGIS , but I tend to do most of my work in R, so I decided to see if I could produce something similar using R. Turns out you can, and the final results are displayed below (read on to see the full work-flow). Instead of trees, I went ahead and mapped the locations of unique visitors to http://www.carsonfarmer.com/ between 2009 and 2011: Work-flow Firstly, I downloaded the logs for www.carsonfarmer.com . I did this directly from the console, though I'm pretty sure I could have done this from R as well. Next, I needed to extract the unique IP addresses from the logs. I found this nice grep one-liner from here , which I modified to grab all unique IP addresses that ‘ GET ' something from the site: grep 'GET' access.log | cut -d ' ' -f1 | sort | uniq > ip_addresses.log To actually map the IP addresses, I obviously needed some way to convert the raw IP addresses to latitude and longitude coordinates. Enter the very nice Data Science Toolkit ( DSTK ) from Pete Warden and the very handy RDSTK R package from Ryan Elmore! Basically, the DSTK has an API that can be queried for all sorts of information useful for ‘data science' applications, and the RDSTK makes it possible to query to API directly from within R. I first heard about both these projects from the Revolution Analytics blog , where there is an article summarising Ryan Elmore's work on RDSTK , and a few other handy links. RDSTK isn't (yet?) available on CRAN , so I downloaded it directly from github: wget https://github.com/rtelmore/RDSTK/raw/master/src/RDSTK_1.0.tar.gz Then I installed it via R CMD INSTALL (note that it requires other R packages: RCurl , rjson , and plyr ): R CMD INSTALL RDSTK_1.0.tar.gz Once I had all that stuff installed and ready to go, I actually started up an R session and got working: addresses = read.table ( 'ip_addresses.log' , col.names = 'address' ) library ( RDSTK ) ? ip2coordinates The ip2coordinates function requires multiple IPs to be contained within a single string with comma-separated IP addresses, but we can only do a few IPs at a time (about 100 I think?) so I had to do this part in a loop (it probably isn't polite to slam DSTK with 1,000s of requests, so be nice!). ips = paste ( as.character ( addresses $ address [ 1 : 80 ]), collapse = ', ' ) out = ip2coordinates ( ips ) last = 80 s = c ( seq ( 160 , nrow ( addresses ), 80 ), nrow ( addresses )) for ( i in s ) { ips = paste ( as.character ( addresses $ address [( last +1 ) : i ]), collapse = ', ' ) out = rbind ( out , ip2coordinates ( ips )) last = i } Once that is done running, the next step(s) are to a) convert the returned data.frame to a SpatialPointsDataFrame , b) create a SpatialGrid based on the points, c) create a SpatialPolygons object from a hexagonal sample of the grid, and then finally d) create a SpatialPolygonsDataFrame for plotting: library ( sp ) # make the output into a Spatial* object coordinates ( out ) = ~ longitude + latitude library ( maptools ) # need this for the following function sg = Sobj_SpatialGrid ( out , maxDim = 200 , n = NULL ) $ SG hex_pts = spsample ( sg , type = 'hexagonal' , cellsize = sg @ grid @ cellsize ) hex_poly = HexPoints2SpatialPolygons ( hex_pts ) pts_poly = over ( hex_poly , out , returnList = TRUE ) pts_poly_count = sapply ( pts_poly , function ( x ) nrow ( x )) poly = SpatialPolygonsDataFrame ( poly , data.frame ( 'count' = pts_poly_count ), match.ID = FALSE ) Ok, now for some plotting! # pick some reasonable break points breaks = c ( 1.0 , 10.0 , 20.0 , 50.0 , 100.0 , 500.0 , 2000.0 ) # use RColorBrewer to get a nice blue palette library ( RColorBrewer ) # don't use the lightest colour, it looks washed out cols = brewer.pal ( 7 , \"Blues\" )[ -1 ] # plot the grid, which produces something close to our final product spplot ( poly [ poly $ count > 0 ,], col = 'white' , col.regions = cols , at = breaks , par.settings = list ( axis.line = list ( col = 'transparent' ))) I then used Inkscape to tweak the final product, adding titles and labels and modifying the colour key to look like something a bit more suited to the map at hand. In the end I had a nice map of my blog readers, and an excellent way to procrastinate on a sunny Friday afternoon!","tags":"Helpful Tip","title":"Because its fun to map stuff…"},{"url":"http://carsonfarmer.com/2011/12/environment-and-planning-a-paper-published/","text":"My latest article, \"Network-based functional regions\", has recently been published on-line , with Environment and Planning A . The article can be cited as: Farmer C J Q, Stewart Fotheringham A, 2011, \"Network-based functional regions\" Environment and Planning A 43(11) 2723 – 2741 If you would like a copy, but do not have access to the article, please email me and I can forward you a PDF version.","tags":"Publication","title":"Environment and Planning A paper published"},{"url":"http://carsonfarmer.com/2011/11/its-about-time/","text":"Well its been a long time since my last post, but I do have a relatively good reason: I was finishing up my PhD thesis. The good news is that I'm now done and graduated! I'm hoping I'll have a bit more time to blog and continue working on side-projects that I had to put on-hold while finishing up. My plan for the next few months is to finish up here in Maynooth, (unofficially) start some post-doc work, and finish/get going on several papers on my PhD research. I'm also going to try to learn Bayesian statistics, fiddle about with some visualizations I've been working on, and start getting back into QGIS and Python development again In the mean time, I've put together a fun little visualization of my PhD thesis in the form of a word-cloud. This is actually a pretty rough version, and I suspect there are a few issues with hyphenated words and things like that; but it does give a pretty good impression of what my thesis is all about, so I'll leave it at that for now. For those who might be interested (and for my own reference), the R code to generate this figure is here (requires the wordcloud and tm packages): # read in all the lines as a character vector lines <- readLines ( 'modified.txt' ) head ( lines ) library ( tm ) # text mining package library ( wordcloud ) # create a corpus object corpus <- Corpus ( DataframeSource ( data.frame ( lines ))) # now start processing the text and removing punctuation etc corpus <- tm_map ( corpus , removePunctuation ) corpus <- tm_map ( corpus , tolower ) corpus <- tm_map ( corpus , function ( x ) removeWords ( x , stopwords ( \"english\" ))) # create a term document matrix (don't really know what that is...) tdm <- TermDocumentMatrix ( corpus ) # convert to matrix m <- as.matrix ( tdm ) # count up re-occuring words v <- sort ( rowSums ( m ), decreasing = TRUE ) # create dataframe for word cloud d <- data.frame ( word = names ( v ), freq = v ) png ( \"wordcloud.png\" , width = 1280 , height = 800 ) wordcloud ( d $ word , d $ freq , c ( 8 , .3 ), 2 , 100 , TRUE , .15 , vfont = c ( \"sans serif\" , \"plain\" )) dev.off () I actually got this snippet from One R Tip A Day via R-bloggers .","tags":"Announcement","title":"It's about time…"},{"url":"http://carsonfarmer.com/2011/03/adding-direct-editing-of-geometry-fields-in-qgis/","text":"Being able to add/remove attributes isn't actually a very new feature for QGIS at this point. However, to date non of the fTools functions (Vector menu) have taken advantage of this capability. If a tool needed to create a new field in the input vector layer, it simply wrote a new version of the vector layer to disk with the additional fields added. There have been several requests to allow some tools to add/update attributes directly on the input layers, so I went ahead and created a script to test this functionality out. I've provided a copy here for anyone who would like to test it out before I add it to QGIS permanently. Basically, the script will replace/update three of the Vector menu tools, including Analysis \\> Sum line lengths , Analysis \\> Points in polygon , and Geometry tools \\> Add/Export geometry info . Here are some examples of the script's usage from the QGIS Python console: Add geometry information (assumes that the target layer in first layer in the layer-list): >>> mc = qgis . utils . iface . mapCanvas () >>> layer = mc . layer ( 0 ) >>> import add_geometry_information >>> add_geometry_information . addGeometryInformation ( vlayer ) True Count the number of points or length of lines in each polygon of an input polygon layer (assumes polygon layer is second, and point or line layer is the first in layer-list): >>> polygonLayer = mc . layer ( 1 ) >>> input Layer = mc . layer ( 0 ) >>> add_geometry_information . countFeaturesInPolygon ( polygonLayer , inputLayer ) True Note that to import the module properly (and easily), make sure it is somewhere that PyQGIS can find it, such as \\~/.qgis/python . If the layer is in editing mode, then udpates can be undone, otherwise, updates are written automatically to the provider. The countFeaturesInPolygon() function automatically recognizes if an input layer is a point or line layer, and computes the correct information accordingly (outputting a count for points, and line lengths for lines). For both functions, the last argument can be a boolean specifying whether to update selected features only ( default=False ). C","tags":"Helpful Tip","title":"Adding direct editing of geometry fields in QGIS"},{"url":"http://carsonfarmer.com/2010/11/because-its-friday/","text":"My two favorite scientific programming languages are Python and R , each for their own specific strengths. I stick with R for most of my serious stats stuff, but for everyday processing, analysis, and GUI building, Python is my modus operandi . Lately however, I've been doing more and more things in Python… even the stats stuff. When doing statistical analysis in Python, I usually use the excellent rpy2 library to communicate between Python and R. As a result, I have put together quite a few little code snippets to work with R commands in Python. Recently, I decided to put a bunch of these snippets together to create what I've called fakeR. Basically it is a simple Python script that emulates a very basic (toy) R console. The fakeR console supports multi-line commands and pretty much all regular R commands, but has no history or any nice features like that. The cool and/or handy thing about it is that it separates the input/output from the actual processing via the very cool multiprocessing Python package. Using this package, it is possible to separate the input/output and processing into two separate processes running in parallel, with communication back and forth done via a duplex (two-way) pipe. I've uploaded the script for anyone interested in having a play with it. If anyone has any ideas on how to (safely) cancel a currently running R command on the processing side, I'd be very interested to hear them. Carson","tags":"Python","title":"Because its Friday"},{"url":"http://carsonfarmer.com/2010/10/evolution-is-beautiful/","text":"Check out this video of the evolution of OpenSteetMap in Europe! It sure is cool to see how much ground a bunch of nerds with GPS units can cover! Link courtesy of slashgeo.org .","tags":"Announcement","title":"Evolution is beautiful…"},{"url":"http://carsonfarmer.com/2010/10/pgrouting-openstreetmap-and-qgis/","text":"I mentioned a few posts back that there was a great resource for downloading OpenStreetMap data, and that it was relatively easy to import osm data into PostgreSQL / PostGIS for use with pgRouting to calculate shortest paths and various other network-based operations. In this post, I'll outline the steps required to get all this up and going, and provide a quick example to show how this can be combined with QGIS to visualise the computed shortest path directly. Firstly, we need to install all the required packages. I'm assuming you already have PostgreSQL and PostGIS installed, but if not, have a look here for a quick guide to getting things set up (Note that the latest version of PostgreSQL is now 8.4). sudo apt-get install postgresql-server-dev-8.4 libboost-graph-dev If you don't have them already, you might also need tools for building the required software packages, as well as subversion and cmake. sudo apt-get install build-essential subversion cmake To be able to run the driving distance algorithms we need CGAL : sudo apt-get install libcgal* And the traveling sales person algorithm requires GAUL : wget http://downloads.sourceforge.net/gaul/gaul-devel-0.1850-0.tar.gz tar -xzf gaul-devel-0.1850-0.tar.gz cd gaul-devel-0.1850-0/ ./configure --disable-slang make sudo make install sudo ldconfig Now it's time to download, build, and install pgRouting . If you don't have subversion, or you don't want to have the latest trunk version of pgRouting , you can also download it manually . svn checkout http://pgrouting.postlbs.org/svn/pgrouting/trunk pgrouting cd pgrouting/ cmake -DWITH_TSP = ON -DWITH_DD = ON . make sudo make install Once we've got that installed and ready to go, we need to set up PostgreSQL so that it ‘trusts' local database connections. sudo gedit /etc/postgresql/8.4/main/pg_hba.conf Scroll to the bottom, and make sure you change the METHOD to ‘trust'. # TYPE DATABASE USER CIDR-ADDRESS METHOD local all all trust And now that we've made these changes, we need to restart PostgreSQL sudo /etc/init.d/postgresql-8.4 restart Next we simply create a routing database to store our data in… createdb -U postgres routing createlang -U postgres plpgsql routing …add the PostGIS functions… psql -U postgres -f /usr/share/postgresql/8.4/contrib/postgis-1.5/postgis.sql routing psql -U postgres -f /usr/share/postgresql/8.4/contrib/postgis-1.5/spatial_ref_sys.sql routing …add all the pgRouting functions… psql -U postgres -f /usr/share/postlbs/routing_core.sql routing psql -U postgres -f /usr/share/postlbs/routing_core_wrappers.sql routing psql -U postgres -f /usr/share/postlbs/routing_topology.sql routing …including the traveling salesman functions… psql -U postgres -f /usr/share/postlbs/routing_tsp.sql routing psql -U postgres -f /usr/share/postlbs/routing_tsp_wrappers.sql routing …and finally the driving distance functions. psql -U postgres -f /usr/share/postlbs/routing_dd.sql routing psql -U postgres -f /usr/share/postlbs/routing_dd_wrappers.sql routing We now have a fully working pgRouting database ready to be populated with data! So in order to do that relatively painlessly, we first install the osm2pgrouting tool, which will help us import our osm data directly into our pgRouting database with the correct structure and everything. svn checkout http://pgrouting.postlbs.org/svn/pgrouting/tools/osm2pgrouting/trunk osm2pgrouting cd osm2pgrouting/ make Once that's finished building, we can go ahead and download our osm data from http://download.geofabrik.de/osm/ . See this previous post for details. For this example, I'll be using the osm data for Ireland wget http://download.geofabrik.de/osm/europe/ireland.osm.bz2 bzip2 -d ireland.osm.bz2 Once we have that downloaded and extracted, we're ready to import the osm data into our database using the osm2pgrouting tool ./osm2pgrouting -file /home/cfarmer/Downloads/ireland.osm -conf mapconfig.xml -dbname routing -user postgres -clean Once that is finished (could take a long time) we're ready to query the network (Note that the values 52343 and 39219 represent network node ids)… psql -U postgres routing select * from shortest_path ( 'select gid as id, source::int4, target::int4, length::double precision as cost from ways' , 52343, 39219, false , false ) ; …to produce something like this: vertex_id | edge_id | cost ----------+---------+--------------------- 52343 | 78055 | 0.217641978736602 52341 | 78052 | 0.0230665826613562 52342 | 78053 | 0.0839311516838216 20390 | 28717 | 0.166809293071158 20389 | 28716 | 0.493120178133836 20388 | 28715 | 0.271165901884914 20387 | 112841 | 0.101669458767093 14183 | 22893 | 0.106433172954507 ... Assuming your database is structured as pgRouting expects (which it should be if you've used osm2pgrouting ), you can use the some of the functions which return geometries for use with other PostGIS functions: select * from dijkstra_sp ( 'ways' , 52343, 39219 ) ; id | gid | the_geom ----+--------+---------------------------- 1 | 78055 | 0105000020E610000001000... 2 | 78052 | 0105000020E610000001000... 3 | 78053 | 0105000020E610000001000... 4 | 28717 | 0105000020E610000001000... 5 | 28716 | 0105000020E610000001000... 6 | 28715 | 0105000020E610000001000... 7 | 112841 | 0105000020E610000001000... 8 | 22893 | 0105000020E610000001000... ... These queries are all fine and dandy, and can easily be used to calculate the distances of shortest paths etc, but what I really want to do is visualise this output in a GIS so I can get an idea of what these shortest paths looks like. In another previous post , I mentioned how we could visualise spatial SQL queries directly in QGIS from both the Python console, and using a handy plugin. We can do the same thing here using pgRouting to produce a lovely spatial representation of our shortest path query: And there you go, a full fledged routing library built right into our database!","tags":"Helpful Tip","title":"pgRouting, OpenStreetMap, and QGIS"},{"url":"http://carsonfarmer.com/2010/10/happy-42-day/","text":"Happy 42 day to all those nerds and geeks out there! May this day bring you the question to the ultimate answer!","tags":"Announcement","title":"Happy 42 day!"},{"url":"http://carsonfarmer.com/2010/09/adding-a-bit-of-classification-to-qgis/","text":"In my last post , I implemented several classification algorithms for quantitative data which could be used directly from the Python console in QGIS . While this was a handy addition to my PyQGIS toolkit, it wasn't quite handy enough for me, so I decided to (re)implement the same algorithms in C++ so that they could be added directly to the QGIS API . Before I did that however, I wanted to fix a few issues, and speed things up a bit, particularly for the Jenks Natural Breaks algorithm, which can be quite slow for large datasets. After porting everything over to C++, I noticed that things were still a little too slow for large datasets. My first thought was to limit the amount of data that the algorithm had to go through by taking a random sample (without replacement) and only running the algorithm on this sample. Based on trial and error, I found that about 1000 values was a good number to use, as it was still relatively fast, but generally not too small to be unrepresentative of the overall distribution. In the end, I went with using max(1000, n*0.10) for layers with more than 1000 features. Since several of my colleagues here still use ESRI products, I also decided to compare my version with the Natural Breaks algorithm in ArcMAP. I noticed right away that their version was much faster (when I didn't use the random sampling scheme), so I decided to search around for information on their implementation. Obviously ESRI 's documentation didn't explain the specifics of their algorithm, but it does produce very similar class breaks to mine (and the implementation available in R , so I was relatively confident that the main underlying algorithm was similar. I then stumbled upon this question in an ESRI forum : Does ArcMap use the Jenks-Caspall or the Fisher-Jenks algorithm for classifying data into natural breaks. I did some support.esri.com research and found that ArcView 3.x appeared to have used the Fisher-Jenks, but ArcGIS Desktop only generically talked about Jenks Optimization without eluded to what algorithm it was using. The initial response was quite good, but unfortunately, like all propriety software companies, when asked exactly how the algorithm was implemented, they responded with: That's proprietary information, along the lines of a trade secret, and so corporate policy is that we do not provide it. Bummer… I was able to figure out a few things from simply clicking around the various different options in ArcMAP, and I found that for large datasets, ArcMAP's version of the Jenks algorithm also uses sampling to reduce computation time. However, I was surprised to find that their sampling technique appears to simply sample the first x data values (where x defaults to 10,000, but can be changed by the user). Depending on how the data was created/digitised, this may not produce a statistically representative sample at all! In my opinion, it is better to use a random sampling scheme, with the constraint that both the minimum and maximum values are included in the random sample so that we don't lose values off the ends of our class intervals. Which is exactly what I've done… At the moment I'm still looking at ways to optimize my rather basic implementation. So far I use a relatively simple sampling procedure, but it might be possible to do something more ‘intelligent' here to speed things up while maintaining a statistically representative sample. Comments are more than welcome. I have also posted the diff file to the QGIS developers mailing list for evaluation, and once I tidy up a few issues I'll commit to this trunk, making it available for future versions of QGIS . Just to remind you of how important it is to carefully consider the classification scheme you use when presenting your data, here is a graphic of the 5 different classification schemes (soon to be) available in QGIS . All 5 methods are grouping the same underlying data (2007 population) into the same number of classes (5).","tags":"Announcement","title":"Adding a bit of class(ification) to QGIS …"},{"url":"http://carsonfarmer.com/2010/09/playing-around-with-classification-algorithms-python-and-qgis/","text":"Data visualization is part of my everyday work-flow. More often than not, I'm playing around with my data in a GIS to tease out interesting or informative spatial patterns, or to ensure that I'm getting the results that I'm expecting. As a result, I am constantly trying out different classification schemes to help me generalize spatial patterns, highlight outliers and/or patterns, or just plain mess around with my data . Unfortunately, QGIS (which has been my primary GIS for several years now) only has had ‘Equal Interval' and ‘Quantiles' classification algorithms implemented. While these classification schemes are no doubt useful and revealing when used in the right context, I often need something that better represents the ‘actual' distribution of values in my data. For this, I usually turn to the Jenks Optimisation (or Natural Breaks) classification. Essentially, this classification algorithm generates class intervals that minimize within group variance, and maximize between group variance. In this way, given a certain number ( k ) of classes, we arrive at an ‘optimal' classification of our data into k classes. In the past, I would import my data into R , and calculate class intervals using the very handy classInt package. However, moving data between QGIS and R, while slightly easier using my manageR tool (shameless plug!), is not optimal when all I really want to do is fiddle around with different classification schemes. So I decided to reimplement the Jenks algorithm in Python so that I could do things directly from the Python console in QGIS . Obviously I didn't really want to implement this algorithm from scratch, so I had a look at the R code from the classInterval function in the classInt package (ah open source!), as well as the handy Python script from here . Once I had the code in hand, it didn't take long to have a nice Python script ready to be run on my data directly from within QGIS . While I was at it, I also implemented a few other classification algorithms to play around with, including ‘Equal Interval', ‘Quantiles', ‘Standard Deviation', and R's ‘Pretty' algorithm. For those of you who don't know, R's pretty algorithm basically computes a sequence of about ‘n+1' equally spaced ‘round' values which cover the range of our input data, such that the class breaks are 1, 2 or 5 times a power of 10. The Python script is available here , and has a version of the pretty algorithm based on code from the labeling package.","tags":"Python","title":"Playing around with classification algorithms: Python and QGIS"},{"url":"http://carsonfarmer.com/2010/07/new-online-gis-resource/","text":"The new Geographic Information Systems Stack Exchange site is now open to the public! It's brand new, and has the potential to be an extremely valuable resource for GIS professionals, academics, enthusiasts, and just about anyone else looking for answers to GIS related questions. Check it out here: http://gis.stackexchange.com","tags":"Announcement","title":"New online GIS resource"},{"url":"http://carsonfarmer.com/2010/05/osm-data-by-country/","text":"For part of a traffic simulation project I am currently working on we need country-wide road network data for Ireland. In the past, getting decent road network data for an area this large was quite a task (not to mention expensive and time consuming), however, with OpenStreetMap we have access to this type of data instantly, and for free! In order to download full country coverage all at once, all I had to do was turn to this extremely useful site , which provides links for daily excerpts of OpenStreetMap data for any country in Europe plus several non-country regions such as the Alps region, as well as select countries outside of Europe. It currently also features special coverage of Haiti . Now that I have the OSM data downloaded, it should be relatively easy to import it into my PostGIS database using pgRouting and the osm2pgrouting import tool. More to come on this topic once I get things working nicely…","tags":"Helpful Tip","title":"OSM data by country"},{"url":"http://carsonfarmer.com/2010/04/postgis-select-statement-as-vector-layer-in-qgis/","text":"Several colleagues of mine have asked whether it is possible to visualise the results of a SELECT statement on a PostGIS database that returns spatial data in QGIS . In other words, can we map the results of something like: SELECT id, st_union ( the_geom ) FROM spatial_table GROUP BY id ; My usual answer to this in the past has been \"not yet…\", but now thanks to Giuseppe Sucameli and Jürgen E. Fischer, the answer is a resounding \"yes!\". A recent patch to QGIS trunk now makes custom Postgres queries possible via the postgres data provider. Unfortunately there is no user interface implemented to take advantage of this functionality (yet!) There is now a plugin available from the Faunalia python plugin repository called RT Sql Layer which provides a GUI for loading PostGIS SELECT statements as layer, but you can also access this handy feature via the QGIS Python console: db_conn = \"dbname='gis' host=localhost port=5432 user='cfarmer' password='xxxx'\" id_field = \"id\" table = \"(select id, st_union(the_geom) from spatial_table group by id)\" uri = \" %s key= %s table= %s (the_geom) sql=\" % ( db_conn , id_field , table ,) layer = QgsVectorLayer ( uri , \"testlayer\" , \"postgres\" ) we can then add the layer to the map canvas via: QgsMapLayerRegistry . instance () . addMapLayer ( layer ) and even query/measure it via something like: provider = layer . dataProvider () feat = QgsFeature () provider . select ([], QgsRectangle ()) provider . nextFeature ( feat ) dist = QgsDistanceArea () dist . measure ( feat . geometry ()) Just another one of the many new features being added to QGIS every day!","tags":"Helpful Tip","title":"PostGIS ‘select' statement as vector layer in QGIS"},{"url":"http://carsonfarmer.com/2010/04/parallel-bootstrapping-with-r/","text":"In a recent post , I mentioned that I was testing the stability of clusters generated from a modified network partitioning algorithm using bootstrap resampling techniques. I also mentioned that I was doing this in R, using the very nice foreach package published by REvolution Computing . To show just how nice this package is, below is a minimal example of bootstrapping a network partitioning algorithm which takes advantage of a multicore processor: library ( doMC ) library ( foreach ) library ( igraph ) # Jaccard coeficcient function (taken from package fpc) clujaccard = function ( c1 , c2 , zerobyzero = NA ) { if ( sum ( c1 ) + sum ( c2 ) - sum ( c1 & c2 ) == 0 ) out = zerobyzero else out = sum ( c1 & c2 ) / ( sum ( c1 ) + sum ( c2 ) - sum ( c1 & c2 )) return ( out ) } registerDoMC () # registers the parallel backend B = 1000 # number of bootstrap replicates to create load ( \"igraph_network.Rdata\" ) # load a previously saved network (network name: g) fg = fastgreedy.community ( g ) # compute original clustering mm = which.max ( fg $ modularity ) # find level of max modularity moc = community.to.membership ( g , fg $ merges , mm ) $ membership # get membership noc = length ( unique ( moc )) # count the number original clusters bg = g # make a copy of g for bootstrapping clusters = foreach ( i = seq ( B ), . combine = cbind ) %dopar% { E ( bg ) $ weight = sample ( E ( g ) $ weight , replace = TRUE ) # resample the edge weights fg = fastgreedy.community ( bg ) # compute bootstrap clustering mm = which.max ( fg $ modularity ) # find level of max modularity mbc = community.to.membership ( bg , fg $ merges , mm ) $ membership # get membership nbc = length ( unique ( mbc )) # count the number new clusters bootresult = c () for ( j in seq ( 0 , noc -1 )) { # for each of the original clusters... maxgamma = 0 if ( nbc > 0 ) { for ( k in seq ( 0 , nbc -1 )) { # for each of the new clusters... bv = as.vector ( mbc == k ) ov = as.vector ( moc == j ) jc = clujaccard ( ov , bv , zerobyzero = 0 ) if ( jc > maxgamma ) # if these two clusters are most similar... maxgamma = jc } } bootresult = c ( bootresult , maxgamma ) # combine results } return bootresult # return the results of this iteration (and cbind with the rest) } bootmean = apply ( clusters , 1 , mean ) # mean Jaccard coefficient for each cluster The above example might not produce great results, as it simply resamples (with replacement) the weights of all the network edges, and therefore a more sophisticated resampling regime might be warranted. Having said that, it's quite a useful example, and as you can see, the only ‘extra' bits required to make this run on multiple cores is the registerDoMC() command which simply registers the parallel backend (uses the multicore package) and the foreach ... %dopar% which tells R to run the loops in parallel. I ran a similar analysis using a different community detection algorithm on a computer with 4 cores, and was (finally) able to take full advantage of my processing power:","tags":"Helpful Tip","title":"Parallel bootstrapping  with R"},{"url":"http://carsonfarmer.com/2010/04/bootstrapping-network-partitioning-methods/","text":"My PhD research at the moment focuses on network-based algorithms for delineating functional regions (geographical regions within which a large majority of the local population seeks employment, and the majority of local employers recruit their labour). Currently I'm using a network partitioning algorithm based on modularity maximisation . I have found my results to be quite good so far, but, ‘quite good' isn't really a very scientific description of validity, so obviously some others means of validation is required. Enter bootstrap resampling! Bootstrapping can be used to assess the validity of a particular network partitioning by measuring the stability of the detected partitions (or clusters). Here, a cluster may be thought of as stable if, for example, it remains relatively invariant to random- or sampling-error and noise. In this sense, we're interested in distinguishing between clusters which reﬂect the true nature of the dataset, and those generated as a result of random effects, data uncertainties, or measurement error. The process works like this: Generate a large number of random ‘bootstrap samples' from a (directed) weighted network, Apply some network partitioning algorithm to the original network, (Re)apply the network partitioning algorithm to each bootstrap sample, For each cluster in the original network partitioning, the most similar cluster in each bootstrap replicate is found using the Jaccard coeffcient γ as a measure of similarity, and similarity is recorded, The stability of each cluster is assessed based on the mean Jaccard similarity over all resampled datasets. Once the above process is run, we get an estimate of how stable each cluster is. We can then use this information to decide which clusters to keep, and which ones need to be merged with their closest neighbour. There are several ways to specify how we resample the data. If we assume no specific structure in the dataset, regular non-parametric bootstrap resampling will work fine, however, alternative resampling strategies include: a) replacing network edge weights with noise, b) adding a small amount of noise to (a percentage of) the network edges, or c) using only a subset of the original network (i.e., generating a subgraph of the original network). I tested this process on a computer generated network with three predefined clusters using resampling strategy ( b ) above, by adding random noise to k percent of the network edges, and observed the effect of increasing levels of uncertainty by applying the resampling technique to increasing values of k . The results show just what we would expect: as more noise is added to the dataset, the stability of the detected clusters goes down. The nice bit however, is that for k <= 0.5 , the detected clusters remained relatively stable ( γ >= 0.6 ), meaning the network partitioning algorithm I was using is doing a pretty good job. Nice! This bootstrapping process is part of a paper I'm working on at the moment, and uses a geographical variant of this algorithm to detect functional regions in travel to work data. I'll post more on the algorithm and my bootstrapping implementation in R (using the very cool foreach package) here soon. References Leicht, E. A., & Newman, M. E. J. (2008). Community structure in directed networks . Physical Review Letters , 100(11), 118703. Hennig, C. (2007). Cluster-wise assessment of cluster stability . Computational Statistics & Data Analysis , 52(1), 258-271.","tags":"Networks","title":"Bootstrapping network partitioning methods"},{"url":"http://carsonfarmer.com/2010/04/why-im-not-going-to-use-mendeley/","text":"Besides the obvious: \"It's not open source!\", I'm also not making the switch from Zotero to Mendeley for my academic reference management needs due to the answer to this question on the Mendeley FAQ page: Is Mendeley free? The straight answer would be yes and no. Yes, it's free, because: Everything you get when you sign up to Mendeley is completely free and will always remain free - including the features described in What is Mendeley? No, it's not completely free, because: At a later point in time, we will expand upon the existing features and introduce additional ones for professional users —- these will be available for a (very reasonable) fee. It's also not free as in freedom …","tags":"Opinion","title":"Why I'm *not* going to use Mendeley"},{"url":"http://carsonfarmer.com/2010/04/speeding-up-geoprocessing-in-qgis/","text":"Last night I had an uncontrollable urge to make geopoprocessing in QGIS better, faster and more fun! I had come across a couple of posts ( here , here ) on the idea of a cascaded union operation, and since it has recently been added to GEOS (which QGIS uses for its geometry operations), I thought I'd give a much needed boost to the fTools union tool and related functions. This required a bit of mucking about with QgsGeometry , but in the end it really didn't take too much hacking to get things working properly. I was able to add a combineCascaded(QList<QgsGeometry*>) function to the QgsGeometry class, as well as the required Python bindings. Basically what this function does is union small subsets of the input layer, then union groups of the resulting features, and so on recursively until the final union of all features in the input list is computed. There is a nice explanation of the algorithm straight from the horses mouth here . I haven't yet committed these additions, as I'm not quite sure I like how I've done things, but just to prove how much faster things can be, here is a quick little demo that can be run from the built-in QGIS Python console: import time canvas = qgis . utils . iface . mapCanvas () layer = canvas . layer ( 0 ) # assumes target layer is fist in layer list provider = layer . dataProvider () attrs = provider . attributeIndexes () provider . select ( attrs ) geoms = [] feat = QgsFeature () # get a list of all the feature geometries in the layer while ( provider . nextFeature ( feat )): geom = QgsGeometry ( feat . geometry ()) geoms . append ( geom ) First, using the current method, which adds all the geometries together one by one: start = time . time () regular_geom = geom # start with the last geometry in the layer for geometry in geoms : regular_geom = QgsGeometry ( regular_geom . combine ( geometry )) end = time . time () total = end - start print total Secondly, using cascaded union, which uses magic to combine geometries together more efficiently. Also requires fewer lines of code! start = time . time () cascaded_geom = geom . combineCascaded ( geoms ) end = time . time () total = end - start print total When I tested this last night on the grassland.shp layer from the QGIS sample dataset the results were about 86.89 seconds for the ‘old' method, and 6.14 seconds for the cascaded union method. That's a 14.15 times speedup on a relatively small layer (about 143 features of varying complexity)! I've tested the function on both poylgons and lines so far, and it appears to work quite nicely. Eventually I'll add this to the official QGIS API so that others can take advantage of the speedup. Additionally, the other fTools functions which rely to some degree on unioning will also benefit from the extra speed, which is always a good thing.","tags":"Announcement","title":"Speeding up geoprocessing in QGIS"},{"url":"http://carsonfarmer.com/2009/11/qgis-developer-meeting-update/","text":"Last week I attended the 2009 QGIS Developers Meeting in Vienna, Austria. We all had a really good time, met many new people , and actually got a lot done in the process. There have been updates about the meeting (hackfest) on the QGIS blog, and Tim Sutton has written a few words about our progress as well. I'm not going to repeat what others have said, but I would like to give a quick update on the work that I was doing at the meeting, and show off the new geoprocessing features now available to all QGIS developers (Python and C++). My main goal for the meeting was to start/continue work on the new ‘Analysis Library' for QGIS . Basically, this was intended to be a port (to C++ from Python) of the fTools suite of tools already available in QGIS . These tools currently provide geoprocessing, geometry, and various other analysis functionality natively within QGIS . However, the Python implementation is simply a Python plugin, and so does not provide these functions to other developers hoping use/add geoprocessing capabilities to their own plugins or tools. In addition, in some cases the Python fTools functions can be quite slow, and would benefit greatly from the speed-ups afforded by a compiled language like C++. Porting these functions to C++ is still underway, and so far we have implementations for buffering, dissolving, centroids, convex hulls, layer extents in the QgsGeometryAnalyzer class, and intersections in the QgsOverlayAnalyzer class. We have set it up so that it should be relatively easy to add functions and classes to the library in the future, though I expect there will be many changes to the library along the way. We have also created Python bindings for the library, so these functions will also be available to Python developers. By doing so, it is now extremely easy to perform geoprocessing functions directly from the QGIS Python console, or from QGIS Python plugins, with only a few lines of code. The old way In the past, Python developers had to operate directly on the geometries of the layers in order to do any sort of geoprocessing. For example, to perform a (very) simple buffer, one had to use the following code in the QGIS Python console: from qgis.core import iface mc = iface . mapCanvas () # get a reference to the map canvas layer = mc . layer ( 0 ) # get a reference to the first layer in the layer list provider = layer . dataProvider () # data provider for the layer provider . select ( layer . pendingAllAttributesList (), QgsRectangle (), True , True ) # select features to buffer in_feat = QgsFeature () # empty input feature out_feat = QgsFeature () # empty output feature writer = QgsVectorFileWriter ( \"output_path.shp\" , provider . encoding (), provider . fields (), QGis . WKBPolygon , provider . crs () ) # use this to write results to disk (as shapefile) while ( provider . nextFeature ( feat )): # for each feature that we selected... geometry = feat . geometry () # grab it's geometry buffer = geometry . buffer ( 100 , 10 ) # buffer the geometry out_feat . setAttributeMap ( in_feat . attributeMap ()) # set the attributes for the output feature out_feat . setGeometry ( buffer ) # set the bufer as the output geometry writer . addFeature ( out_feat ) # write the feature to file del writer # delete/close the writer to save to disk The new way With the new QGIS Analysis Library, things are much simpler, and the same (or more complex) buffering example can be completed with only 5 lines of code in the QGIS Python Console: from qgis.core import iface # import iface (interface) from qgis.analysis import QgsGeometryAnalyzer # import (par of) the analysis library mc = iface . mapCanvas () # get a reference to the map canvas layer = mc . layer ( 0 ) # get a reference to the first layer in the layer list QgsGeometryAnalyzer () . buffer ( layer , \"output_path.shp\" , 100 , False , False , - 1 ) # perform the buffer Note: only the first two parameters in the buffer function are really required, and the additional parameters control the buffer distance, whether to use a subset (selected features) of the layer, whether to dissolve the output buffer regions, and whether to use a field (field ID ) from the input layer's attribute table as the buffer distance. Pretty nice eh? For now, the QGIS Analysis Library is only available in Trunk, however, before long these tools should be available as part of the official releases, so I hope we will start to see more Plugins taking advantage of these new capabilities in the future…","tags":"Announcement","title":"QGIS developer meeting update"},{"url":"http://carsonfarmer.com/2009/10/community-structure-in-directed-weighted-networks/","text":"Many natural and human systems can be represented as networks, including the Internet, social interactions, food webs, and transportation and communication flows. One thing that these types of networks have in common, is that they can each be represented as a series of vertices (or nodes) and edges (or links). This blog entry presents a nice description of networks, highlighting the differences between various network types (directed, undirected, weighted, unweighted, etc.). According to this paper , many networks are found to display \"community structure\", which basically refers to groupings of vertices where within -group edge connections are more dense than between -group edge connections. In order to detect and delineate these groupings, Leicht & Newman (2008) present a nice \"modularity\" optimisation algorithm which is designed to find a \"good\" division of a network by maximising $$Q = \\frac{1}{2m}s&#94;TB_s,$$ where \\(s\\) is a vector whose elements define which group each node belongs to, and \\(\\mathbf{B}\\) is the so-called modularity matrix, with elements $$B_{ij} = A_{ij} - \\frac{k_{i}&#94;{in} k_{j}&#94;{out}}{m},$$ where \\(A_{ij}\\) is an element in the adjacency matrix \\(\\mathbf{A}\\) , \\(k_{i}&#94;{in}\\) and \\(k_{j}&#94;{out}\\) are the in- and out-degrees of the vertices, and \\(m\\) is the total sum of edges in the network. In practice, this can be extended to directed networks by considering the matrix \\(\\mathbf{B} + \\mathbf{B}&#94;T\\) (for an explanation of why this is the case, see Leicht & Newman ). It is relatively straight-forward to extend the above modularity optimisation algorithm to the case of a weighted network by computing the modularity matrix using the in- and out- strength (see link to blog post above) of the vertices instead of the degree. This is similar to the concept presented in Newman (2004) , and indeed the theory of the modularity algorithm holds for this more general case (note that an unweighted network can simply be represented as a weighted network where the edge weights are all set to 1). As such, our new modularity matrix can be computed as $$B_{ij} = A_{ij} - \\frac{s_{i}&#94;{in} s_{j}&#94;{out}}{m},$$ where \\(m = \\sum_{i}s_{i}&#94;{in} = \\sum_{j} s_j&#94;{out}\\) , and \\(s\\) represents the vertex strength. As such, using the above new definition of \\(\\mathbf{B}\\) , the modularity of a directed, weighted network is computed as $$Q = \\frac{1}{4m}s&#94;{T}(\\mathbf{B}-\\mathbf{B}&#94;{T})s.$$ My current research uses a modified modularity optimisation algorithm to compute functional regions for Ireland based on a range of socio-economic variables. The goal is to provide a consistent framework for computing functional regions which are comparable across different countries and/or regions. C References Leicht, E. A. & Newman, M. E. J. (2008). Community structure in directed networks . Physical Review Letters , 100(11), 118703. Newman, M. E. J. (2004). Analysis of weighted networks . Physical Review E , 70(5), 056131. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); var location_protocol = (false) ? 'https' : document.location.protocol; if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:'; mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Networks","title":"Community structure in directed, weighted networks"},{"url":"http://carsonfarmer.com/2009/10/canadian-geographer-paper-published/","text":"My article \"Spatial-temporal patterns of snow cover in western Canada\", has now been published on-line with The Canadian Geographer . At this stage, only the proof copy is available online. The article is now officially published, and can be cited as: Farmer, C. J. Q., Nelson, T. A., Wulder, M.A. and Derksen, C. (2009). Spatial-temporal patterns of snow cover in western Canada, Canadian Geographer , 53 (4): 473-487. If you would like a copy, but do not have access to the article, please email me and I can forward you a PDF version. C","tags":"Publication","title":"Canadian Geographer paper published"},{"url":"http://carsonfarmer.com/2009/10/remote-sensing-of-environment-paper-published/","text":"My latest article, \"Identification of snow cover regimes through spatial and temporal clustering of satellite microwave brightness temperatures\", has recently been published on-line , with the journal Remote Sensing of Environment . For now, UPDATE : The article can be cited as: Farmer, C. J. Q., Nelson, T. A., Wulder, M.A. and Derksen, C. (2010). Identiﬁcation of snow cover regimes through spatial and temporal clustering of satellite microwave brightness temperatures, Remote Sensing of Environment , 114 (1): 199-210. If you would like a copy, but do not have access to the article, please email me and I can forward you a PDF version.","tags":"Publication","title":"Remote Sensing of Environment paper published"},{"url":"http://carsonfarmer.com/2009/09/introduction-to-open-source-geospatial-software/","text":"Announcing an opportunity to learn about the leading edge free and open-source technologies for desktop and web-based mapping and data analysis. This is a two day Masterclass focusing on introducing participants to the wonderful world of open source geospatial software. Check out the announcement from the Postgraduate Statistics Centre at Lancaster University.","tags":"Announcement","title":"Introduction to Open Source Geospatial Software"},{"url":"http://carsonfarmer.com/2009/09/voronoi-polygons-with-r/","text":"To create a nice bounded Voronoi polygons tessellation of a point layer in R , we need two libraries: sp and deldir . The following function takes a SpatialPointsDataFrame as input, and returns a SpatialPolygonsDataFrame that represents the Voronoi tessellation of the input point layer. voronoipolygons = function ( layer ) { require ( deldir ) crds = layer @ coords z = deldir ( crds [, 1 ], crds [, 2 ]) w = tile.list ( z ) polys = vector ( mode = 'list' , length = length ( w )) require ( sp ) for ( i in seq ( along = polys )) { pcrds = cbind ( w [[ i ]] $ x , w [[ i ]] $ y ) pcrds = rbind ( pcrds , pcrds [ 1 ,]) polys [[ i ]] = Polygons ( list ( Polygon ( pcrds )), ID = as.character ( i )) } SP = SpatialPolygons ( polys ) voronoi = SpatialPolygonsDataFrame ( SP , data = data.frame ( x = crds [, 1 ], y = crds [, 2 ], row.names = sapply ( slot ( SP , 'polygons' ), function ( x ) slot ( x , 'ID' )))) } To save the output to shapefile, simply use writeOGR from the rgdal library: library ( rgdal ) ? writeOGR","tags":"Helpful Tip","title":"Voronoi polygons with R"},{"url":"http://carsonfarmer.com/2009/09/journal-of-spatial-information-science/","text":"Check out the Journal of Spatial Information Science , a new, peer-reviewed, open-access journal with a range of well established geospatial academics on the editorial board. The Journal of Spatial Information Science ( JOSIS ) is an international, interdisciplinary, open-access journal dedicated to publishing high-quality, original research articles in spatial information science. The journal aims to publish research spanning the theoretical foundations of spatial and geographical information science, through computation with geospatial information, to technologies for geographical information use. JOSIS is run as a service to the geographic information science community, supported entirely through the efforts of volunteers. JOSIS does not aim to profit from the articles published in the journal, which are open access. Since it's run entirely by volunteers, they provide a link to become involved as a reader, reviewer, or author. Thanks to Professor Chris Brunsdon for the heads-up!","tags":"Announcement","title":"Journal of Spatial Information Science"},{"url":"http://carsonfarmer.com/2009/08/python-matlab-and-r/","text":"One project I'm working on at the moment involves exploring a dynamic extension of the Isomap algorithm for visualising constantly varying real-world road networks. Currently, we are testing out the method on a small scale simulated road network, and most of the original code (written by Laurens van der Maaten , with updates by Alexei Pozdnoukhov ), was done in Matlab. Since this work is eventually going to have to run on relatively large datasets, and probably behind the scenes on a server somewhere, we decided that Python was the way to go. The goal therefore was to reproduce the Matlab code using only Python libraries, and the fewer additional libraries required, the better. The most difficult stage in all this was to convert the Matlab code to Python code, while still remaining relatively fast and simple. The solution is of course the NumPy Python library, and nothing could have made this conversion more simple than this pdf document . It is basically a syntax conversion chart between Matlab/Octave, Python, and R… brilliant! Check out Vidar Bronken Gundersen's Mathesaurus site for this, and other useful resources for converting between different mathematical computation environments.","tags":"Helpful Tip","title":"Python, Matlab, and R"},{"url":"http://carsonfarmer.com/2009/07/foss4g-and-teaching-gis/","text":"Two quicks notes to share: Firstly, please check out this excellent introduction to GIS by Tim Sutton, Otto Dassau, and Marcelle Sutton in partnership with the Chief Directorate for Spatial Planning & Information, Department of Land Affairs, Eastern Cape, South Africa, and the Spatial Information Management Unit, Office of the Premier, Eastern Cape, South Africa. They use QGIS to present some basic GIS concepts and skills, and I particularly like their section on Coordinate Reference Systems. Secondly, don't forget to checkout the FOSS4G 2009 Free and open source software for geospatial conference in Sydney in October. There will be loads of excellent presentations and exhibitors, and the atmosphere is always very cool. I will be presenting some software that I've been developing for a while now, and will hopefully get a chance to represent QGIS there as well!","tags":"Announcement","title":"FOSS4G and teaching GIS"},{"url":"http://carsonfarmer.com/2009/07/keep-an-eye-on-long-running-processes/","text":"The other day I was loading a shapefile of approximately 11 million records into a PostGIS database (stay tuned for more on that later) and I wanted to know when shp2pgsql was done. Instead of continually checking the console, I decided to ‘watch' the process using the *nix command watch . I discovered this handy tool a while ago, and have found that for long running processes, I can use watch to notify me when the process has finished, using the following command: watch -ben 1 \"ps u -C shp2pgsql\" In this case, the three parameters b , e , and n tell watch to [b] eep if the command has a non-zero exit (in this case when shp2pgsql is no longer running), [e] xit watch if the command has a non-zero exit (again when shp2pgsql is done), and the i [n] terval (in seconds) to wait between updates (in this case 1 second). The rest of the command, ps u -C is the command that watch runs each second. In this case, it uses ps to report info on the running process, where the -C flag tells ps to report the process matching the name \"shp2pgsql\" . When shp2pgsql is no longer running, ps u -C will have a non-zero exit, and I get my beep: very handy! This can be made even more useful by changing the above command to: watch -ben 1 \"ps u -C shp2pgsql\" ; mail -s \"Process complete!\" email.address@some.one < /home/username/email_text.txt Here I've added the mail command to send me an email when watch exits (the ‘;' simply allows me to have two commands on one line). If you're really smart, you could probably have watch save important info about the running process to a file and send this with the email, but for my purposes, the above works just fine. The next step is figuring out how to make my computer text me when a long process is complete… and thanks to Will , I may be one step closer to this goal.","tags":"Helpful Tip","title":"‘ Watch' long running processes"},{"url":"http://carsonfarmer.com/2009/07/syntax-highlighting-with-pyqt/","text":"A few months ago I decided to add syntax highlighting capabilities to a piece of software that I have been working on. Since it is a PyQt based application, the obvious choice for implementing syntax highlighting was to use Qt's QSyntaxHighlighter. Unfortunately, there weren't many examples around that implemented syntax highlighting in Python, so I decided to post my own. The Python file used in this example is available here . To implement syntax highlighting, we need to subclass QSyntaxHighlighter, reimplement the highlightBlock function, and specify several highlighting rules. Generally, a rule consists of a QRegExp pattern and a QTextCharFormat instance. For this example, the syntax rules are based on the R statistical programming language. The various rules can be stored using a Python list. import sys from PyQt4.QtGui import * from PyQt4.QtCore import * class MyHighlighter ( QSyntaxHighlighter ): def __init__ ( self , parent , theme ): QSyntaxHighlighter . __init__ ( self , parent ) self . parent = parent self . highlightingRules = [] keyword = QTextCharFormat () keyword . setForeground ( Qt . darkBlue ) keyword . setFontWeight ( QFont . Bold ) keywords = QStringList ( [ \"break\" , \"else\" , \"for\" , \"if\" , \"in\" , \"next\" , \"repeat\" , \"return\" , \"switch\" , \"try\" , \"while\" ] ) for word in keywords : pattern = QRegExp ( \" \\\\ b\" + word + \" \\\\ b\" ) rule = HighlightingRule ( pattern , keyword ) self . highlightingRules . append ( rule ) MyHighlighter is the subclassed QSyntaxHighlighter class, and will contain our reimplemented highlightBlock function. The above example is for the keyword rule, which recognizes the most common R keywords. We give keyword a bold, dark blue font. For each keyword, we assign the keyword and the specified format to a HighlightingRule object (see the attached Python file) and append the object to our list of rules. We can specify further syntax rules, including reservedClasses , assignmentOperators , and numbers : reservedClasses = QTextCharFormat () reservedClasses . setForeground ( Qt . darkRed ) reservedClasses . setFontWeight ( QFont . Bold ) keywords = QStringList ( [ \"array\" , \"character\" , \"complex\" , \"data.frame\" , \"double\" , \"factor\" , \"function\" , \"integer\" , \"list\" , \"logical\" , \"matrix\" , \"numeric\" , \"vector\" ] ) for word in keywords : pattern = QRegExp ( \" \\\\ b\" + word + \" \\\\ b\" ) rule = HighlightingRule ( pattern , reservedClasses ) self . highlightingRules . append ( rule ) assignmentOperator = QTextCharFormat () pattern = QRegExp ( \"(<){1,2}-\" ) assignmentOperator . setForeground ( Qt . green ) assignmentOperator . setFontWeight ( QFont . Bold ) rule = HighlightingRule ( pattern , assignmentOperator ) self . highlightingRules . append ( rule ) number = QTextCharFormat () pattern = QRegExp ( \"[-+]?[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?\" ) pattern . setMinimal ( True ) number . setForeground ( Qt . blue ) rule = HighlightingRule ( pattern , number ) self . highlightingRules . append ( rule ) After a QSyntaxHighlighter object is created, its highlightBlock() function will be called automatically whenever it is necessary by the rich text engine, highlighting the given text block. To perform the actual formatting, the QSyntaxHighlighter class provides the setFormat function. This function operates on the text block that is passed as argument to the highlightBlock function. The specified format is applied to the text from the given start position for the given length. The formatting properties set in the given format are merged at display time with the formatting information stored directly in the document. def highlightBlock ( self , text ): for rule in self . highlightingRules : expression = QRegExp ( rule . pattern ) index = expression . indexIn ( text ) while index >= 0 : length = expression . matchedLength () self . setFormat ( index , length , rule . format ) index = text . indexOf ( expression , index + length ) self . setCurrentBlockState ( 0 ) This process is repeated until the last occurrence of the pattern in the current text block is found. For rules that apply over multiple blocks or lines, further logic is needed. For an example, see the QSynatxHighlighter documentation. In order to apply the syntax highlighter to a QTextEdit, we simply create an instance of our QSyntaxHighlighter subclass, and pass it the QTextEdit or QTextDocument that we want the syntax highlighting to be applied to, as the following test application demonstrates: class TestApp ( QMainWindow ): def __init__ ( self ): QMainWindow . __init__ ( self ) editor = QTextEdit () highlighter = MyHighlighter ( editor ) self . setCentralWidget ( editor ) self . setWindowTitle ( \"Syntax Highlighter Example\" ) Once implemented, the above example produces output like this:","tags":"Helpful Tip","title":"Syntax highlighting with PyQt"},{"url":"http://carsonfarmer.com/2009/01/r-featured-in-new-york-times/","text":"I'm sure everyone has seen this already, but I'm going to post it anyway, as I think the more exposure open-source tools get, the better off we'll all be! Check out this New York Times article which features R , the open-source statistical programming language. R now has quite an extensive range of spatial analysis options , and is the software of choice for researchers using spatial statistics and geographic information analysis.","tags":"News","title":"R featured in New York Times"},{"url":"http://carsonfarmer.com/2009/01/open-up-your-web-maps-with-openstreetmap/","text":"OpenStreetMap ( OSM ) is a project designed to create and provide free spatial data (street maps) to anyone and everyone who wants them. It is based on an open-source philosophy , and combines wiki-like user generated data, with free access , allowing users to create, edit, download, and use OSM data to their hearts content. According to the OSM website , \"the project was started because most maps you think of as free actually have legal or technical restrictions on their use, holding back people from using them in creative, productive or unexpected ways.\" There are now tones of websites and open-source software projects that incorporate OSM data, and the growing popularity of the site means that the data is only going to get better (more accurate) and bigger (more data). Essentially, OpenStreetMap contributors go out into the world with handheld GPS units, and an insatiable need to map everything around them. They track their own movements down streets and trails, and along the way they record street names, parks, towns, cities, and other points of interest (POIs). All these GPS tracks can be uploaded into the OSM database, along with place names, street names, and any other pertinent information (type of road, and/or road intersections). All the while, others can do the same thing, in the same area, adjusting the same data, making more and more accurate maps of the region. If all this free data isn't enough, OSM also processes the uploaded data, and produces detailed street-level maps which are freely available for publishing on websites. If fact, in 5 simple steps, you too can have a beautiful OSM powered map on your website. How to put an OpenStreetMap on your own site Browse the OpenStreetMap and find the area you want to map, using the zoom tools to zoom right in to your area of interest. Select the export tab. When choosing the export format, select \"Embeddable HTML \". If you want, pop in a marker symbol so everyone knows where to look… Copy the provided HTML code, and paste into your site wherever you want the map to show up. For an example, check out my contact page . As you can see, there aren't a lot of people collecting data in Maynooth… , recent work in Maynooth has created an extremely rich, usable, and downright decent map of Maynooth. Nice work Blazej Ciepluch!","tags":"Helpful Tip","title":"Open up your online maps with OpenStreetMap"},{"url":"http://carsonfarmer.com/2009/01/understanding-spatial-reference-systems/","text":"For those of you who are still unclear about what exactly a spatial reference system is, how it is used, and what it means for your data, I found a pretty good quick guide to spatial references, coordinate systems, projections, datums and ellipsoids . This article was written by Morten Nielsen (who works for ESRI ), and it does a good job of quickly and simply describing what makes up a spatial reference system, and some of the errors that people make when talking about their spatial data. Having a good grasp of this stuff is important when working with spatial data, so guides like the one above should really only be used as a quick reference to more in-depth material covering these concepts. Check out the links below if you want to learn a little bit more: An ESRI overview of map projections spatialreference.org/ Carlos A. Furuti's Map Projection Pages Knippers, R.A. - Map projections","tags":"Helpful Tip","title":"Understanding spatial reference systems"},{"url":"http://carsonfarmer.com/2008/12/gedit-the-ultimate-latex-editor/","text":"Out of the box gedit is a basic text editor, but it comes equipped with about 12 standard plugins, and another 9 readily available. In addition to this, there are a range of ‘third-party' plugins developed to do various specific tasks, such as assist you in writing and exporting LaTeX documents! First, get all the basic plugins: sudo apt-get install gedit-plugins ` and enable them in gedit by going to Edit > Preferences > Plugins , and checking the ones that you want. Second, make sure you have all the required dependencies for the actual \\(\\LaTeX\\) plugin: 1. The plugin is written in Python 2.4 and relies on PyGTK 2.4: sudo apt-get install python-gtk2 2. Ensure that you have rubber installed. It is used for automated document compiling: sudo apt-get install rubber 3. To use the DVI inverse search you need the Python bindings for D- BUS : sudo apt-get install python-dbus Third, download the latest version of the \\(\\LaTeX\\) plugin from here , and extract and copy the contained folder and a file to ~/.gnome2/gedit/plugins . You may have to create gedit/plugins if you haven't installed any other plugins yet. After that, restart gedit and activate the plugin in the settings dialog as we did with the other plugins. Now you have an editor with all sorts of handy functions, including inline spell check, code completion, tag, symbol, and character insertion, a file and document browser, and an embedded terminal, as well as tools to automatically create new \\(\\LaTeX\\) files, insert graphics, tables, and matrices, and a fantastic dialog for automatically inserting BibTeX entries. Also, if you're an R user who creates reports etc. you can use Sweave directly from gedit to embed R code in your LaTeX documents. All this in a lightweight text editor, nice! if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); var location_protocol = (false) ? 'https' : document.location.protocol; if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:'; mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Helpful Tip","title":"gedit: The ultimate LaTeX editor"},{"url":"http://carsonfarmer.com/2008/11/quick-guide-to-setting-up-postgis-database/","text":"Recently I decided to seriously start using PostGIS to manage my spatial data. As I have several projects on the go, organizing and managing my data effectively has become extremely important, and PostGIS is by far the most convenient way to do this. There is lots of documentation out there that explains in detail how to set up PostGIS, but by far the best reference I've found is from Tim Sutton's blog , mainly because he uses Ubuntu, and sudo-apt gets everything you need to have PostGIS working in minutes. Here is a link to the article , and below is a quote from his blog: Another reason I love Ubuntu - getting postgis + postgresql is really easy… sudo apt-get install postgis postgresql-8.3-postgis sudo su postgres createuser -s -d -r -l -P -E -e timlinux exit Enter prompts following above commands as needed. Now you have postgres installed and a user created. Next create an empty spatial database: createdb qgis createlang plpgsql qgis psql qgis < /usr/share/postgresql-8.3-postgis/lwpostgis.sql psql qgis < /usr/share/postgresql-8.3-postgis/spatial_ref_sys.sql Easy peasy.","tags":"Helpful Tip","title":"Quick guide to setting up a PostGIS database"},{"url":"http://carsonfarmer.com/2008/10/view-spatial-data-attribute-tables-in-r/","text":"Many GIS offer the ability to view the attribute table of a vector layer. While this is perhaps less obvious in the R environment, it is not impossible. The following command allows you to visually inspect, and change any data.frame (or other vector, matrix, etc.), including Spatial*DataFrames. invisible ( edit ( spatial_layer @ data )) Note: invisible allows you to close the viewer without filling the console with the attributes of the table. You could also use: new_data = edit ( spatial_layer @ data ) to assign changes made to the data to a new variable, or use: spatial_layer @ data = edit ( spatial_layer @ data ) or, fix ( spatial_layer @ data ) to make changes to the Spatial*DataFrame itself.","tags":"Helpful Tip","title":"View spatial data attribute tables in R"},{"url":"http://carsonfarmer.com/2008/09/r-spatial-indentify-tool/","text":"This is useful for visually exploring R spatial data such as SpatialPointDataFrames or SpatialGridDataFrames . By clicking on various features, the value at that point will be displayed. library ( rgdal ) y = readGDAL ( system.file ( \"pictures/Rlogo.jpg\" , package = \"rgdal\" )[ 1 ], band = 1 ) y.grid = y @ grid y.coords = coordinates ( y.grid ) image ( y ) identify ( x = y.coords , y = NULL , n = 1 ) where x and y refer to coordinates (in this case because y.coords contains both x and y coordinates, y can be set to NULL ), and n is the number of features to identify.","tags":"Helpful Tip","title":"R spatial indentify tool"},{"url":"http://carsonfarmer.com/2008/09/find-and-replace-multiple-files/","text":"Recently, I had to do a find and replace over several individual python files.There are plenty of scripts out there which will accomplish this, but I was interested in something simple, and preferably a single line command. After a lot of Google-ing, I ended up finding this post , which does a great job of explaining how to do this in linux. The basic command is: find . -name \"\\*.py\" -print | xargs sed -i 's/foo/bar/g' where find . -name \"*.py\" is used to find all python files (recursively) in your directory, and xargs sed -i 's/foo/bar/g' is used to replace all occurrences of ‘foo' in the files with ‘bar'. The link above gives a good explanation of each command (find, xargs, sed), and how they combine together to create this useful single line command.","tags":"Helpful Tip","title":"Find and replace multiple files"}]}