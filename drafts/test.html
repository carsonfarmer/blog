<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8">
        <title>test</title>
        <link rel="stylesheet" href="../theme/css/main.css">
                <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Carson Farmer Atom Feed" />
                        <link href="/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="Carson Farmer RSS Feed" />
                        <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.js"></script>
        <script src="../static/libs/bootstrap/js/bootstrap.min.js"></script>
        <!--<script src="http://twitter.github.io/bootstrap/assets/js/bootstrap-tooltip.js"></script>-->
        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="../">Carson Farmer  <strong>www.carsonfarmer.com</strong></a></h1>
                <form class="form-inline" id="top-search">
    <div class='input-append'>
        <input type="text" id="top-query" placeholder="Search..." />
        <button class="btn add-on" id="top-btn" type="button">
            <i class="icon-search"></i>
        </button>
<div class="dropdown pull-right" id="top-dropdown">
<a class="dropdown-toggle" id="top-toggle" role="button" data-toggle="dropdown" data-target="#" href="."></a>
<ul class="dropdown-menu pull-right" role="menu" id="top-results" aria-labelledby="top-toggle"></ul></div>
    </div>
</form>
                <div class="navbar">
                    <div class="navbar-inner">
                        <ul class="nav">
                            <li><a id="home" href="../"><i class="icon-home"></i></a></li>
                                <li class="divider-vertical"></li>
                                                                                                                            <li><a href="../about/"><i class="icon-info-sign"></i> About</a></li>
                                                                                                                                <li><a href="../curriculum-vitae/"><i class="icon-rocket"></i> Curriculum Vitae</a></li>
                                                                                                                                <li class="dropdown">
                                        <a class="dropdown-toggle" data-toggle="dropdown" id="Work-drop" role="button" href="#">
                                        <i class="icon-globe"></i> Work<i class="icon-chevron-down"></i></a>
                                        <ul class="dropdown-menu" role="menu" id="Work-menu" aria-labelledby="Work-drop">
                                                                                            <li><a href="../research/"><i class="icon-bar-chart"></i> Reserach</a></li>
                                                                                            <li><a href="../spatial-software/"><i class="icon-code"></i> Software</a></li>
                                                                                    </ul>
                                    </li>
                                                                                                                                                                                <li class="dropdown pull-right">
                                    <a class="dropdown-toggle" data-toggle="dropdown" role="button" id="social-drop" href="#">
                                    <i class="icon-group"></i> Social<i class="icon-chevron-down"></i></a>
                                    <ul class="dropdown-menu pull-right" role="menu" id="social-menu" aria-labelledby="social-drop">
                                        <li>
                                          <a href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">
                                          <i class="icon-rss"></i> atom feed</a>
                                        </li>
                                                                                <li>
                                          <a href="/feeds/all.rss.xml" type="application/rss+xml" rel="alternate">
                                          <i class="icon-rss"></i> rss feed</a>
                                        </li>
                                                                                                                            <li><a href="https://twitter.com/CarsonFarmer"><i class="icon-twitter"></i> twitter</a></li>
                                                                                    <li><a href="https://github.com/cfarmer"><i class="icon-github"></i> github</a></li>
                                                                                    <li><a href="http://about.me/carson.farmer"><i class="icon-user"></i> about.me</a></li>
                                                                                    <li><a href="http://hunter-cuny.academia.edu/CarsonFarmer"><i class="icon-book"></i> academia.edu</a></li>
                                                                                    <li><a href="http://www.linkedin.com/pub/carson-farmer/40/3bb/27/"><i class="icon-linkedin"></i> linkedin</a></li>
                                                                                    <li><a href="https://plus.google.com/108062014159451325697/about/p/pub"><i class="icon-google-plus"></i> google+</a></li>
                                                                                    <li><a href="mailto:carson.farmer@gmail.com"><i class="icon-envelope"></i> email</a></li>
                                                                            </ul>
                                </li>
                                                    </ul>
                    </div>
                </div>
        </header><!-- /#banner -->
        <section id="content" class="body">
  <article class="hentry shadow">
    <header>
      <h1 class="entry-title">
        <a href="../2013/05/test/" rel="bookmark"
           title="Permalink to test">test</a></h1>
      <a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="CarsonFarmer">Tweet</a><script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
      <abbr class="published" title="2013-05-22T12:00:00">
          <i class="icon-calendar"></i>
          Wed 22 May 2013
      </abbr>
    </header>

    <div class="entry-content">
      <h1>Chapter&nbsp;3</h1>
<hr />
<h2>Opening the black box of&nbsp;<span class="caps">MCMC</span></h2>
<p>The previous two chapters hid the inner-mechanics of PyMC, and more generally
Monte Carlo Markov Chains (<span class="caps">MCMC</span>), from the reader. The reason for including this
chapter is three-fold. The first is that any book on Bayesian inference must
discuss <span class="caps">MCMC</span>. I cannot fight this. Blame the statisticians. Secondly, knowing
the process of <span class="caps">MCMC</span> gives you insight into whether your algorithm has converged.
(Converged to what? We will get to that) Thirdly, we&#8217;ll understand <em>why</em> we are
returned thousands of samples from the positerior as a solution, which at first
thought can be&nbsp;odd.</p>
<h3>The Bayesian&nbsp;landscape</h3>
<p>When we setup a Bayesian inference problem with $N$ unknowns, we are implicitly
creating an $N$ dimensional space for the prior distributions to exist in.
Associated with the space is an additional dimension, which we can describe as
the <em>surface</em>, or <em>curve</em>, that sits ontop of the space, that reflects the
<em>prior probability</em> of a particular point. The surface on the space is defined
by our prior distributions. For example, if we have two unknowns $p_1$ and
$p_2$, and priors for both are $\text{Uniform}(0,5)$, the space created is a
square of length 5 and the surface is a flat plane that sits ontop of the square
(representing that every point is equally&nbsp;likely).</p>
<p>In[24]:</p>
<div class="highlight"><pre><span class="c">%pylab inline</span>
<span class="n">import</span> <span class="n">scipy</span><span class="p">.</span><span class="n">stats</span> <span class="n">as</span> <span class="n">stats</span>
<span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>

<span class="n">import</span> <span class="n">matplotlib</span><span class="p">.</span><span class="n">pyplot</span> <span class="n">as</span> <span class="n">plt</span>
<span class="n">from</span> <span class="n">mpl_toolkits</span><span class="p">.</span><span class="n">mplot3d</span> <span class="n">import</span> <span class="n">Axes3D</span>

<span class="n">fig</span> <span class="p">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">x</span> <span class="p">=</span> <span class="n">y</span> <span class="p">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span> <span class="mi">100</span> <span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="p">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">uni_x</span> <span class="p">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">uniform</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span><span class="n">loc</span><span class="p">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="p">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">uni_y</span> <span class="p">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">uniform</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="p">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="p">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">M</span> <span class="p">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">dot</span><span class="p">(</span> <span class="n">uni_x</span><span class="p">[:,</span><span class="n">None</span><span class="p">],</span> <span class="n">uni_y</span><span class="p">[</span><span class="n">None</span><span class="p">,:]</span> <span class="p">)</span>
<span class="n">im</span> <span class="p">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">interpolation</span><span class="p">=</span><span class="s">&#39;none&#39;</span><span class="p">,</span> <span class="n">origin</span><span class="p">=</span><span class="s">&#39;lower&#39;</span><span class="p">,</span>
                <span class="n">cmap</span><span class="p">=</span><span class="n">cm</span><span class="p">.</span><span class="n">jet</span><span class="p">,</span> <span class="n">vmax</span><span class="p">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmin</span> <span class="p">=</span> <span class="o">-</span><span class="p">.</span><span class="mi">15</span><span class="p">,</span> <span class="n">extent</span><span class="p">=(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span> &quot;<span class="n">Landscape</span> <span class="n">formed</span> <span class="n">by</span> <span class="n">Uniform</span> <span class="n">priors</span><span class="p">.</span>&quot; <span class="p">)</span>

<span class="n">ax</span> <span class="p">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">,</span> <span class="n">projection</span><span class="p">=</span><span class="s">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">cmap</span><span class="p">=</span><span class="n">cm</span><span class="p">.</span><span class="n">jet</span><span class="p">,</span> <span class="n">vmax</span><span class="p">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmin</span> <span class="p">=</span> <span class="o">-</span><span class="p">.</span><span class="mi">15</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">view_init</span><span class="p">(</span> <span class="n">azim</span> <span class="p">=</span> <span class="mi">390</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span> &quot;<span class="n">Uniform</span> <span class="n">prior</span> <span class="n">landscape</span><span class="p">;</span> <span class="n">alternate</span> <span class="n">view</span>&quot;<span class="p">);</span>
</pre></div>


<p><a href="_fig_300.png">!image</a></p>
<p>Alternatively, if the two priors are $\text{Exp}(3)$ and $\text{Exp}(10)$, then
the space is all postive numbers on the 2-D plane, and the surface induced by
the priors looks like a water fall that starts at the point (0,0) and flows over
the positive&nbsp;numbers.</p>
<p>The plots below visualize this. The more dark red the color, the more prior
probability is assigned to that location. Conversely, areas with darker blue
represent that our priors assign very low probability to that&nbsp;location.</p>
<p>In[22]:</p>
<div class="highlight"><pre><span class="n">figsize</span><span class="p">(</span> <span class="mf">12.5</span><span class="p">,</span><span class="mi">5</span> <span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>

<span class="n">exp_x</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">expon</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">exp_y</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">expon</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span> <span class="n">exp_x</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="n">None</span><span class="p">],</span> <span class="n">exp_y</span><span class="p">[</span><span class="n">None</span><span class="p">,</span><span class="o">:</span><span class="p">]</span> <span class="p">)</span>
<span class="n"><span class="caps">CS</span></span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">contour</span><span class="p">(</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">M</span> <span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="err">&#39;</span><span class="n">none</span><span class="err">&#39;</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="err">&#39;</span><span class="n">lower</span><span class="err">&#39;</span><span class="p">,</span>
                <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="p">.</span><span class="n">jet</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="cp">#plt.xlabel( &quot;prior on $p_1$&quot;)</span>
<span class="cp">#plt.ylabel( &quot;prior on $p_2$&quot;)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span> <span class="s">&quot;$Exp(3), Exp(10)$ prior landscape&quot;</span> <span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="err">&#39;</span><span class="mi">3</span><span class="n">d</span><span class="err">&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="p">.</span><span class="n">jet</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">view_init</span><span class="p">(</span> <span class="n">azim</span> <span class="o">=</span> <span class="mi">390</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span> <span class="s">&quot;$Exp(3), Exp(10)$ prior landscape; </span><span class="se">\n</span><span class="s">alternate view&quot;</span><span class="p">)</span>
</pre></div>


<p><a href="_fig_500.png">!image</a></p>
<p>These are simple examples in 2D space, where our brains can understand surfaces
well. In practice, spaces and surfaces generated by our priors can be much
higher&nbsp;dimensional.</p>
<p>If these surfaces describe our <em>prior distributions</em> on the unknowns, what
happens to our space after we incorporate our observed data $X$? The data $X$
does not change the space, but it changes the surface of the space by <em>pulling
and stretching the fabric of the prior surface</em> to reflect where the true
parameters likely live. More data means more pulling and stretching, and our
original shape becomes mangled or insignificant compared to the newly formed
shape. Less data, and our original shape is more present.  Regardless, the
resulting surface describes the <em>posterior distribution</em>.</p>
<p>Again I must stress that it is, unfortunately, impossible to visualize this in
large dimensions. For two dimensions, the data essentially <em>pushes up</em> the
original surface to make <em>tall mountains</em>. The tendency of the observed data to
<em>push up</em> the posterior probability in certain areas is checked by the prior
probability distribution, so that less prior probability means more resistance.
Thus in the double-exponential prior case above, a mountain (or multiple
mountains) that might erupt near the (0,0) corner would be much higher than
mountains that erupt closer to (5,5), since there is more resistance (low prior
probability) near (5,5). The peak reflects the posterior probability of where
the true parameters are likely to be found. Importantly, if the prior has
assigned a probability of 0, then no posterior probability will be assigned&nbsp;there.</p>
<p>Suppose the priors mentioned above represent different parameters $\lambda$ of
two Poisson distributions. We observe a few data points and visualize the new&nbsp;landscape:</p>
<p>In[394]:</p>
<div class="highlight"><pre><span class="cp">### create the observed data</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">1</span> <span class="err">#</span><span class="n">sample</span> <span class="n">size</span> <span class="n">of</span> <span class="n">data</span> <span class="n">we</span> <span class="n">observe</span>
<span class="n">p_1_true</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">p_2_true</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(</span> <span class="p">[</span> <span class="n">stats</span><span class="p">.</span><span class="n">poisson</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span> <span class="n">p_1_true</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> \
                            <span class="n">stats</span><span class="p">.</span><span class="n">poisson</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span> <span class="n">p_2_true</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="mi">1</span><span class="p">))],</span>  <span class="n">axis</span><span class="o">=</span><span class="mi">1</span> <span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">.01</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span> <span class="mi">100</span> <span class="p">)</span>
<span class="n">likelihood_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span> <span class="p">[</span><span class="n">stats</span><span class="p">.</span><span class="n">poisson</span><span class="p">.</span><span class="n">pmf</span><span class="p">(</span> <span class="n">data</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">_x</span><span class="p">)</span> <span class="k">for</span> <span class="n">_x</span> <span class="n">in</span> <span class="n">x</span><span class="p">]).</span><span class="n">prod</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">likelihood_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span> <span class="p">[</span><span class="n">stats</span><span class="p">.</span><span class="n">poisson</span><span class="p">.</span><span class="n">pmf</span><span class="p">(</span> <span class="n">data</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">_y</span><span class="p">)</span> <span class="k">for</span> <span class="n">_y</span> <span class="n">in</span> <span class="n">y</span><span class="p">]).</span><span class="n">prod</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span> <span class="n">likelihood_x</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="n">None</span><span class="p">],</span> <span class="n">likelihood_y</span><span class="p">[</span><span class="n">None</span><span class="p">,</span><span class="o">:</span><span class="p">]</span> <span class="p">)</span>
</pre></div>


<p>In[395]:</p>
<div class="highlight"><pre><span class="n">figsize</span><span class="p">(</span> <span class="mf">12.5</span><span class="p">,</span><span class="mi">12</span> <span class="p">)</span>

<span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>
<span class="n">uni_x</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">uniform</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">uni_y</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">uniform</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span> <span class="n">uni_x</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="n">None</span><span class="p">],</span> <span class="n">uni_y</span><span class="p">[</span><span class="n">None</span><span class="p">,</span><span class="o">:</span><span class="p">]</span> <span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="err">&#39;</span><span class="n">none</span><span class="err">&#39;</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="err">&#39;</span><span class="n">lower</span><span class="err">&#39;</span><span class="p">,</span>
                <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="p">.</span><span class="n">jet</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmin</span> <span class="o">=</span> <span class="o">-</span><span class="mf">.15</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span> <span class="n">p_1_true</span><span class="p">,</span> <span class="n">p_2_true</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s">&quot;k&quot;</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span> <span class="s">&quot;Landscape formed by Uniform priors on $p_1, p_2$.&quot;</span> <span class="p">)</span>

<span class="n">subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">contour</span><span class="p">(</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">M</span><span class="o">*</span><span class="n">L</span> <span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">M</span><span class="o">*</span><span class="n">L</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="err">&#39;</span><span class="n">none</span><span class="err">&#39;</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="err">&#39;</span><span class="n">lower</span><span class="err">&#39;</span><span class="p">,</span>
                <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="p">.</span><span class="n">jet</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span> <span class="s">&quot;Landscape warped by data observations;</span><span class="se">\n</span><span class="s"> Uniform priors on $p_1, p_2$.&quot;</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span> <span class="n">p_1_true</span><span class="p">,</span> <span class="n">p_2_true</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s">&quot;k&quot;</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>
<span class="n">exp_x</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">expon</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">exp_y</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">expon</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span> <span class="n">exp_x</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="n">None</span><span class="p">],</span> <span class="n">exp_y</span><span class="p">[</span><span class="n">None</span><span class="p">,</span><span class="o">:</span><span class="p">]</span> <span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">contour</span><span class="p">(</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">M</span> <span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="err">&#39;</span><span class="n">none</span><span class="err">&#39;</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="err">&#39;</span><span class="n">lower</span><span class="err">&#39;</span><span class="p">,</span>
                <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="p">.</span><span class="n">jet</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span> <span class="n">p_1_true</span><span class="p">,</span> <span class="n">p_2_true</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s">&quot;k&quot;</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span> <span class="s">&quot;Landscape formed by Exponential priors on $p_1, p_2$.&quot;</span> <span class="p">)</span>

<span class="n">subplot</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">contour</span><span class="p">(</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">M</span><span class="o">*</span><span class="n">L</span> <span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">M</span><span class="o">*</span><span class="n">L</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="err">&#39;</span><span class="n">none</span><span class="err">&#39;</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="err">&#39;</span><span class="n">lower</span><span class="err">&#39;</span><span class="p">,</span>
                <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="p">.</span><span class="n">jet</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span> <span class="n">p_1_true</span><span class="p">,</span> <span class="n">p_2_true</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s">&quot;k&quot;</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span> <span class="s">&quot;Landscape warped by data observations;</span><span class="se">\n</span><span class="s"> Exponential priors on \</span>
<span class="s">$p_1, p_2$.&quot;</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">);</span>
</pre></div>


<p>Out[395]:</p>
<div class="highlight"><pre><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>


<p><a href="_fig_801.png">!image</a></p>
<p>The plot on the left is the deformed landscape with the $\text{Uniform}(0,5)$
priors, and the plot on the right is the deformed landscape with the exponential
priors. Notice that the posterior landscapes look different from one another,
though the data observed is identical in both cases. The reason is as follows.
The exponential-prior landscape, on the right, puts very little posterior weight
on values in the upper right corner: this is because <em>the prior does not put
much weight there</em>, whereas the uniform-prior landscape is happy to put
posterior weight there. Also, the highest-point, corresponding the the darkest
red, is biased towards (0,0) in the exponential case, which is the result from
the exponential prior putting more prior weight in the (0,0)&nbsp;corner.</p>
<p>The black dot represents the true parameters. Even with 1 sample point, the
mountains attempts to contain the true parameter. Of course, inference with a
sample size of 1 is incredibly naive, and choosing such a small sample size was
only illustrative. Try observing how to mountain changes as you vary the sample&nbsp;size.</p>
<h3>Exploring the landscape using the&nbsp;<span class="caps">MCMC</span></h3>
<p>We should explore the deformed posterior space generated by our prior surface
and observed data to find the posterior mountain. However, we cannot naively
search the space: any computer scientist will tell you that traversing
$N$-dimensional space is exponentially difficult in $N$: the size of the space
quickly blows-up as we increase $N$ (see <a href="http://en.wikipedia.org/wiki/Curse_of_dimensionality">the curse of
dimensionality</a> ). What
hope do we have to find these hidden mountains? The idea behind <span class="caps">MCMC</span> is to
perform an intelligent search of the space. To say &#8220;search&#8221; implies we are
looking for a particular object, which is perhaps not an accurate description of
what <span class="caps">MCMC</span> is doing. Recall: <span class="caps">MCMC</span> returns <em>samples</em> from the posterior
distribution, not the distribution itself. Stretching our mountainous analogy to
its limit, <span class="caps">MCMC</span> performs a task similar to repeatedly asking  &#8220;How likely is
this pebble I found to be from the mountain I am searching for?&#8221;, and completes
its task by returning thousands of accepted pebbles in hopes of reconstructing
the original mountain. In <span class="caps">MCMC</span> and PyMC lingo, the returned sequence of
&#8220;pebbles&#8221; are the samples, more often called the <em>traces</em>.</p>
<p>When I say <span class="caps">MCMC</span> intelligently searches, I mean <span class="caps">MCMC</span> will <em>hopefully</em> converge
towards the areas of high posterior probability. <span class="caps">MCMC</span> does this by exploring
nearby positions and moving into areas with higher probability. Again, perhaps
&#8220;converge&#8221; is not an accurate term to describe <span class="caps">MCMC</span>&#8217;s progression. Converging
usually implies moving towards a point in space, but <span class="caps">MCMC</span> moves towards a
<em>broader area</em> in the space and randomly walks in that area, picking up samples
from that&nbsp;area.</p>
<p>At first, returning thousands of samples to the user might sound like being an
inefficient way to describe the posterior distributions. I would argue that this
is extremely efficient. Consider the alternative&nbsp;possibilities::</p>
<ol>
<li>Returning a mathematical formula for the &#8220;mountain ranges&#8221; would involve
describing a N-dimensional surface with arbitrary peaks and&nbsp;valleys.</li>
<li>Returning the &#8220;peak&#8221; of the landscape, while mathematically possible and a
sensible thing to do as the highest point corresponds to most probable estimate
of the unknowns, ignores the shape of the landscape, which we have previously
argued is very important in determining posterior confidence in&nbsp;unknowns.</li>
</ol>
<p>Besides computational reasons, likely the strongest reason for returning samples
is that we can easily use <em>The Law of Large Numbers</em> to solve otherwise
intractable problems. I postpone this discussion for the next chapter. With the
thousands of samples, we can reconstruct the posterior surface by organizing
them in a&nbsp;histogram.</p>
<h3>Algorithms to perform&nbsp;<span class="caps">MCMC</span></h3>
<p>There is a large family of algorithms that perform <span class="caps">MCMC</span>. Most of these
algorithms can be expressed at a high level as follows: (Mathematical details
can be found in the appendix.&nbsp;)</p>
<div class="highlight"><pre><span class="mf">1.</span> <span class="n">Start</span> <span class="n">at</span> <span class="n">current</span> <span class="n">position</span><span class="p">.</span>
<span class="mf">2.</span> <span class="n">Propose</span> <span class="n">moving</span> <span class="n">to</span> <span class="n">a</span> <span class="n">new</span> <span class="n">position</span> <span class="p">(</span><span class="n">investigate</span> <span class="n">a</span> <span class="n">pebble</span> <span class="n">near</span> <span class="n">you</span><span class="p">).</span>
<span class="mf">3.</span> <span class="n">Accept</span><span class="o">/</span><span class="n">Reject</span> <span class="n">the</span> <span class="n">new</span> <span class="n">position</span> <span class="n">based</span> <span class="n">on</span> <span class="n">the</span> <span class="n">position</span><span class="err">&#39;</span><span class="n">s</span> <span class="n">adherence</span> <span class="n">to</span> <span class="n">the</span>
</pre></div>


<p>data
    and prior distributions (ask if the pebble likely came from the mountain).
    4.  -  If you accept: Move to the new position. Return to Step 1.
        -  Else: Do not move to new position. Return to Step 1.
    5. After a large number of iterations, return the&nbsp;positions.</p>
<p>This way we move in the general direction towards the regions where the
posterior distributions exist, and collect samples sparingly on the journey.
Once we reach the posterior distribution, we can easily collect samples as they
likely all belong to the posterior&nbsp;distribution.</p>
<p>If the current position of the <span class="caps">MCMC</span> algorithm is in an area of extremely low
probability, which is often the case when the algorithm begins (typically at a
random location in the space), the algorithm will move in positions <em>that are
likely not from the posterior</em> but better than everything else nearby. Thus the
first moves of the algorithm are not reflective of the&nbsp;posterior.</p>
<p>In the above algorithm&#8217;s pseudocode, notice that only the current position
matters (new positions are investigated only near the current position). We can
describe this property as <em>memorylessness</em>, i.e. the algorithm does not care
<em>how</em> it arrived at it&#8217;s current position, only that it is&nbsp;there.</p>
<h3>Other approximation solutions to the&nbsp;posterior</h3>
<p>Besides <span class="caps">MCMC</span>, there are other procedures available for determining the posterior
distributions. A <a href="http://en.wikipedia.org/wiki/Laplace's_method">Laplace
approximation</a> is an
approximation of the posterior using simple functions. A more advanced method is
<a href="http://en.wikipedia.org/wiki/Variational_Bayesian_methods">Variational Bayes</a>.</p>
<h5>Example: Unsupervised Clustering using a Mixture&nbsp;Model</h5>
<p>Suppose we are given the following&nbsp;dataset:</p>
<p>In[112]:</p>
<div class="highlight"><pre><span class="n">figsize</span><span class="p">(</span> <span class="mf">12.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">loadtxt</span><span class="p">(</span> <span class="s">&quot;data/mixture_data.csv&quot;</span><span class="p">,</span>  <span class="n">delimiter</span><span class="o">=</span><span class="s">&quot;,&quot;</span> <span class="p">)</span>

<span class="n">hist</span><span class="p">(</span> <span class="n">data</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>  <span class="n">color</span> <span class="o">=</span> <span class="s">&quot;k&quot;</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;Histogram of the dataset&quot;</span> <span class="p">)</span>
<span class="n">print</span> <span class="n">data</span><span class="p">[</span> <span class="o">:</span><span class="mi">10</span> <span class="p">],</span> <span class="s">&quot;...&quot;</span>
</pre></div>


<div class="highlight"><pre><span class="p">[</span> <span class="mf">115.85679142</span>  <span class="mf">152.26153716</span>  <span class="mf">178.87449059</span>  <span class="mf">162.93500815</span>  <span class="mf">107.02820697</span>
  <span class="mf">105.19141146</span>  <span class="mf">118.38288501</span>  <span class="mf">125.3769803</span>   <span class="mf">102.88054011</span>  <span class="mf">206.71326136</span><span class="p">]</span> <span class="p">...</span>
</pre></div>


<p><a href="_fig_1201.png">!image</a></p>
<p>What does the data suggest? It appears the data has a bimodal form, that is, it
appears to have two peaks, one near 120 and the other near 200. Perhaps there
are <em>two clusters</em> within this&nbsp;dataset.</p>
<p>This dataset is a good example of the data-generation modeling technique from
last chapter. We can propose <em>how</em> the data might have been created. I suggest
the following data generation&nbsp;algorithm:</p>
<ol>
<li>For each data point, choose cluster 1 with probability $p$, else choose
cluster&nbsp;2.</li>
<li>Draw a random variable from a Normal distribution with parameters $\mu_i$ and
$\sigma_i$ where $i$ was chosen in step&nbsp;1.</li>
<li>Repeat.</li>
</ol>
<p>This algorithm would create a similar effect as the observed dataset, so we
choose this as our model. Of course, we do not know $p$ or the parameters of the
Normal distributions. Hence we must infer, or <em>learn</em>, these&nbsp;unknowns.</p>
<p>Denote the Normal distributions $\text{Nor}_0$ and $\text{Nor}_1$ (having
variables&#8217; index start at 0 is just Pythonic). Both currently have unknown mean
and standard deviation, denoted $\mu_i$ and $\sigma_i, \; i =0,1$ respectively.
A specific data point can be from either $\text{Nor}_0$ or $\text{Nor}_1$, and
we assume that the data point is assigned to $\text{Nor}_0$ with probability&nbsp;$p$.</p>
<p>An appropriate way to assign data points to clusters is to use a PyMC
<code>Categorical</code> stochastic variable. Its parameter is a $k$-length array of
probabilities that must sum to one and its <code>value</code> attribute is a integer
between 0 and $k-1$ randomly chosen according to the crafted array of
probabilities. (In our case $k=2$) <em>A priori</em>, we do not know what the
probability of assignment to cluster 1 is, so we create a uniform variable over
0,1 to model this. Call this <code>p</code>. Thus the probability array we enter into the
<code>Categorical</code> variable is <code>[p, 1-p]</code>.</p>
<p>In[93]:</p>
<div class="highlight"><pre><span class="n">import</span> <span class="n">pymc</span> <span class="n">as</span> <span class="n">mc</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">mc</span><span class="p">.</span><span class="n">Uniform</span><span class="p">(</span> <span class="s">&quot;p&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">assignment</span> <span class="o">=</span> <span class="n">mc</span><span class="p">.</span><span class="n">Categorical</span><span class="p">(</span><span class="s">&quot;assignment&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">],</span> <span class="n">size</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">)</span> 
<span class="n">print</span> <span class="s">&quot;prior assignment, with p = %.2f:&quot;</span><span class="o">%</span><span class="n">p</span><span class="p">.</span><span class="n">value</span>
<span class="n">print</span> <span class="n">assignment</span><span class="p">.</span><span class="n">value</span><span class="p">[</span> <span class="o">:</span><span class="mi">10</span> <span class="p">],</span> <span class="s">&quot;...&quot;</span>
</pre></div>


<div class="highlight"><pre><span class="n">prior</span> <span class="n">assignment</span><span class="p">,</span> <span class="n">with</span> <span class="n">p</span> <span class="o">=</span> <span class="mf">0.45</span><span class="o">:</span>
<span class="p">[</span><span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">1</span><span class="p">]</span> <span class="p">...</span>
</pre></div>


<p>Looking at the above dataset, I would guess that the standard deviations of the
two Normals are different. To maintain ignorance of what the standard deviations
might be, we will initially model them as uniform on 0 to 100. Really we are
talking about $\tau$, the <em>precision</em> of the Normal distribution, but it is
easier to think in terms of standard deviation. Our PyMC code will need to
transform our standard deviation into precision by the&nbsp;relation:</p>
<p>$$ \tau = \frac{1}{\sigma^2}&nbsp;$$</p>
<p>In PyMC, we can do this in one step by&nbsp;writing:</p>
<div class="highlight"><pre><span class="n">taus</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">mc</span><span class="p">.</span><span class="n">Uniform</span><span class="p">(</span> <span class="s">&quot;stds&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span> <span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
</pre></div>


<p>Notice that we specified <code>size=2</code>: we are modelling both $\tau$s as a single
PyMC variable. Note that is does not induce a necessary relationship between the
two $\tau$s, it is simply for&nbsp;succinctness.</p>
<p>We also need to specify priors on the centers of the clusters. The centers are
really the $\mu$ parameters in this Normal distributions. Their priors can be
modeled by a Normal distribution. Looking at the data, I have an idea where the
two centers might be &mdash; I would guess somewhere around 120 and 190
respectively, though I am not very confident in these eyeballed estimates. Hence
I will set $\mu_0 = 120, \mu_1 = 190$ and $\sigma_{0,1} = 10$ (recall we enter
the $\tau$ parameter, so enter $1/\sigma^2 = 0.01$ in the PyMC&nbsp;variable.)</p>
<p>In[94]:</p>
<div class="highlight"><pre><span class="nx">taus</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="nx">mc</span><span class="p">.</span><span class="nx">Uniform</span><span class="p">(</span> <span class="s2">&quot;stds&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="nx">size</span><span class="o">=</span> <span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> 
<span class="nx">centers</span> <span class="o">=</span> <span class="nx">mc</span><span class="p">.</span><span class="nx">Normal</span><span class="p">(</span> <span class="s2">&quot;centers&quot;</span><span class="p">,</span> <span class="cp">[</span><span class="mi">120</span><span class="p">,</span> <span class="mi">190</span><span class="cp">]</span><span class="p">,</span> <span class="cp">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="cp">]</span><span class="p">,</span> <span class="nx">size</span> <span class="o">=</span><span class="mi">2</span> <span class="p">)</span>

<span class="s2">&quot;&quot;&quot;</span>
<span class="s2">The below determinsitic functions map a assingment, in this case 0 or 1,</span>
<span class="s2">to a set of parameters, located in the (1,2) arrays `taus` and `centers.`</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="err">@</span><span class="nx">mc</span><span class="p">.</span><span class="nx">deterministic</span> 
<span class="nx">def</span> <span class="nx">center_i</span><span class="p">(</span> <span class="nx">assignment</span> <span class="o">=</span> <span class="nx">assignment</span><span class="p">,</span> <span class="nx">centers</span> <span class="o">=</span> <span class="nx">centers</span> <span class="p">)</span><span class="o">:</span>
        <span class="k">return</span> <span class="nx">centers</span><span class="cp">[</span> <span class="nx">assignment</span><span class="cp">]</span> 

<span class="err">@</span><span class="nx">mc</span><span class="p">.</span><span class="nx">deterministic</span>
<span class="nx">def</span> <span class="nx">tau_i</span><span class="p">(</span> <span class="nx">assignment</span> <span class="o">=</span> <span class="nx">assignment</span><span class="p">,</span> <span class="nx">taus</span> <span class="o">=</span> <span class="nx">taus</span> <span class="p">)</span><span class="o">:</span>
        <span class="k">return</span> <span class="nx">taus</span><span class="cp">[</span> <span class="nx">assignment</span><span class="cp">]</span> 

<span class="nx">print</span> <span class="s2">&quot;Random assignments: &quot;</span><span class="p">,</span> <span class="nx">assignment</span><span class="p">.</span><span class="nx">value</span><span class="cp">[</span> <span class="p">:</span><span class="mi">4</span> <span class="cp">]</span><span class="p">,</span> <span class="s2">&quot;...&quot;</span>    
<span class="nx">print</span> <span class="s2">&quot;Assigned center: &quot;</span><span class="p">,</span> <span class="nx">center_i</span><span class="p">.</span><span class="nx">value</span><span class="cp">[</span> <span class="p">:</span><span class="mi">4</span><span class="cp">]</span><span class="p">,</span> <span class="s2">&quot;...&quot;</span>
<span class="nx">print</span> <span class="s2">&quot;Assigned precision: &quot;</span><span class="p">,</span> <span class="nx">tau_i</span><span class="p">.</span><span class="nx">value</span><span class="cp">[</span> <span class="p">:</span><span class="mi">4</span><span class="cp">]</span><span class="p">,</span> <span class="s2">&quot;...&quot;</span>
</pre></div>


<div class="highlight"><pre><span class="n">Random</span> <span class="n">assignments</span><span class="o">:</span>  <span class="p">[</span><span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">1</span><span class="p">]</span> <span class="p">...</span>
<span class="n">Assigned</span> <span class="n">center</span><span class="o">:</span>  <span class="p">[</span> <span class="mf">174.87922993</span>  <span class="mf">174.87922993</span>  <span class="mf">129.56485407</span>  <span class="mf">174.87922993</span><span class="p">]</span> <span class="p">...</span>
<span class="n">Assigned</span> <span class="n">precision</span><span class="o">:</span>  <span class="p">[</span> <span class="mf">0.00012929</span>  <span class="mf">0.00012929</span>  <span class="mf">0.00050314</span>  <span class="mf">0.00012929</span><span class="p">]</span> <span class="p">...</span>
</pre></div>


<p>In[95]:</p>
<div class="highlight"><pre><span class="cp">#and to combine it with the observations:</span>
<span class="n">observations</span> <span class="o">=</span> <span class="n">mc</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span> <span class="s">&quot;obs&quot;</span><span class="p">,</span> <span class="n">center_i</span><span class="p">,</span> <span class="n">tau_i</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="n">True</span> <span class="p">)</span>

<span class="cp">#below we create a model class</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mc</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span> <span class="p">[</span><span class="n">p</span><span class="p">,</span> <span class="n">assignment</span><span class="p">,</span> <span class="n">taus</span><span class="p">,</span> <span class="n">centers</span> <span class="p">]</span> <span class="p">)</span>
</pre></div>


<p>PyMC has an <span class="caps">MCMC</span> class, <code>MCMC</code> in the main namespace of PyMC, that implements
the <span class="caps">MCMC</span> exploring algorithm. We initialize it by passing in a <code>Model</code> instance:</p>
<div class="highlight"><pre><span class="n">mcmc</span> <span class="o">=</span> <span class="n">mc</span><span class="p">.</span><span class="n"><span class="caps">MCMC</span></span><span class="p">(</span> <span class="n">model</span> <span class="p">)</span>
</pre></div>


<p>The method for asking the <code>MCMC</code> to explore the space is <code>sample( iterations )</code>,
where <code>iterations</code> is the number of steps you wish the algorithm to perform. We
try 50000 steps&nbsp;below:</p>
<p>In[96]:</p>
<div class="highlight"><pre><span class="n">mcmc</span> <span class="o">=</span> <span class="n">mc</span><span class="p">.</span><span class="n"><span class="caps">MCMC</span></span><span class="p">(</span> <span class="n">model</span> <span class="p">)</span>
<span class="n">mcmc</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span> <span class="mi">50000</span> <span class="p">)</span>
</pre></div>


<p>[<strong><em>*</em></strong><strong><em>*</em></strong><strong>100%<strong><em>*</em></strong><strong><em>*</em></strong></strong>**]  50000 of 50000&nbsp;complete</p>
<p>Below I plot the paths, or &#8220;traces&#8221;, the unknown parameters (centers,
precisions, and $p$) have taken thus far. The traces can be retrieved using the
<code>trace</code> method in the <code>MCMC</code> object created, which accepts the assigned PyMC
variable <code>name</code>. For example, <code>mcmc.trace("centers")</code> will retrieve a <code>Trace</code>
object that can be indexed (using <code>[:]</code> or <code>.gettrace()</code> to retrieve all traces,
or fancy-indexing like <code>[1000:]</code>).</p>
<p>In[99]:</p>
<div class="highlight"><pre><span class="n">figsize</span><span class="p">(</span> <span class="mf">12.5</span><span class="p">,</span> <span class="mi">9</span> <span class="p">)</span>
<span class="n">subplot</span><span class="p">(</span><span class="mi">311</span><span class="p">)</span>
<span class="n">lw</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">center_trace</span> <span class="o">=</span> <span class="n">mcmc</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="s">&quot;centers&quot;</span><span class="p">)[</span><span class="o">:</span><span class="p">]</span>

<span class="cp">#for pretty colors later in the book.</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;#<span class="caps">348ABD</span>&quot;</span><span class="p">,</span> <span class="s">&quot;#A60628&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="n">center_trace</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">center_trace</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> \
                                <span class="k">else</span> <span class="p">[</span> <span class="s">&quot;#A60628&quot;</span><span class="p">,</span> <span class="s">&quot;#<span class="caps">348ABD</span>&quot;</span><span class="p">]</span>

<span class="n">plot</span><span class="p">(</span> <span class="n">center_trace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s">&quot;trace of center 0&quot;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>  <span class="n">lw</span> <span class="o">=</span> <span class="n">lw</span>  <span class="p">)</span>
<span class="n">plot</span><span class="p">(</span> <span class="n">center_trace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s">&quot;trace of center 1&quot;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">lw</span> <span class="o">=</span> <span class="n">lw</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span> <span class="s">&quot;Traces of unknown parameters&quot;</span> <span class="p">)</span>
<span class="n">leg</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">leg</span><span class="p">.</span><span class="n">get_frame</span><span class="p">().</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.7</span><span class="p">)</span>

<span class="n">subplot</span><span class="p">(</span><span class="mi">312</span><span class="p">)</span>
<span class="n">std_trace</span> <span class="o">=</span> <span class="n">mcmc</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="s">&quot;stds&quot;</span><span class="p">)[</span><span class="o">:</span><span class="p">]</span> 
<span class="n">plot</span><span class="p">(</span> <span class="n">std_trace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s">&quot;trace of standard deviation of cluster 0&quot;</span><span class="p">,</span> 
    <span class="n">c</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">lw</span> <span class="o">=</span> <span class="n">lw</span>  <span class="p">)</span>
<span class="n">plot</span><span class="p">(</span> <span class="n">std_trace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s">&quot;trace of standard deviation of cluster 1&quot;</span><span class="p">,</span> 
    <span class="n">c</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">lw</span> <span class="o">=</span> <span class="n">lw</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s">&quot;upper left&quot;</span><span class="p">)</span>

<span class="n">subplot</span><span class="p">(</span><span class="mi">313</span><span class="p">)</span>
<span class="n">p_trace</span> <span class="o">=</span> <span class="n">mcmc</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="s">&quot;p&quot;</span><span class="p">)[</span><span class="o">:</span><span class="p">]</span>
<span class="n">plot</span><span class="p">(</span> <span class="n">p_trace</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">&quot;$p$: frequency of assignment to cluster 0&quot;</span><span class="p">,</span>
     <span class="n">color</span> <span class="o">=</span> <span class="s">&quot;#467821&quot;</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span> <span class="s">&quot;Steps&quot;</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>


<p><a href="_fig_2100.png">!image</a></p>
<p>Notice the following&nbsp;characteristics:</p>
<ol>
<li>The traces converges, not to a single point, but to a <em>distribution</em> of
possible points. This is <em>convergence</em> in an <span class="caps">MCMC</span>&nbsp;algorithm.</li>
<li>Inference using the first few thousand points is a bad idea, as they are
unrelated to the final distribution we are interested in. Thus is it a good idea
to discard those samples before using the samples for inference. We call this
period before converge the <em>burn-in period</em>.</li>
<li>The traces appear as a random &#8220;walk&#8221; around the space, that is, the paths
exhibit correlation with previous positions. This is both good and bad. We will
always have correlation between current positions and the previous positions,
but too much of it means we are not exploring the space well. This will be
detailed in the Diagnostics section  later in this&nbsp;chapter.</li>
</ol>
<p>To achieve further convergence, we will perform more <span class="caps">MCMC</span> steps. Starting the
<span class="caps">MCMC</span> again after it has already been called does not mean starting the entire
algorithm over. In the pseudo-code algorithm of <span class="caps">MCMC</span> above, the only position
that matters is the current position (new positions are investigated near the
current position), implicitly stored in PyMC variables&#8217; <code>value</code> attribute. Thus
it is fine to halt an <span class="caps">MCMC</span> algorithm and inspect its progress, with the
intention of starting it up again later. The `value&#8217; attributes are not&nbsp;overwritten.</p>
<p>We will sample the <span class="caps">MCMC</span> one hundred thousand more times and visualize the
progress&nbsp;below:</p>
<p>In[100]:</p>
<div class="highlight"><pre><span class="n">mcmc</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span> <span class="mi">100000</span> <span class="p">)</span>
</pre></div>


<p>[<strong><em>*</em></strong><strong><em>*</em></strong><strong>100%<strong><em>*</em></strong><strong><em>*</em></strong></strong>**]  100000 of 100000&nbsp;complete</p>
<p>In[101]:</p>
<div class="highlight"><pre><span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">center_trace</span> <span class="o">=</span> <span class="n">mcmc</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="s">&quot;centers&quot;</span><span class="p">,</span> <span class="n">chain</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)[</span><span class="o">:</span><span class="p">]</span>
<span class="n">prev_center_trace</span> <span class="o">=</span> <span class="n">mcmc</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="s">&quot;centers&quot;</span><span class="p">,</span> <span class="n">chain</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)[</span><span class="o">:</span><span class="p">]</span>

<span class="n">x</span> <span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">50000</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prev_center_trace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s">&quot;previous trace of center 0&quot;</span><span class="p">,</span>  
    <span class="n">lw</span> <span class="o">=</span> <span class="n">lw</span><span class="p">,</span>  <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.4</span>  <span class="p">,</span> <span class="n">c</span><span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prev_center_trace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s">&quot;previous trace of center 1&quot;</span><span class="p">,</span> 
     <span class="n">lw</span> <span class="o">=</span> <span class="n">lw</span><span class="p">,</span>  <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.4</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">50000</span><span class="p">,</span> <span class="mi">150000</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>  <span class="n">center_trace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s">&quot;new trace of center 0&quot;</span><span class="p">,</span>  <span class="n">lw</span> <span class="o">=</span> <span class="n">lw</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s">&quot;#<span class="caps">348ABD</span>&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">center_trace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s">&quot;new trace of center 1&quot;</span><span class="p">,</span>  <span class="n">lw</span> <span class="o">=</span> <span class="n">lw</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s">&quot;#A60628&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span> <span class="s">&quot;Traces of unknown center parameters&quot;</span> <span class="p">)</span>
<span class="n">leg</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">leg</span><span class="p">.</span><span class="n">get_frame</span><span class="p">().</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span> <span class="s">&quot;Steps&quot;</span> <span class="p">);</span>
</pre></div>


<p><a href="_fig_2400.png">!image</a></p>
<p>The <code>trace</code> method in the <code>MCMC</code> instance has a keyword argument <code>chain</code>, that
indexes which call to <code>sample</code> you would like to be returned. (Often we need to
call <code>sample</code> multiple times, and the ability to retrieve past samples is a
useful procedure). The default for <code>chain</code> is -1, which will return the samples
from the lastest call to <code>sample</code>.</p>
<h4>Cluster&nbsp;Investigation</h4>
<p>We have not forgotten our main challenge: identify the clusters. We have
determined posterior distributions for our unknowns. We plot the posterior
distributions of the center and standard deviation variables&nbsp;below:</p>
<p>In[102]:</p>
<div class="highlight"><pre><span class="n">figsize</span><span class="p">(</span> <span class="mf">11.0</span><span class="p">,</span> <span class="mi">4</span> <span class="p">)</span>
<span class="n">std_trace</span> <span class="o">=</span>  <span class="n">mcmc</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="s">&quot;stds&quot;</span><span class="p">)[</span><span class="o">:</span><span class="p">]</span> 

<span class="n">_i</span> <span class="o">=</span> <span class="p">[</span> <span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="n">in</span> <span class="n">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">:</span>
    <span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">_i</span><span class="p">[</span> <span class="mi">2</span><span class="o">*</span><span class="n">i</span> <span class="p">]</span> <span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;Posterior of center of cluster %d&quot;</span><span class="o">%</span><span class="n">i</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span> <span class="n">center_trace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">bins</span> <span class="o">=</span> <span class="mi">30</span><span class="p">,</span> 
            <span class="n">histtype</span><span class="o">=</span><span class="s">&quot;stepfilled&quot;</span> <span class="p">)</span>

    <span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">_i</span><span class="p">[</span> <span class="mi">2</span><span class="o">*</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span> <span class="s">&quot;Posterior of standard deviation of cluster %d&quot;</span><span class="o">%</span><span class="n">i</span> <span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span> <span class="n">std_trace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>  <span class="n">bins</span> <span class="o">=</span> <span class="mi">30</span><span class="p">,</span> 
            <span class="n">histtype</span><span class="o">=</span><span class="s">&quot;stepfilled&quot;</span>  <span class="p">)</span>
    <span class="err">#</span><span class="n">plt</span><span class="p">.</span><span class="n">autoscale</span><span class="p">(</span><span class="n">tight</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>


<p><a href="_fig_2600.png">!image</a></p>
<p>The <span class="caps">MCMC</span> algorithm has proposed that the most likely centers of the two clusters
are near 120 and 200 respectively. Similar inference can be applied to the
standard&nbsp;deviation.</p>
<p>We are also given the posterior distributions for the labels of the data point,
which is present in <code>mcmc.trace("assignment")</code>. Below is a visualization of
this. The y-axis represents a subsample of the posterior labels for each data
point. The x-axis are the sorted values of the data points. A red square is an
assignment to cluster 1, and a blue square is an assignment to cluster&nbsp;0.</p>
<p>In[103]:</p>
<div class="highlight"><pre><span class="n">figsize</span><span class="p">(</span> <span class="mf">12.5</span><span class="p">,</span> <span class="mf">4.5</span> <span class="p">)</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">mpl</span><span class="p">.</span><span class="n">colors</span><span class="p">.</span><span class="n">ListedColormap</span><span class="p">(</span><span class="n">colors</span><span class="p">)</span>
<span class="n">imshow</span><span class="p">(</span> <span class="n">mcmc</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="s">&quot;assignment&quot;</span><span class="p">)[</span><span class="o">::</span><span class="mi">400</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="p">],</span> 
       <span class="n">cmap</span> <span class="o">=</span> <span class="n">cmap</span> <span class="p">,</span><span class="n">aspect</span><span class="o">=</span><span class="mf">.4</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">.9</span> <span class="p">)</span>
<span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">40</span> <span class="p">),</span> <span class="p">[</span> <span class="s">&quot;%.2f&quot;</span><span class="o">%</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="n">in</span> <span class="n">np</span><span class="p">.</span><span class="n">sort</span><span class="p">(</span> <span class="n">data</span> <span class="p">)[</span><span class="o">::</span><span class="mi">40</span><span class="p">]</span> <span class="p">]</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;posterior sample&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;value of $i$th data point&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;Posterior labels of data points&quot;</span> <span class="p">);</span>
</pre></div>


<p><a href="_fig_2800.png">!image</a></p>
<p>Looking at the above plot, it appears that the most uncertainity is between 150
and 170. The above plot slightly misrepresents things, as the x-axis is not a
true scale (it displays the value of the $i$th sorted data point.) A more clear
diagram is below, where we have estimated the <em>frequency</em> of each data point
belonging to the labels 0 and&nbsp;1.</p>
<p>In[104]:</p>
<div class="highlight"><pre><span class="n">cmap</span> <span class="o">=</span> <span class="n">mpl</span><span class="p">.</span><span class="n">colors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="n">from_list</span><span class="p">(</span><span class="s">&quot;<span class="caps">BMH</span>&quot;</span><span class="p">,</span> <span class="n">colors</span> <span class="p">)</span>
<span class="n">assign_trace</span> <span class="o">=</span> <span class="n">mcmc</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="s">&quot;assignment&quot;</span><span class="p">)[</span><span class="o">:</span><span class="p">]</span>
<span class="n">scatter</span><span class="p">(</span> <span class="n">data</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">assign_trace</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> 
        <span class="n">c</span><span class="o">=</span><span class="n">assign_trace</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span> <span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span> <span class="s">&quot;Probability of data point belonging to cluster 0&quot;</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;probability&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;value of data point&quot;</span> <span class="p">);</span>
</pre></div>


<p><a href="_fig_3000.png">!image</a></p>
<p>Even though we modeled the clusters using Normal distributions, we didn&#8217;t get
just a single Normal distribution that <em>best</em> fits the data (whatever our
definition of best is), but a distribution of values for the Normal&#8217;s
parameters. How can we choose just a single pair of values for the mean and
variance and determine a <em>sorta-best-fit</em>&nbsp;gaussian?</p>
<p>One quick and dirty way (which has nice theoretical properties we will see in
Chapter 5), is to use the <em>mean</em> of the posterior distributions. Below we
overlay the Normal density functions, using the mean of the posterior
distributions as the chosen parameters, with our observed&nbsp;data:</p>
<p>In[106]:</p>
<div class="highlight"><pre><span class="n">norm</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">500</span> <span class="p">)</span>
<span class="n">posterior_center_means</span> <span class="o">=</span> <span class="n">center_trace</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> 
<span class="n">posterior_std_means</span> <span class="o">=</span> <span class="n">std_trace</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> 
<span class="n">posterior_p_mean</span> <span class="o">=</span> <span class="n">mcmc</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="s">&quot;p&quot;</span><span class="p">)[</span><span class="o">:</span><span class="p">].</span><span class="n">mean</span><span class="p">()</span>

<span class="n">hist</span><span class="p">(</span> <span class="n">data</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s">&quot;step&quot;</span><span class="p">,</span> <span class="n">normed</span> <span class="o">=</span> <span class="n">True</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">&quot;k&quot;</span><span class="p">,</span> 
    <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">&quot;histogram of data&quot;</span> <span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">posterior_p_mean</span><span class="o">*</span><span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">posterior_center_means</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> 
                              <span class="n">scale</span><span class="o">=</span><span class="n">posterior_std_means</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">&quot;Cluster 0 (using posterior-mean parameters)&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">posterior_p_mean</span><span class="p">)</span><span class="o">*</span><span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">posterior_center_means</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> 
                                  <span class="n">scale</span><span class="o">=</span><span class="n">posterior_std_means</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">&quot;Cluster 1 (using posterior-mean parameters)&quot;</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span> <span class="s">&quot;Visualizing Clusters using posterior-mean parameters&quot;</span> <span class="p">);</span>
</pre></div>


<p><a href="_fig_3200.png">!image</a></p>
<h3>Important: Don&#8217;t mix posterior&nbsp;samples</h3>
<p>In the above example, a possible (though less likely) scenario is that cluster 0
has a very large standard deviation, and cluster 1 has a small standard
deviation. This would still satisfy the evidence, albeit less so than our
original inference. Alternatively, it would be incredibly unlikely for <em>both</em>
distributions to have a small standard deviation, as the data does not support
this hypothesis at all. Thus the two standard deviations are <em>dependent</em> on each
other: if one is small, the other must be large. In fact, <em>all</em> the unknowns are
related in a similar manner. For example, if a standard deviation is large, the
mean has a wider possible space of realizations. Conversely, a small standard
deviation restricts the mean to a small&nbsp;area.</p>
<p>During <span class="caps">MCMC</span>, we are returned vectors representing samples from the unknown
posteriors. Elements of different vectors cannot be used together, as this would
break the above logic: perhaps a sample has returned that cluster 1 has a small
standard deviation, hence all the other variables in that sample would
incorporate that and be adjusted accordingly. It is easy to avoid this problem
though, just make sure you are indexing traces&nbsp;correctly.</p>
<p>Another small example to illustrate the point. Suppose two variables, $x$ and
$y$, are related by $x+y=10$. We model $x$ as a Normal random variable with mean
4 and explore 300&nbsp;samples.</p>
<p>In[2]:</p>
<div class="highlight"><pre><span class="n">import</span> <span class="n">pymc</span> <span class="n">as</span> <span class="n">mc</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">mc</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">&quot;x&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span> <span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mc</span><span class="p">.</span><span class="n">Lambda</span><span class="p">(</span> <span class="s">&quot;y&quot;</span><span class="p">,</span> <span class="n">lambda</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="o">:</span> <span class="mi">10</span><span class="o">-</span><span class="n">x</span><span class="p">,</span> <span class="n">trace</span> <span class="o">=</span> <span class="n">True</span> <span class="p">)</span>

<span class="n">ex_mcmc</span> <span class="o">=</span> <span class="n">mc</span><span class="p">.</span><span class="n"><span class="caps">MCMC</span></span><span class="p">(</span> <span class="n">mc</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">]</span> <span class="p">)</span> <span class="p">)</span>
<span class="n">ex_mcmc</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span> <span class="mi">500</span> <span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">ex_mcmc</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="s">&quot;x&quot;</span><span class="p">)[</span><span class="o">:</span><span class="p">]</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">ex_mcmc</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="s">&quot;y&quot;</span><span class="p">)[</span><span class="o">:</span><span class="p">]</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span> <span class="s">&quot;Displaying (extreme) case of dependence between unknowns&quot;</span> <span class="p">);</span>
</pre></div>


<p>[<strong><em>*</em></strong><strong><em>*</em></strong><strong>100%<strong><em>*</em></strong><strong><em>*</em></strong></strong>**]  500 of 500&nbsp;complete</p>
<p><a href="_fig_3401.png">!image</a></p>
<p>As you can see, the two variables are not unrelated, and it would be wrong to
add the $i$th sample of $x$ to the $j$th sample of $y$, unless $i =&nbsp;j$.</p>
<h4>Returning to Clustering:&nbsp;Prediction</h4>
<p>The above clustering can be generalized to $k$ clusters. Choosing $k=2$ allowed
us to visualize the <span class="caps">MCMC</span> better, and examine some very interesting&nbsp;plots.</p>
<p>What about prediction? Suppose we observe a new data point, say $x = 175$, and
we wish to label it to a cluster. It is foolish to simply assign it to the
<em>closer</em> cluster center, as this ignores the standard deviation of the clusters,
and we have seen from the plots aboves that this consideration is very
important. More formally: we are interested in the <em>probability</em> (as we cannot
be certain about labels) of assigning $x=175$ to cluster 1. Denote the
assignment of $x$ as $L_x$, which is equal to 0 or 1, and we are interested in
$P(L_x = 1 \;|\; x = 175&nbsp;)$.</p>
<p>A naive method to compute this is to re-reun the above <span class="caps">MCMC</span> with the additional
data point appended. The disadvantage with this method is that it will be slow
to infer for each novel data point. Alternatively, we can try a <em>less precise</em>,
but much quicker&nbsp;method.</p>
<p>We will use Bayes Theorem for this. If you recall, Bayes Theorem looks&nbsp;like:</p>
<p>$$ P( A | X ) = \frac{ P( X  | A )P(A) }{P(X)&nbsp;}$$</p>
<p>In our case, $A$ represents $L_x = 1$ and $X$ is the evidence we have: we
observe that $x = 175$. For a particular sample set of parameters for our
posterior distribution, $( \mu_0, \sigma_0, \mu_1, \sigma_1, p)$, we are
interested in asking &#8220;Is the probability that $x$ is in cluster 1 <em>greater</em> than
the probability it is in cluster 0?&#8221;, where the probability is dependent on the
chosen&nbsp;parameters.</p>
<p>\begin{align}
<span class="amp">&amp;</span> P(L_x = 1| x = 175 ) \gt P( (L_x = 0| x = 175 ) &#92;[5pt]
<span class="amp">&amp;</span> \frac{ P( x=175  | L_x = 1  )P( L_x = 1 ) }{P(x = 175) } \gt \frac{ P( x=175
| L_x = 0  )P( L_x = 0 )}{P(x = 175) }&nbsp;\end{align}</p>
<p>As the denominators are equal, they can be ignored (and good riddance, because
computing the quantity $P(x = 175)$ can be&nbsp;difficult).</p>
<p>$$  P( x=175  | L_x = 1  )P( L_x = 1 ) \gt  P( x=175  | L_x = 0  )P( L_x = 0 )&nbsp;$$</p>
<p>In[385]:</p>
<div class="highlight"><pre><span class="n">norm_pdf</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">pdf</span>
<span class="n">p_trace</span> <span class="o">=</span> <span class="n">mcmc</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="s">&quot;p&quot;</span><span class="p">)[</span><span class="o">:</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">175</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">p_trace</span><span class="o">*</span><span class="n">norm_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span> <span class="o">=</span> <span class="n">center_trace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">std_trace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="p">)</span> <span class="o">&gt;</span> \
    <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p_trace</span><span class="p">)</span><span class="o">*</span><span class="n">norm_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span> <span class="o">=</span> <span class="n">center_trace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">std_trace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="p">)</span>

<span class="n">print</span> <span class="s">&quot;Probability of belonging to cluster 1:&quot;</span><span class="p">,</span> <span class="n">v</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span class="n">Probability</span> <span class="n">of</span> <span class="n">belonging</span> <span class="n">to</span> <span class="n">cluster</span> <span class="mi">1</span><span class="o">:</span> <span class="mf">0.9406</span>
</pre></div>


<p>Giving us a probability instead of a label is a very useful thing. Instead of
the&nbsp;naive</p>
<div class="highlight"><pre><span class="n">L</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">prob</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="k">else</span> <span class="mi">0</span>
</pre></div>


<p>we can optimize our guesses using <em>loss function</em>, of which the entire fifth
chapter is devoted&nbsp;to.</p>
<h3>Using <code>MAP</code> to improve&nbsp;convergence</h3>
<p>If you ran the above example yourself, you may have noticed that our results
were not consistent: perhaps your cluster cluster division was more scattered,
or perhaps less scattered. The problem is that our traces are a function of the
<em>starting values</em> of the <span class="caps">MCMC</span>&nbsp;algorithm.</p>
<p>It can be mathematically shown that letting the <span class="caps">MCMC</span> run long enough, by
performing many steps, the algorithm <em>should forget its initial position</em>. In
fact, this is what it means to say the <span class="caps">MCMC</span> converged (in practice though we can
never achieve total convergence). Hence if we observe different posterior
analysis, it is likely because our <span class="caps">MCMC</span> has not fully converged yet, and we
should not use samples from it yet (we should use a larger burn-in period&nbsp;).</p>
<p>In fact, poor starting values can prevent any convergence, or significantly slow
it down. Ideally, we would like to have the chain start at the <em>peak</em> of our
landscape, as this is exactly where the posterior distributions exist. Hence, if
we started at the &#8220;peak&#8221;, we could avoid a lengthy burn-in period and incorrect
inference. Generally, we call this &#8220;peak&#8221; the <em>maximum a posterior</em> or, more
simply, the <em><span class="caps">MAP</span></em>.</p>
<p>Of course, we do not know where the <span class="caps">MAP</span> is. PyMC provides an object that will
approximate, if not find, the <span class="caps">MAP</span> location. In the PyMC main namespace is the
<code>MAP</code> object that accepts a PyMC <code>Model</code> instance. Calling <code>.fit()</code> from the
<code>MAP</code> instance sets the variables in the model to their <span class="caps">MAP</span>&nbsp;values.</p>
<div class="highlight"><pre><span class="n">map_</span> <span class="o">=</span> <span class="n">mc</span><span class="p">.</span><span class="n"><span class="caps">MAP</span></span><span class="p">(</span> <span class="n">model</span> <span class="p">)</span>
<span class="n">map</span><span class="p">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>


<p>The <code>MAP.fit()</code> methods has the flexibility of allowing the user to choose which
opimization algorithm to use (after all, this is a optimization problem: we are
looking for the values that maximize our landscape), as not all optimization
algorithms are created equal. The default optimization algorithm in the call to
<code>fit</code> is scipy&#8217;s <code>fmin</code> algorithm (which attemps to minimize the <em>negative of
the landscape</em>). An alternative algorithm that is available is Powell&#8217;s Method,
a favourite of PyMC blogger <a href="http://healthyalgorithms.com/">Abraham Flaxman</a>
[1], by calling <code>fit(method='fmin_powell')</code>. From my experience, I use the
default, but if my convergence is slow or not guaranteed, I experiment with
Powell&#8217;s&nbsp;method.</p>
<p>The <span class="caps">MAP</span> can also be used as a solution to the inference problem, as
mathematically it  is the <em>most likely</em> value for the unknowns. But as mentioned
earlier in this chapter,  this location ignores the uncertainity and doesn&#8217;t
return a&nbsp;distribution.</p>
<p>Typically, it is always a good idea, and rarely a bad idea, to prepend your call
to <code>mcmc</code> with a call to <code>MAP(model).fit()</code>. The intermediate call to <code>fit</code> is
hardly computationally intensive, and will save you time later due to a shorter
burn-in&nbsp;period.</p>
<h4>Speaking of the burn-in&nbsp;period</h4>
<p>Tt is still a good idea to provide a burn-in period, even if we are using <code>MAP</code>
prior to calling <code>MCMC.sample</code>, just to be safe. We can have PyMC automatically
discard the first $n$ samples by specifying the <code>burn</code> parameter in the call to
<code>sample</code>. As one does not know when the chain has fully converged, I like to
assign the first <em>half</em> of my samples to be discarded, sometimes up to 90% of my
samples for longer runs. To continue the clustering example from above, my new
code would look something&nbsp;like:</p>
<div class="highlight"><pre><span class="n">model</span> <span class="o">=</span> <span class="n">mc</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span> <span class="p">[</span><span class="n">p</span><span class="p">,</span> <span class="n">assignment</span><span class="p">,</span> <span class="n">taus</span><span class="p">,</span> <span class="n">centers</span> <span class="p">]</span> <span class="p">)</span>

<span class="n">map_</span> <span class="o">=</span> <span class="n">mc</span><span class="p">.</span><span class="n"><span class="caps">MAP</span></span><span class="p">(</span> <span class="n">model</span> <span class="p">)</span>
<span class="n">map_</span><span class="p">.</span><span class="n">fit</span><span class="p">()</span> <span class="err">#</span><span class="n">stores</span> <span class="n">the</span> <span class="n">fitted</span> <span class="n">variables</span><span class="err">&#39;</span> <span class="n">values</span> <span class="n">in</span> <span class="n">foo</span><span class="p">.</span><span class="n">value</span>

<span class="n">mcmc</span> <span class="o">=</span> <span class="n">mc</span><span class="p">.</span><span class="n"><span class="caps">MCMC</span></span><span class="p">(</span> <span class="n">model</span> <span class="p">)</span>
<span class="n">mcmc</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span> <span class="mi">100000</span><span class="p">,</span> <span class="mi">50000</span> <span class="p">)</span>
</pre></div>


<h2>Diagnosing&nbsp;Convergence</h2>
<h3>Autocorrelation</h3>
<p>Autocorrelation is a measure of how related a series of numbers is with itself.
A measurement of 1.0 is perfect positive autocorrelation, 0 no autocorrelation,
and -1 is perfect negative correlation.  If you are familiar with standard
<em>correlation</em>, then autocorrelation is just how correlated a series, $x_\tau$,
at time $t$ is with the series at time&nbsp;$t-k$:</p>
<p>$$R(k) = Corr( x_t, x_{t-k} )&nbsp;$$</p>
<p>For example, consider the two&nbsp;series:</p>
<p>$$x_t \sim \text{Normal}(0,1), \;\; x_0 = 0$$
$$y_t \sim \text{Normal}(y_{t-1}, 1 ), \;\; y_0 =&nbsp;0$$</p>
<p>which has an example paths&nbsp;like:</p>
<p>In[40]:</p>
<div class="highlight"><pre><span class="n">figsize</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>

<span class="n">import</span> <span class="n">pymc</span> <span class="n">as</span> <span class="n">mc</span>
<span class="kt">x_t</span> <span class="o">=</span> <span class="n">mc</span><span class="p">.</span><span class="n">rnormal</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">200</span> <span class="p">)</span>
<span class="kt">x_t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="kt">y_t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span> <span class="mi">200</span> <span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="n">in</span> <span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span><span class="o">:</span>
    <span class="kt">y_t</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mc</span><span class="p">.</span><span class="n">rnormal</span><span class="p">(</span> <span class="kt">y_t</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span> <span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span> <span class="kt">y_t</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span> <span class="s">&quot;$y_t$&quot;</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span> <span class="kt">x_t</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">&quot;$x_t$&quot;</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;time, $t$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>


<p><a href="_fig_4000.png">!image</a></p>
<p>One way to think of autocorrelation is &#8220;If I know the position of the series at
time $s$, can it help me know where I am at time $t$?&#8221; In the series $x_t$, the
answer is No. By construction, $x_t$ are random variables. If I told you that
$x_2 = 0.5$, could you give me a better guess about $x_3$?&nbsp;No.</p>
<p>On the other hand, $y_t$ is autocorrelated. By construction, if I knew that $y_2
= 10$, I can be very confident that $y_3$ will not be very far from 10.
Similarly, I can even make a (less confident guess) about $y_4$: it will
probably not be near 0 or 20, but a value of 5 is not too unlikely. I can make a
similar argument about $y_5$, but again, I am less confident. Taking this to
it&#8217;s logical conclusion, we must concede that as $k$, the lag between time
points, increases the autocorrelation decreases. We can visualize&nbsp;this:</p>
<p>In[41]:</p>
<div class="highlight"><pre><span class="n">def</span> <span class="nf">autocorr</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">:</span>
    <span class="err">#</span><span class="n">from</span> <span class="n">http</span><span class="o">:</span><span class="c1">//tinyurl.com/afz57c4</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">correlate</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="err">&#39;</span><span class="n">full</span><span class="err">&#39;</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">result</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">max</span><span class="p">(</span> <span class="n">result</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">result</span><span class="p">[</span><span class="n">result</span><span class="p">.</span><span class="n">size</span><span class="o">/</span><span class="mi">2</span><span class="o">:</span><span class="p">]</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span> <span class="s">&quot;#<span class="caps">348ABD</span>&quot;</span><span class="p">,</span> <span class="s">&quot;#A60628&quot;</span><span class="p">,</span> <span class="s">&quot;#<span class="caps">7A68A6</span>&quot;</span><span class="p">]</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">autocorr</span><span class="p">(</span> <span class="kt">y_t</span> <span class="p">)[</span><span class="mi">1</span><span class="o">:</span><span class="p">],</span> <span class="n">width</span> <span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;$y_t$&quot;</span><span class="p">,</span> 
        <span class="n">edgecolor</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">autocorr</span><span class="p">(</span> <span class="kt">x_t</span> <span class="p">)[</span><span class="mi">1</span><span class="o">:</span><span class="p">],</span> <span class="n">width</span> <span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s">&quot;$x_t$&quot;</span><span class="p">,</span> 
        <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span> <span class="n">title</span><span class="o">=</span><span class="s">&quot;Autocorrelation&quot;</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;measured correlation </span><span class="se">\n</span><span class="s">between $y_t$ and $y_{t-k}$.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;k (lag)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;Autocorrelation plot of $y_t$ and $x_t$ for differing $k$ lags.&quot;</span><span class="p">);</span>
</pre></div>


<p><a href="_fig_4200.png">!image</a></p>
<p>Notice that as $k$ increases, the autocorrelation of $y_t$ decreases from a very
high point. Compare with the autocorrelation of $x_t$ which looks like noise
(which it really is), hence we can conclude no autocorrelation exists in this&nbsp;series.</p>
<h4>How does this relate to <span class="caps">MCMC</span>&nbsp;convergence?</h4>
<p>By the nature of the <span class="caps">MCMC</span> algorithm, we will always be returned samples that
exhibit autocorrelation (this is because of the step <code>from your current
position, move to a position near you</code>).</p>
<p>A chain that is [Isn&#8217;t meandering exploring?] exploring the space well will
exhibit very high autocorrelation. Visually, if the trace seems to meander like
a river, and not settle down, the chain will have high&nbsp;autocorrelation.</p>
<p>This does not imply that a converged <span class="caps">MCMC</span> has low autocorrelation. Hence low
autocorrelation is not necessary for convergence, but it is sufficient. PyMC has
an built-in autocorrelation plotting function in the <code>Matplot</code> module.</p>
<h3>Thinning</h3>
<p>Another issue can arise if there is high-autocorrelation between posterior
samples. Many post-processing algorithms require samples to be <em>independent</em> of
each other. This can be solved, or at least reduced, by only returning to the
user every $n$th sample, thus removing some autocorrelation. Below we perform an
autocorrelation plot for $y_t$ with differing levels of&nbsp;thinning:</p>
<p>In[25]:</p>
<div class="highlight"><pre><span class="n">max_x</span> <span class="o">=</span> <span class="mi">200</span><span class="o">/</span><span class="mi">3</span><span class="o">+</span><span class="mi">1</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">max_x</span> <span class="p">)</span>


<span class="n">plt</span><span class="p">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">autocorr</span><span class="p">(</span> <span class="kt">y_t</span> <span class="p">)[</span><span class="mi">1</span><span class="o">:</span><span class="n">max_x</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">label</span><span class="o">=</span><span class="s">&quot;no thinning&quot;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">width</span> <span class="o">=</span><span class="mi">1</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">autocorr</span><span class="p">(</span> <span class="kt">y_t</span><span class="p">[</span><span class="o">::</span><span class="mi">2</span><span class="p">]</span> <span class="p">)[</span><span class="mi">1</span><span class="o">:</span><span class="n">max_x</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">label</span><span class="o">=</span><span class="s">&quot;keeping every 2d sample&quot;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">width</span><span class="o">=</span><span class="mi">1</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">autocorr</span><span class="p">(</span> <span class="kt">y_t</span><span class="p">[</span><span class="o">::</span><span class="mi">3</span><span class="p">]</span> <span class="p">)[</span><span class="mi">1</span><span class="o">:</span><span class="n">max_x</span><span class="p">],</span> <span class="n">width</span> <span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
        <span class="n">label</span><span class="o">=</span><span class="s">&quot;keeping every 3rd sample&quot;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">autoscale</span><span class="p">(</span><span class="n">tight</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span> <span class="n">title</span><span class="o">=</span><span class="s">&quot;Autocorrelation plot for $y_t$&quot;</span><span class="p">,</span><span class="n">loc</span><span class="o">=</span><span class="s">&quot;lower left&quot;</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;measured correlation </span><span class="se">\n</span><span class="s">between $y_t$ and $y_{t-k}$.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;k (lag)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;Autocorrelation of $y_t$ (no thinning vs. thinning) \</span>
<span class="s">at differing $k$ lags.&quot;</span><span class="p">);</span>
</pre></div>


<p><a href="_fig_4500.png">!image</a></p>
<p>With more thinning, the autocorrelation drops quicker. There is a tradeoff
though: higher thinning requires more <span class="caps">MCMC</span> iterations to achieve the same number
of retured samples. For example, 10 000 samples unthinned is 100 000 with a
thinning of 10 (though the latter has less&nbsp;autocorrelation).</p>
<p>What is a good amount of thinning. The returned samples will always exhibit some
autocorrelation, regardless of how much thinning is done. So long as the
autocorrelation tends to zero, you are probably ok. Typically thinning of more
than 10 is not&nbsp;necessary.</p>
<p>PyMC exposes a <code>thinning</code> parameter in the call the <code>sample</code>, for example:
<code>sample( 10000, burn = 5000, thinning = 5)</code>.</p>
<h3><code>pymc.Matplot.plot()</code></h3>
<p>It seems silly to have to manually create histograms, autocorrelation plots and
trace plots each time we perform <span class="caps">MCMC</span>. The authors of PyMC have included a
visualization tool for just this&nbsp;purpose.</p>
<p>As the title suggests, the <code>pymc.Matplot</code> module contains a poorly named
function <code>plot</code>, which I prefer to import as <code>mcplot</code> so there is no conflict
with other namespaces. <code>plot</code>, or <code>mcplot</code> as I suggest, accepts an <code>MCMC</code>
object and will return posterior distributions, traces and auto-correlations for
each variable (up to 10&nbsp;variables).</p>
<p>Below we use the tool to plot the centers of the clusters, after sampling 25 000
more times and <code>thinning = 10</code>.</p>
<p>In[110]:</p>
<div class="highlight"><pre><span class="n">from</span> <span class="n">pymc</span><span class="p">.</span><span class="n">Matplot</span> <span class="n">import</span> <span class="n">plot</span> <span class="n">as</span> <span class="n">mcplot</span>

<span class="n">mcmc</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span> <span class="mi">25000</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">mcplot</span><span class="p">(</span> <span class="n">mcmc</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="s">&quot;centers&quot;</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">common_scale</span> <span class="o">=</span> <span class="n">False</span>  <span class="p">)</span>
</pre></div>


<p>[<strong><em>*</em></strong><strong><em>*</em></strong><strong>100%<strong><em>*</em></strong><strong><em>*</em></strong></strong>**]  25000 of 25000 completePlotting centers_0
    Plotting&nbsp;centers_1</p>
<p><a href="_fig_4801.png">!image</a></p>
<p>There are really two figures here, one for each unknown in the <code>centers</code>
variable. In each figure, the subfigure in the top left corner is the trace of
the variable. This is useful for inspecting that possible &#8220;meandering&#8221; property
that is a result of&nbsp;non-convergence.</p>
<p>The largest plot on the right-hand side is the histograms of the samples, plus a
few extra features. The thickest vertical line represents the posterior mean,
which is a good summary of posterior distribution. The interval between the two
dashed vertical lines in each the posterior distributions represent the <em>95%
credible interval</em>, not to be confused with a <em>95% confidence interval</em>. I won&#8217;t
get into the latter, but the former can be interpreted as &#8220;there is a 95% chance
the parameter of interested lies in this interval&#8221;. (Changing default parameters
in the call to <code>mcplot</code> provides alternatives to 95%.) When communicating your
results to others, it is incredibly important to state this interval. One of our
purposes for studying Bayesian methods is to have a clear understanding of our
uncertainity in unknowns. Combined with the posterior mean, the 95% credible
interval provides a reliable interval to communicate the likely location of the
unknown (provided by the mean) <em>and</em> the uncertainty (represented by the width
of the&nbsp;interval).</p>
<p>The plots titled <code>center_0_acorr</code> and <code>center_1_acorr</code> are the generated
autocorrelation plots. They look different than the ones I have displayed above,
but the only difference is that 0-lag is centered in the middle of the figure,
whereas I have 0 centered to the&nbsp;left.</p>
<h2>Useful tips for&nbsp;<span class="caps">MCMC</span></h2>
<p>Bayesian inference would be the <em>de facto</em> method if it weren&#8217;t for <span class="caps">MCMC</span>&#8217;s
computational difficulties. In fact, <span class="caps">MCMC</span> is what turns most users off practical
Bayesian inference. Below I present some good heuristics to help convergence and
speed up the <span class="caps">MCMC</span>&nbsp;engine:</p>
<h3>Intelligent starting&nbsp;values</h3>
<p>It would be great to start the <span class="caps">MCMC</span> algorithm off near the posterior
distribution, so that it will take little time to start sampling correctly. We
can aid the algorithm by telling where we <em>think</em> the posterior distribution
will be by specifying the <code>value</code> parameter in the <code>Stochastic</code> variable
creation. Often we posses guess about this anyways. For example, if we have data
from a Normal distribution, and we wish to estimate the $\mu$ paramter, then a
good starting value would the <em>mean</em> of the&nbsp;data.</p>
<div class="highlight"><pre> <span class="n">mu</span> <span class="o">=</span> <span class="n">mc</span><span class="p">.</span><span class="n">Uniform</span><span class="p">(</span> <span class="s">&quot;mu&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span> <span class="p">)</span>
</pre></div>


<p>For most parameters in models, there is a frequentist esimate of it. These
estimates are a good starting value for our <span class="caps">MCMC</span> algoithms. Of course, this is
not always possible for some variables, but including as many appropriate
initial values is always a good idea. Even if your guesses are wrong, the <span class="caps">MCMC</span>
will still converge to the proper distribution, so there is little to&nbsp;lose.</p>
<p>This is what using <code>MAP</code> tries to do, by giving good initial values to the <span class="caps">MCMC</span>.
So why bother specifing user-defined values? Well, even giving <code>MAP</code> good values
will help it find the maximum&nbsp;a-posterior.</p>
<p>Also important, <em>bad initial values</em> are a source of major bugs in PyMC and can
hurt&nbsp;convergence.</p>
<h3>priors</h3>
<h3>covariance matrices and eliminating&nbsp;parameters</h3>
<h2>Conclusion</h2>
<p>PyMC provides a very strong backend to performing Bayesian inference, mostly
because it has abstracted the inner mechanics of <span class="caps">MCMC</span> from the user. Despite
this, some care must be applied to ensure your inference is not being biased by
the iterative nature of&nbsp;<span class="caps">MCMC</span>.</p>
<h3>References</h3>
<ol>
<li>Flaxman, Abraham. &#8220;Powell&#8217;s Methods for Maximization in PyMC.&#8221; Healthy
Algorithms. N.p., 9 02 2012. Web. 28 Feb 2013.
<a href="http://healthyalgorithms.com/2012/02/09/powells-method-for-maximization-in-
pymc/">http://healthyalgorithms.com/2012/02/09/powells-method-for-maximization-in-
pymc/</a>.</li>
</ol>
<p>In[1]:</p>
<div class="highlight"><pre><span class="n">from</span> <span class="n">IPython</span><span class="p">.</span><span class="n">core</span><span class="p">.</span><span class="n">display</span> <span class="n">import</span> <span class="n"><span class="caps">HTML</span></span>
<span class="n">def</span> <span class="n">css_styling</span><span class="p">()</span><span class="o">:</span>
    <span class="n">styles</span> <span class="o">=</span> <span class="n">open</span><span class="p">(</span><span class="s">&quot;../styles/custom.css&quot;</span><span class="p">,</span> <span class="s">&quot;r&quot;</span><span class="p">).</span><span class="n">read</span><span class="p">()</span>
    <span class="k">return</span> <span class="n"><span class="caps">HTML</span></span><span class="p">(</span><span class="n">styles</span><span class="p">)</span>
<span class="n">css_styling</span><span class="p">()</span>
</pre></div>


<p>Out[1]:</p>
<div class="highlight"><pre><span class="o">&lt;</span><span class="n">IPython</span><span class="p">.</span><span class="n">core</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="n"><span class="caps">HTML</span></span> <span class="n">at</span> <span class="mh">0x59a6ef0</span><span class="o">&gt;</span>
</pre></div>


<p>====================== Keys in Resources ==================================
[&#8216;text&#8217;,&nbsp;&#8216;binary&#8217;]</p>
<p>===========================================================================
you are responsible from writing those data do a file in the right place if
they need to be.&nbsp;===========================================================================</p>
          <p class="category">
        <i class="icon-certificate"></i>
        <a href="../category/testing.html">Testing</a> 
            </p>
          <p class="tag">
        <i class="icon-tags"></i>
         <a href="../tag/testing.html">Testing</a>
            </p>
    </div><!-- /.entry-content -->
    
  </article>
</section>
        <section id="extras" class="body shadow">
                    <div class="twitter">
        <h2><i class="icon-twitter"></i> twitter</h2>
        <a class="twitter-timeline" href="https://twitter.com/CarsonFarmer" 
            data-link-color="#2A75A9" 
            data-chrome="nofooter noheader noborders transparent" 
            data-widget-id="330339195518337025"
            height="300" lang="EN"
            data-related="CarsonFarmer">
            Tweets by @CarsonFarmer
        </a>
        <script>
            !function(d,s,id){
                var js,fjs=d.getElementsByTagName(s)[0], p=/^http:/.test(d.location)?'http':'https';
                if(!d.getElementById(id)){
                    js=d.createElement(s);
                    js.id=id;js.src=p+"://platform.twitter.com/widgets.js";
                    fjs.parentNode.insertBefore(js,fjs);
                }
            }(document,"script","twitter-wjs");
        </script>

</div>
                
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body ">
                <address id="about" class="vcard body shadow">
                                                                            <ul id="links">
                                    <li><a href="http://www.geo.hunter.cuny.edu/cfarmer/">CUNY Faculty Page</a></li>
                                    <li><a href="http://www.carsilab.org/">CARSI Lab</a></li>
                                </ul>
                                            Carson Farmer| Assistant Professor of GIScience | Department Geography</br>
                Hunter College - CUNY | 695 Park Avenue, New York | New York, 10065</br>
                </address><!-- /#about -->
                <p style="text-align: center;">This blog is proudly powered by <a href="http://getpelican.com/">Pelican</a>, 
                which takes great advantage of <a href="http://python.org">Python</a>.</p>
        </footer><!-- /#contentinfo -->

    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-41110370-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
    </script>
<script type="text/javascript">
    var disqus_shortname = 'carsonfarmer';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<script>
    $(function() {
        var search = window.location.search.substring(1)
        if (search == "feed=rss2") window.location = "http://www.carsonfarmer.com/feeds/all.rss.xml"
    });

    $(document).ready(function() {
    
        //Calculate the height of <header>
        //Use outerHeight() instead of height() if have padding
        var aboveHeight = $('.navbar').position().top;

    //when scroll
        $(window).scroll(function() {
            //if scrolled down more than the header’s height
                if ($(window).scrollTop() > aboveHeight){
                // if yes, add "fixed" class to the <nav>
                // add padding top to the #content 
                    // if (!$('nav').hasClass('fixed')) {
                    //     $('.home_link').show("slide", { direction: "right" }, 250)
                    // };
                    $('.navbar').addClass('fixed').addClass('shadow').css('top','0').next() //.addClass('navbar-fixed-top').next()//
                    $('#content').css('margin-top',aboveHeight+20+'px'); // value is same as the height of the nav
                } else {
                // when scroll up or less than aboveHeight,
                // remove the “fixed” class, and the padding-top
                    // if ($('nav').hasClass('fixed')) {
                    //     console.log('test')
                    //     $('.home_link').hide("slide", { direction: "right" }, 250)
                    // };
                    $('.navbar').removeClass('fixed').removeClass('shadow').next()
                    $('#content').css('margin-top','0px');
                }
        });
        $("#top-query").keydown(function(event) {
            if (event.keyCode == 13) {
                event.preventDefault();
                site_search();
                return false;
            }
        });
        $("#top-btn").click(function(event) {
            event.preventDefault();
            site_search();
            return false;
        });
    });
    
function site_search() {
    var query = $("#top-query").val();
    if (query == "") return false;
    var api = 'http://tapirgo.com/api/1/search.json?';
    var token = '51980cd33f61b05bbb000ce0';
    var el = $("#top-results");
    el.empty();
    el.append('<li style="padding-left: 10px">Fetching results...</li>')
    $.getJSON(api + 'token=' + token + '&query=' + query + '&callback=?', 
        function(data) {
            // insert function here if needed
            if (data.length) {
                el.empty();
                $.each(data, function(key, val) {
                    var xx = new Date(val.published_on)+""
                    
                    el.append('<li><a href="http://' + val.link + '" data-toggle="tooltip" data-placement="right" ' + 
                        'title="'+ xx.substring(0,15) + '">' + val.title + '</a>' +
                        '</li>');
                });
            } else {
                el.empty();
                el.append('<li style="padding-left: 10px">No results found...</li>')
            };
        $("[data-toggle=tooltip]").tooltip();
    });
    $("#top-toggle").dropdown('toggle');
}
</script>
</body>
</html>